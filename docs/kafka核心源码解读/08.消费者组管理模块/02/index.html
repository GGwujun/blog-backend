<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"
    />
    <link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
    <link rel="stylesheet" href="/blog-backend/umi.3ec1f225.css" />
    <script>
      window.routerBase = "/blog-backend";
    </script>
    <script>
      //! umi version: 3.5.41
    </script>
    <script>
      !(function () {
        var e =
            navigator.cookieEnabled && void 0 !== window.localStorage
              ? localStorage.getItem("dumi:prefers-color")
              : "auto",
          o = window.matchMedia("(prefers-color-scheme: dark)").matches,
          t = ["light", "dark", "auto"];
        document.documentElement.setAttribute(
          "data-prefers-color",
          e === t[2] ? (o ? t[1] : t[0]) : t.indexOf(e) > -1 ? e : t[0]
        );
      })();
    </script>
    <title>28 | 消费者组元数据（下）：Kafka如何管理这些元数据？ - 大师兄</title>
  </head>
  <body>
    <div id="root"><div class="__dumi-default-layout" data-route="/kafka核心源码解读/08.消费者组管理模块/02" data-show-sidemenu="true" data-show-slugs="true" data-site-mode="true" data-gapless="false"><div class="__dumi-default-navbar" data-mode="site"><button class="__dumi-default-navbar-toggle"></button><a class="__dumi-default-navbar-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-backend/">大师兄</a><nav><div class="__dumi-default-search"><input type="search" class="__dumi-default-search-input" value=""/><ul></ul></div><span>后端开发<ul><li><a href="/blog-backend/go语言核心36讲">go语言核心36讲</a></li><li><a href="/blog-backend/go并发编程实战">go并发编程实战</a></li><li><a href="/blog-backend/go语言项目开发实战">go语言项目开发实战</a></li><li><a href="/blog-backend/kafka核心技术与实战">kafka核心技术与实战</a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/零基础学python">零基础学python</a></li><li><a href="/blog-backend/python核心技术与实战">python核心技术与实战</a></li><li><a href="/blog-backend/redis核心技术与实战">redis核心技术与实战</a></li><li><a href="/blog-backend/redis源码剖析与实战">redis源码剖析与实战</a></li><li><a href="/blog-backend/陈天rust编程第一课">陈天rust编程第一课</a></li><li><a href="/blog-backend/tonybaigo语言第一课">tonybaigo语言第一课</a></li><li><a href="/blog-backend/后端存储实战课">后端存储实战课</a></li><li><a href="/blog-backend/后端技术面试38讲">后端技术面试38讲</a></li><li><a href="/blog-backend/深入c语言和程序运行原理">深入c语言和程序运行原理</a></li><li><a href="/blog-backend/现代c编程实战">现代c编程实战</a></li><li><a href="/blog-backend/罗剑锋的c实战笔记">罗剑锋的c实战笔记</a></li><li><a href="/blog-backend/零基础入门spark">零基础入门spark</a></li></ul></span><span>架构师<ul><li><a href="/blog-backend/mysql实战45讲">mysql实战45讲</a></li><li><a href="/blog-backend/数据中台实战课">数据中台实战课</a></li></ul></span><div class="__dumi-default-navbar-tool"><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "></div></div></div></nav></div><div class="__dumi-default-menu" data-mode="site"><div class="__dumi-default-menu-inner"><div class="__dumi-default-menu-header"><a class="__dumi-default-menu-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-backend/"></a><h1>大师兄</h1><p></p></div><div class="__dumi-default-menu-mobile-area"><ul class="__dumi-default-menu-nav-list"><li>后端开发<ul><li><a href="/blog-backend/go语言核心36讲">go语言核心36讲</a></li><li><a href="/blog-backend/go并发编程实战">go并发编程实战</a></li><li><a href="/blog-backend/go语言项目开发实战">go语言项目开发实战</a></li><li><a href="/blog-backend/kafka核心技术与实战">kafka核心技术与实战</a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/零基础学python">零基础学python</a></li><li><a href="/blog-backend/python核心技术与实战">python核心技术与实战</a></li><li><a href="/blog-backend/redis核心技术与实战">redis核心技术与实战</a></li><li><a href="/blog-backend/redis源码剖析与实战">redis源码剖析与实战</a></li><li><a href="/blog-backend/陈天rust编程第一课">陈天rust编程第一课</a></li><li><a href="/blog-backend/tonybaigo语言第一课">tonybaigo语言第一课</a></li><li><a href="/blog-backend/后端存储实战课">后端存储实战课</a></li><li><a href="/blog-backend/后端技术面试38讲">后端技术面试38讲</a></li><li><a href="/blog-backend/深入c语言和程序运行原理">深入c语言和程序运行原理</a></li><li><a href="/blog-backend/现代c编程实战">现代c编程实战</a></li><li><a href="/blog-backend/罗剑锋的c实战笔记">罗剑锋的c实战笔记</a></li><li><a href="/blog-backend/零基础入门spark">零基础入门spark</a></li></ul></li><li>架构师<ul><li><a href="/blog-backend/mysql实战45讲">mysql实战45讲</a></li><li><a href="/blog-backend/数据中台实战课">数据中台实战课</a></li></ul></li></ul><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "><button title="Dark theme" class="__dumi-default-dark-moon "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3854" width="22" height="22"><path d="M991.816611 674.909091a69.166545 69.166545 0 0 0-51.665455-23.272727 70.795636 70.795636 0 0 0-27.438545 5.585454A415.674182 415.674182 0 0 1 754.993338 698.181818c-209.594182 0-393.472-184.785455-393.472-395.636363 0-52.363636 38.539636-119.621818 69.515637-173.614546 4.887273-8.610909 9.634909-16.756364 14.103272-24.901818A69.818182 69.818182 0 0 0 384.631156 0a70.842182 70.842182 0 0 0-27.438545 5.585455C161.678429 90.298182 14.362065 307.898182 14.362065 512c0 282.298182 238.824727 512 532.38691 512a522.286545 522.286545 0 0 0 453.957818-268.334545A69.818182 69.818182 0 0 0 991.816611 674.909091zM546.679156 954.181818c-248.785455 0-462.941091-192-462.941091-442.181818 0-186.647273 140.637091-372.829091 300.939637-442.181818-36.817455 65.629091-92.578909 151.970909-92.578909 232.727273 0 250.181818 214.109091 465.454545 462.917818 465.454545a488.331636 488.331636 0 0 0 185.181091-46.545455 453.003636 453.003636 0 0 1-393.565091 232.727273z m103.656728-669.323636l-14.266182 83.781818a34.909091 34.909091 0 0 0 50.362182 36.770909l74.775272-39.563636 74.752 39.563636a36.142545 36.142545 0 0 0 16.174546 3.956364 34.909091 34.909091 0 0 0 34.210909-40.727273l-14.289455-83.781818 60.509091-59.345455a35.025455 35.025455 0 0 0-19.223272-59.578182l-83.61891-12.101818-37.376-76.101818a34.56 34.56 0 0 0-62.254545 0l-37.376 76.101818-83.618909 12.101818a34.909091 34.909091 0 0 0-19.246546 59.578182z m70.423272-64.698182a34.280727 34.280727 0 0 0 26.135273-19.083636l14.312727-29.090909 14.336 29.090909a34.257455 34.257455 0 0 0 26.135273 19.083636l32.046546 4.887273-23.272728 22.574545a35.234909 35.234909 0 0 0-10.007272 30.952727l5.46909 32.116364-28.625454-15.127273a34.490182 34.490182 0 0 0-32.302546 0l-28.695272 15.127273 5.469091-32.116364a35.141818 35.141818 0 0 0-9.984-30.952727l-23.272728-22.574545z" p-id="3855"></path></svg></button><button title="Light theme" class="__dumi-default-dark-sun "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4026" width="22" height="22"><path d="M915.2 476.16h-43.968c-24.704 0-44.736 16-44.736 35.84s20.032 35.904 44.736 35.904H915.2c24.768 0 44.8-16.064 44.8-35.904s-20.032-35.84-44.8-35.84zM512 265.6c-136.704 0-246.464 109.824-246.464 246.4 0 136.704 109.76 246.464 246.464 246.464S758.4 648.704 758.4 512c0-136.576-109.696-246.4-246.4-246.4z m0 425.6c-99.008 0-179.2-80.128-179.2-179.2 0-98.944 80.192-179.2 179.2-179.2S691.2 413.056 691.2 512c0 99.072-80.192 179.2-179.2 179.2zM197.44 512c0-19.84-19.136-35.84-43.904-35.84H108.8c-24.768 0-44.8 16-44.8 35.84s20.032 35.904 44.8 35.904h44.736c24.768 0 43.904-16.064 43.904-35.904zM512 198.464c19.776 0 35.84-20.032 35.84-44.8v-44.8C547.84 84.032 531.84 64 512 64s-35.904 20.032-35.904 44.8v44.8c0 24.768 16.128 44.864 35.904 44.864z m0 627.136c-19.776 0-35.904 20.032-35.904 44.8v44.736C476.096 940.032 492.16 960 512 960s35.84-20.032 35.84-44.8v-44.736c0-24.768-16.064-44.864-35.84-44.864z m329.92-592.832c17.472-17.536 20.288-43.072 6.4-57.024-14.016-14.016-39.488-11.2-57.024 6.336-4.736 4.864-26.496 26.496-31.36 31.36-17.472 17.472-20.288 43.008-6.336 57.024 13.952 14.016 39.488 11.2 57.024-6.336 4.8-4.864 26.496-26.56 31.296-31.36zM213.376 759.936c-4.864 4.8-26.56 26.624-31.36 31.36-17.472 17.472-20.288 42.944-6.4 56.96 14.016 13.952 39.552 11.2 57.024-6.336 4.8-4.736 26.56-26.496 31.36-31.36 17.472-17.472 20.288-43.008 6.336-56.96-14.016-13.952-39.552-11.072-56.96 6.336z m19.328-577.92c-17.536-17.536-43.008-20.352-57.024-6.336-14.08 14.016-11.136 39.488 6.336 57.024 4.864 4.864 26.496 26.56 31.36 31.424 17.536 17.408 43.008 20.288 56.96 6.336 14.016-14.016 11.264-39.488-6.336-57.024-4.736-4.864-26.496-26.56-31.296-31.424z m527.168 628.608c4.864 4.864 26.624 26.624 31.36 31.424 17.536 17.408 43.072 20.224 57.088 6.336 13.952-14.016 11.072-39.552-6.4-57.024-4.864-4.8-26.56-26.496-31.36-31.36-17.472-17.408-43.072-20.288-57.024-6.336-13.952 14.016-11.008 39.488 6.336 56.96z" p-id="4027"></path></svg></button><button title="Default to system" class="__dumi-default-dark-auto "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="11002" width="22" height="22"><path d="M127.658667 492.885333c0-51.882667 10.24-101.717333 30.378666-149.162666s47.786667-88.064 81.92-122.538667 75.093333-61.781333 122.538667-81.92 96.938667-30.378667 149.162667-30.378667 101.717333 10.24 149.162666 30.378667 88.405333 47.786667 122.88 81.92 61.781333 75.093333 81.92 122.538667 30.378667 96.938667 30.378667 149.162666-10.24 101.717333-30.378667 149.162667-47.786667 88.405333-81.92 122.88-75.093333 61.781333-122.88 81.92-97.28 30.378667-149.162666 30.378667-101.717333-10.24-149.162667-30.378667-88.064-47.786667-122.538667-81.92-61.781333-75.093333-81.92-122.88-30.378667-96.938667-30.378666-149.162667z m329.045333 0c0 130.048 13.994667 244.394667 41.984 343.381334h12.970667c46.762667 0 91.136-9.216 133.461333-27.306667s78.848-42.666667 109.568-73.386667 54.954667-67.242667 73.386667-109.568 27.306667-86.698667 27.306666-133.461333c0-46.421333-9.216-90.794667-27.306666-133.12s-42.666667-78.848-73.386667-109.568-67.242667-54.954667-109.568-73.386667-86.698667-27.306667-133.461333-27.306666h-11.605334c-28.672 123.562667-43.349333 237.909333-43.349333 343.722666z" p-id="11003"></path></svg></button></div></div></div><ul class="__dumi-default-menu-list"><li><a href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学">01.课前必学</a><ul><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/01"><span>开篇词 |  阅读源码，逐渐成了职业进阶道路上的“必选项”</span></a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/02"><span>导读 | 构建Kafka工程和源码阅读环境、Scala语言热身</span></a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/03"><span>重磅加餐 | 带你快速入门Scala语言</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块">02.日志模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/01"><span>01 | 日志段：保存消息文件的对象是怎么实现的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/02"><span>02 | 日志（上）：日志究竟是如何加载日志段的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/03"><span>03 | 日志（下）：彻底搞懂Log对象的常见操作</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/04"><span>04 | 索引（上）：改进的二分查找算法在Kafka索引的应用</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/05"><span>05 | 索引（下）：位移索引和时间戳索引的区别是什么？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块">03.请求处理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/01"><span>06 | 请求通道：如何实现Kafka请求队列？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/02"><span>07 | SocketServer（上）：Kafka到底是怎么应用NIO实现网络通信的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/03"><span>08 | SocketServer（中）：请求还要区分优先级？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/04"><span>09 | SocketServer（下）：请求处理全流程源码分析</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/05"><span>10 | KafkaApis：Kafka最重要的源码入口，没有之一</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块">04.Controller模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/01"><span>11 | Controller元数据：Controller都保存有哪些东西？有几种状态？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/02"><span>12 | ControllerChannelManager：Controller如何管理请求发送？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/03"><span>13 | ControllerEventManager：变身单线程后的Controller如何处理事件？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/04"><span>14 | Controller选举是怎么实现的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/05"><span>15 | 如何理解Controller在Kafka集群中的作用？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块">05.状态机模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/01"><span>16 | TopicDeletionManager： Topic是怎么被删除的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/02"><span>17 | ReplicaStateMachine：揭秘副本状态机实现原理</span></a></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/03"><span>18 | PartitionStateMachine：分区状态转换如何实现？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块">06.延迟操作模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块/01"><span>19 | TimingWheel：探究Kafka定时器背后的高效时间轮算法</span></a></li><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块/02"><span>20 | DelayedOperation：Broker是怎么延时处理请求的？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块">07.副本管理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/01"><span>21 | AbstractFetcherThread：拉取消息分几步？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/02"><span>22 | ReplicaFetcherThread：Follower如何拉取Leader消息？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/03"><span>23 | ReplicaManager（上）：必须要掌握的副本管理类定义和核心字段</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/04"><span>24 | ReplicaManager（中）：副本管理器是如何读写副本的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/05"><span>25 | ReplicaManager（下）：副本管理器是如何管理副本的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/06"><span>26 | MetadataCache：Broker是怎么异步更新元数据缓存的？</span></a></li></ul></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块">08.消费者组管理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/01"><span>27 | 消费者组元数据（上）：消费者组都有哪些元数据？</span></a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02"><span>28 | 消费者组元数据（下）：Kafka如何管理这些元数据？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03"><span>29 | GroupMetadataManager：组元数据管理器是个什么东西？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/04"><span>30 | GroupMetadataManager：位移主题保存的只是位移吗？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/05"><span>31 | GroupMetadataManager：查询位移时，不用读取位移主题？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/06"><span>32 | GroupCoordinator：在Rebalance中，Coordinator如何处理成员入组？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/07"><span>33 | GroupCoordinator：在Rebalance中，如何进行组同步？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送">09.特别放送</a><ul><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/01"><span>特别放送（一）| 经典的Kafka学习资料有哪些？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/02"><span>特别放送（二）| 一篇文章带你了解参与开源社区的全部流程</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/03"><span>特别放送（三）| 我是怎么度过日常一天的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/04"><span>特别放送（四）| 20道经典的Kafka面试题详解</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/05"><span>特别放送（五） | Kafka 社区的重磅功能：移除 ZooKeeper 依赖</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/10.测试题">10.测试题</a><ul><li><a href="/blog-backend/kafka核心源码解读/10.测试题/01"><span>期中测试 | 这些源码知识，你都掌握了吗？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/10.测试题/02"><span>期末测试 | 一套习题，测试你的掌握程度</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/11.结束语">11.结束语</a><ul><li><a href="/blog-backend/kafka核心源码解读/11.结束语/01"><span>结束语 | 源码学习，我们才刚上路呢</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/summary">kafka核心源码解读</a></li></ul></div></div><ul role="slug-list" class="__dumi-default-layout-toc"><li title="消费者组状态管理方法" data-depth="2"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#消费者组状态管理方法"><span>消费者组状态管理方法</span></a></li><li title="成员管理方法" data-depth="2"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#成员管理方法"><span>成员管理方法</span></a></li><li title="添加成员" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#添加成员"><span>添加成员</span></a></li><li title="移除成员" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#移除成员"><span>移除成员</span></a></li><li title="查询成员" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#查询成员"><span>查询成员</span></a></li><li title="位移管理方法" data-depth="2"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#位移管理方法"><span>位移管理方法</span></a></li><li title="添加位移值" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#添加位移值"><span>添加位移值</span></a></li><li title="移除位移值" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#移除位移值"><span>移除位移值</span></a></li><li title="分区分配策略管理方法" data-depth="2"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#分区分配策略管理方法"><span>分区分配策略管理方法</span></a></li><li title="确认消费者组支持的分区分配策略集" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#确认消费者组支持的分区分配策略集"><span>确认消费者组支持的分区分配策略集</span></a></li><li title="选出消费者组的分区消费分配策略" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#选出消费者组的分区消费分配策略"><span>选出消费者组的分区消费分配策略</span></a></li><li title="总结" data-depth="2"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#总结"><span>总结</span></a></li><li title="课后讨论" data-depth="2"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#课后讨论"><span>课后讨论</span></a></li></ul><div class="__dumi-default-layout-content"><div class="markdown"><h1 id="28--消费者组元数据下kafka如何管理这些元数据"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#28--消费者组元数据下kafka如何管理这些元数据"><span class="icon icon-link"></span></a>28 | 消费者组元数据（下）：Kafka如何管理这些元数据？</h1><p>你好，我是胡夕。今天我们继续学习消费者组元数据。</p><p>学完上节课之后，我们知道，Kafka定义了非常多的元数据，那么，这就必然涉及到对元数据的管理问题了。</p><p>这些元数据的类型不同，管理策略也就不一样。这节课，我将从消费者组状态、成员、位移和分区分配策略四个维度，对这些元数据进行拆解，带你一起了解下Kafka管理这些元数据的方法。</p><p>这些方法定义在MemberMetadata和GroupMetadata这两个类中，其中，GroupMetadata类中的方法最为重要，是我们要重点学习的对象。在后面的课程中，你会看到，这些方法会被上层组件GroupCoordinator频繁调用，因此，它们是我们学习Coordinator组件代码的前提条件，你一定要多花些精力搞懂它们。</p><h2 id="消费者组状态管理方法"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#消费者组状态管理方法"><span class="icon icon-link"></span></a>消费者组状态管理方法</h2><p>消费者组状态是很重要的一类元数据。管理状态的方法，要做的事情也就是设置和查询。这些方法大多比较简单，所以我把它们汇总在一起，直接介绍给你。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">// GroupMetadata.scala</span></div><div class="token-line"><span class="token plain">    // 设置/更新状态</span></div><div class="token-line"><span class="token plain">    def transitionTo(groupState: GroupState): Unit = {</span></div><div class="token-line"><span class="token plain">      assertValidTransition(groupState) // 确保是合法的状态转换</span></div><div class="token-line"><span class="token plain">      state = groupState  // 设置状态到给定状态</span></div><div class="token-line"><span class="token plain">      currentStateTimestamp = Some(time.milliseconds() // 更新状态变更时间戳</span></div><div class="token-line"><span class="token plain">    // 查询状态</span></div><div class="token-line"><span class="token plain">    def currentState = state</span></div><div class="token-line"><span class="token plain">    // 判断消费者组状态是指定状态</span></div><div class="token-line"><span class="token plain">    def is(groupState: GroupState) = state == groupState</span></div><div class="token-line"><span class="token plain">    // 判断消费者组状态不是指定状态</span></div><div class="token-line"><span class="token plain">    def not(groupState: GroupState) = state != groupState</span></div><div class="token-line"><span class="token plain">    // 消费者组能否Rebalance的条件是当前状态是PreparingRebalance状态的合法前置状态</span></div><div class="token-line"><span class="token plain">    def canRebalance = PreparingRebalance.validPreviousStates.contains(state)</span></div></pre></div><p><strong>1.transitionTo方法</strong></p><p>transitionTo方法的作用是<strong>将消费者组状态变更成给定状态</strong>。在变更前，代码需要确保这次变更必须是合法的状态转换。这是依靠每个GroupState实现类定义的<strong>validPreviousStates集合</strong>来完成的。只有在这个集合中的状态，才是合法的前置状态。简单来说，只有集合中的这些状态，才能转换到当前状态。</p><p>同时，该方法还会<strong>更新状态变更的时间戳字段</strong>。Kafka有个定时任务，会定期清除过期的消费者组位移数据，它就是依靠这个时间戳字段，来判断过期与否的。</p><p><strong>2.canRebalance方法</strong></p><p>它用于判断消费者组是否能够开启Rebalance操作。判断依据是，<strong>当前状态是否是PreparingRebalance状态的合法前置状态</strong>。只有<strong>Stable</strong>、<strong>CompletingRebalance</strong>和<strong>Empty</strong>这3类状态的消费者组，才有资格开启Rebalance。</p><p><strong>3.is和not方法</strong></p><p>至于is和not方法，它们分别判断消费者组的状态与给定状态吻合还是不吻合，主要被用于<strong>执行状态校验</strong>。特别是is方法，被大量用于上层调用代码中，执行各类消费者组管理任务的前置状态校验工作。</p><p>总体来说，管理消费者组状态数据，依靠的就是这些方法，还是很简单的吧？</p><h2 id="成员管理方法"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#成员管理方法"><span class="icon icon-link"></span></a>成员管理方法</h2><p>在介绍管理消费者组成员的方法之前，我先帮你回忆下GroupMetadata中保存成员的字段。GroupMetadata中使用members字段保存所有的成员信息。该字段是一个HashMap，Key是成员的member ID字段，Value是MemberMetadata类型，该类型保存了成员的元数据信息。</p><p>所谓的管理成员，也就是添加成员（add方法）、移除成员（remove方法）和查询成员（has、get、size方法等）。接下来，我们就逐一来学习。</p><h3 id="添加成员"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#添加成员"><span class="icon icon-link"></span></a>添加成员</h3><p>先说添加成员的方法：add。add方法的主要逻辑，是将成员对象添加到members字段，同时更新其他一些必要的元数据，比如Leader成员字段、分区分配策略支持票数等。下面是add方法的源码：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def add(member: MemberMetadata, callback: JoinCallback = null): Unit = {</span></div><div class="token-line"><span class="token plain">      // 如果是要添加的第一个消费者组成员</span></div><div class="token-line"><span class="token plain">      if (members.isEmpty)</span></div><div class="token-line"><span class="token plain">        // 就把该成员的procotolType设置为消费者组的protocolType</span></div><div class="token-line"><span class="token plain">        this.protocolType = Some(member.protocolType)</span></div><div class="token-line"><span class="token plain">      // 确保成员元数据中的groupId和组Id相同</span></div><div class="token-line"><span class="token plain">      assert(groupId == member.groupId)</span></div><div class="token-line"><span class="token plain">      // 确保成员元数据中的protoclType和组protocolType相同</span></div><div class="token-line"><span class="token plain">      assert(this.protocolType.orNull == member.protocolType)</span></div><div class="token-line"><span class="token plain">      // 确保该成员选定的分区分配策略与组选定的分区分配策略相匹配</span></div><div class="token-line"><span class="token plain">      assert(supportsProtocols(member.protocolType, MemberMetadata.plainProtocolSet(member.supportedProtocols)))</span></div><div class="token-line"><span class="token plain">      // 如果尚未选出Leader成员</span></div><div class="token-line"><span class="token plain">      if (leaderId.isEmpty)</span></div><div class="token-line"><span class="token plain">        // 把该成员设定为Leader成员</span></div><div class="token-line"><span class="token plain">        leaderId = Some(member.memberId)</span></div><div class="token-line"><span class="token plain">      // 将该成员添加进members</span></div><div class="token-line"><span class="token plain">      members.put(member.memberId, member)</span></div><div class="token-line"><span class="token plain">      // 更新分区分配策略支持票数</span></div><div class="token-line"><span class="token plain">      member.supportedProtocols.foreach{ case (protocol, _) =&gt; supportedProtocols(protocol) += 1 }</span></div><div class="token-line"><span class="token plain">      // 设置成员加入组后的回调逻辑</span></div><div class="token-line"><span class="token plain">      member.awaitingJoinCallback = callback</span></div><div class="token-line"><span class="token plain">      // 更新已加入组的成员数</span></div><div class="token-line"><span class="token plain">      if (member.isAwaitingJoin)</span></div><div class="token-line"><span class="token plain">        numMembersAwaitingJoin += 1</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>我再画一张流程图，帮助你更直观地理解这个方法的作用。</p><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimage67d5679b34908b9d3cf7e10905fcf96e69d5.281e4a8c.jpg" alt=""/></p><p>我再具体解释一下这个方法的执行逻辑。</p><p>第一步，add方法要判断members字段是否包含已有成员。如果没有，就说明要添加的成员是该消费者组的第一个成员，那么，就令该成员协议类型（protocolType）成为组的protocolType。我在上节课中讲过，对于普通的消费者而言，protocolType就是字符串&quot;consumer&quot;。如果不是首个成员，就进入到下一步。</p><p>第二步，add方法会连续进行三次校验，分别确保<strong>待添加成员的组ID、protocolType</strong>和组配置一致，以及该成员选定的分区分配策略与组选定的分区分配策略相匹配。如果这些校验有任何一个未通过，就会立即抛出异常。</p><p>第三步，判断消费者组的Leader成员是否已经选出了。如果还没有选出，就将该成员设置成Leader成员。当然了，如果Leader已经选出了，自然就不需要做这一步了。需要注意的是，这里的Leader和我们在学习副本管理器时学到的Leader副本是不同的概念。这里的Leader成员，是指<strong>消费者组下的一个成员</strong>。该成员负责为所有成员制定分区分配方案，制定方法的依据，就是消费者组选定的分区分配策略。</p><p>第四步，更新消费者组分区分配策略支持票数。关于supportedProtocols字段的含义，我在上节课的末尾用一个例子形象地进行了说明，这里就不再重复说了。如果你没有印象了，可以再复习一下。</p><p>最后一步，设置成员加入组后的回调逻辑，同时更新已加入组的成员数。至此，方法结束。</p><p>作为关键的成员管理方法之一，add方法是实现消费者组Rebalance流程至关重要的一环。每当Rebalance开启第一大步——加入组的操作时，本质上就是在利用这个add方法实现新成员入组的逻辑。</p><h3 id="移除成员"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#移除成员"><span class="icon icon-link"></span></a>移除成员</h3><p>有add方法，自然也就有remove方法，下面是remove方法的完整源码：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def remove(memberId: String): Unit = {</span></div><div class="token-line"><span class="token plain">      // 从members中移除给定成员</span></div><div class="token-line"><span class="token plain">      members.remove(memberId).foreach { member =&gt;</span></div><div class="token-line"><span class="token plain">        // 更新分区分配策略支持票数</span></div><div class="token-line"><span class="token plain">        member.supportedProtocols.foreach{ case (protocol, _) =&gt; supportedProtocols(protocol) -= 1 }</span></div><div class="token-line"><span class="token plain">        // 更新已加入组的成员数</span></div><div class="token-line"><span class="token plain">        if (member.isAwaitingJoin)</span></div><div class="token-line"><span class="token plain">          numMembersAwaitingJoin -= 1</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">      // 如果该成员是Leader，选择剩下成员列表中的第一个作为新的Leader成员</span></div><div class="token-line"><span class="token plain">      if (isLeader(memberId))</span></div><div class="token-line"><span class="token plain">        leaderId = members.keys.headOption</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>remove方法比add要简单一些。<strong>首先</strong>，代码从members中移除给定成员。<strong>之后</strong>，更新分区分配策略支持票数，以及更新已加入组的成员数。<strong>最后</strong>，代码判断该成员是否是Leader成员，如果是的话，就选择成员列表中尚存的第一个成员作为新的Leader成员。</p><h3 id="查询成员"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#查询成员"><span class="icon icon-link"></span></a>查询成员</h3><p>查询members的方法有很多，大多都是很简单的场景。我给你介绍3个比较常见的。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def has(memberId: String) = members.contains(memberId)</span></div><div class="token-line"><span class="token plain">    def get(memberId: String) = members(memberId)</span></div><div class="token-line"><span class="token plain">    def size = members.size</span></div></pre></div><ul><li>has方法，判断消费者组是否包含指定成员；</li><li>get方法，获取指定成员对象；</li><li>size方法，统计总成员数。</li></ul><p>其它的查询方法逻辑也都很简单，比如allMemberMetadata、rebalanceTimeoutMs，等等，我就不多讲了。课后你可以自行阅读下，重点是体会这些方法利用members都做了什么事情。</p><h2 id="位移管理方法"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#位移管理方法"><span class="icon icon-link"></span></a>位移管理方法</h2><p>除了组状态和成员管理之外，GroupMetadata还有一大类管理功能，就是<strong>管理消费者组的提交位移</strong>（Committed Offsets），主要包括添加和移除位移值。</p><p>不过，在学习管理位移的功能之前，我再带你回顾一下保存位移的offsets字段的定义。毕竟，接下来我们要学习的方法，主要操作的就是这个字段。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private val offsets = new mutable.HashMap[TopicPartition, CommitRecordMetadataAndOffset]</span></div></pre></div><p>它是HashMap类型，Key是TopicPartition类型，表示一个主题分区，而Value是CommitRecordMetadataAndOffset类型。该类封装了位移提交消息的位移值。</p><p>在详细阅读位移管理方法之前，我先解释下这里的“位移”和“位移提交消息”。</p><p>消费者组需要向Coordinator提交已消费消息的进度，在Kafka中，这个进度有个专门的术语，叫作提交位移。Kafka使用它来定位消费者组要消费的下一条消息。那么，提交位移在Coordinator端是如何保存的呢？它实际上是保存在内部位移主题中。提交的方式是，消费者组成员向内部主题写入符合特定格式的事件消息，这类消息就是所谓的位移提交消息（Commit Record）。关于位移提交消息的事件格式，我会在第30讲具体介绍，这里你可以暂时不用理会。而这里所说的CommitRecordMetadataAndOffset类，就是标识位移提交消息的地方。我们看下它的代码：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">case class CommitRecordMetadataAndOffset(appendedBatchOffset: Option[Long], offsetAndMetadata: OffsetAndMetadata) {</span></div><div class="token-line"><span class="token plain">      def olderThan(that: CommitRecordMetadataAndOffset): Boolean = appendedBatchOffset.get &lt; that.appendedBatchOffset.get</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>这个类的构造函数有两个参数。</p><ul><li>appendedBatchOffset：保存的是位移主题消息自己的位移值；</li><li>offsetAndMetadata：保存的是位移提交消息中保存的消费者组的位移值。</li></ul><h3 id="添加位移值"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#添加位移值"><span class="icon icon-link"></span></a>添加位移值</h3><p>在GroupMetadata中，有3个向offsets中添加订阅分区的已消费位移值的方法，分别是initializeOffsets、onOffsetCommitAppend和completePendingTxnOffsetCommit。</p><p>initializeOffsets方法的代码非常简单，如下所示：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def initializeOffsets(</span></div><div class="token-line"><span class="token plain">      offsets: collection.Map[TopicPartition, CommitRecordMetadataAndOffset],</span></div><div class="token-line"><span class="token plain">      pendingTxnOffsets: Map[Long, mutable.Map[TopicPartition, CommitRecordMetadataAndOffset]]): Unit = {</span></div><div class="token-line"><span class="token plain">      this.offsets ++= offsets</span></div><div class="token-line"><span class="token plain">      this.pendingTransactionalOffsetCommits ++= pendingTxnOffsets</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>它仅仅是将给定的一组订阅分区提交位移值加到offsets中。当然，同时它还会更新pendingTransactionalOffsetCommits字段。</p><p>不过，由于这个字段是给Kafka事务机制使用的，因此，你只需要关注这个方法的第一行语句就行了。当消费者组的协调者组件启动时，它会创建一个异步任务，定期地读取位移主题中相应消费者组的提交位移数据，并把它们加载到offsets字段中。</p><p>我们再来看第二个方法，onOffsetCommitAppend的代码。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def onOffsetCommitAppend(topicPartition: TopicPartition, offsetWithCommitRecordMetadata: CommitRecordMetadataAndOffset): Unit = {</span></div><div class="token-line"><span class="token plain">      if (pendingOffsetCommits.contains(topicPartition)) {</span></div><div class="token-line"><span class="token plain">        if (offsetWithCommitRecordMetadata.appendedBatchOffset.isEmpty)</span></div><div class="token-line"><span class="token plain">          throw new IllegalStateException(&quot;Cannot complete offset commit write without providing the metadata of the record &quot; +</span></div><div class="token-line"><span class="token plain">            &quot;in the log.&quot;)</span></div><div class="token-line"><span class="token plain">        // offsets字段中没有该分区位移提交数据，或者</span></div><div class="token-line"><span class="token plain">        // offsets字段中该分区对应的提交位移消息在位移主题中的位移值小于待写入的位移值</span></div><div class="token-line"><span class="token plain">        if (!offsets.contains(topicPartition) || offsets(topicPartition).olderThan(offsetWithCommitRecordMetadata))</span></div><div class="token-line"><span class="token plain">          // 将该分区对应的提交位移消息添加到offsets中</span></div><div class="token-line"><span class="token plain">          offsets.put(topicPartition, offsetWithCommitRecordMetadata)</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">      pendingOffsetCommits.get(topicPartition) match {</span></div><div class="token-line"><span class="token plain">        case Some(stagedOffset) if offsetWithCommitRecordMetadata.offsetAndMetadata == stagedOffset =&gt;</span></div><div class="token-line"><span class="token plain">          pendingOffsetCommits.remove(topicPartition)</span></div><div class="token-line"><span class="token plain">        case _ =&gt;</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>该方法在提交位移消息被成功写入后调用。主要判断的依据，是offsets中是否已包含该主题分区对应的消息值，或者说，offsets字段中该分区对应的提交位移消息在位移主题中的位移值是否小于待写入的位移值。如果是的话，就把该主题已提交的位移值添加到offsets中。</p><p>第三个方法completePendingTxnOffsetCommit的作用是完成一个待决事务（Pending Transaction）的位移提交。所谓的待决事务，就是指正在进行中、还没有完成的事务。在处理待决事务的过程中，可能会出现将待决事务中涉及到的分区的位移值添加到offsets中的情况。不过，由于该方法是与Kafka事务息息相关的，你不需要重点掌握，这里我就不展开说了。</p><h3 id="移除位移值"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#移除位移值"><span class="icon icon-link"></span></a>移除位移值</h3><p>offsets中订阅分区的已消费位移值也是能够被移除的。你还记得，Kafka主题中的消息有默认的留存时间设置吗？位移主题是普通的Kafka主题，所以也要遵守相应的规定。如果当前时间与已提交位移消息时间戳的差值，超过了Broker端参数offsets.retention.minutes值，Kafka就会将这条记录从offsets字段中移除。这就是方法removeExpiredOffsets要做的事情。</p><p>这个方法的代码有点长，为了方便你掌握，我分块给你介绍下。我先带你了解下它的内部嵌套类方法getExpireOffsets，然后再深入了解它的实现逻辑，这样你就能很轻松地掌握Kafka移除位移值的代码原理了。</p><p>首先，该方法定义了一个内部嵌套方法<strong>getExpiredOffsets</strong>，专门用于获取订阅分区过期的位移值。我们来阅读下源码，看看它是如何做到的。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def getExpiredOffsets(</span></div><div class="token-line"><span class="token plain">      baseTimestamp: CommitRecordMetadataAndOffset =&gt; Long,</span></div><div class="token-line"><span class="token plain">      subscribedTopics: Set[String] = Set.empty): Map[TopicPartition, OffsetAndMetadata] = {</span></div><div class="token-line"><span class="token plain">      // 遍历offsets中的所有分区，过滤出同时满足以下3个条件的所有分区</span></div><div class="token-line"><span class="token plain">      // 条件1：分区所属主题不在订阅主题列表之内</span></div><div class="token-line"><span class="token plain">      // 条件2：该主题分区已经完成位移提交</span></div><div class="token-line"><span class="token plain">      // 条件3：该主题分区在位移主题中对应消息的存在时间超过了阈值</span></div><div class="token-line"><span class="token plain">      offsets.filter {</span></div><div class="token-line"><span class="token plain">        case (topicPartition, commitRecordMetadataAndOffset) =&gt;</span></div><div class="token-line"><span class="token plain">          !subscribedTopics.contains(topicPartition.topic()) &amp;&amp;</span></div><div class="token-line"><span class="token plain">          !pendingOffsetCommits.contains(topicPartition) &amp;&amp; {</span></div><div class="token-line"><span class="token plain">            commitRecordMetadataAndOffset</span></div><div class="token-line"><span class="token plain">              .offsetAndMetadata.expireTimestamp match {</span></div><div class="token-line"><span class="token plain">              case None =&gt;</span></div><div class="token-line"><span class="token plain">                currentTimestamp - baseTimestamp(commitRecordMetadataAndOffset) &gt;= offsetRetentionMs</span></div><div class="token-line"><span class="token plain">              case Some(expireTimestamp) =&gt;</span></div><div class="token-line"><span class="token plain">                currentTimestamp &gt;= expireTimestamp</span></div><div class="token-line"><span class="token plain">            }</span></div><div class="token-line"><span class="token plain">          }</span></div><div class="token-line"><span class="token plain">      }.map {</span></div><div class="token-line"><span class="token plain">        // 为满足以上3个条件的分区提取出commitRecordMetadataAndOffset中的位移值</span></div><div class="token-line"><span class="token plain">        case (topicPartition, commitRecordOffsetAndMetadata) =&gt;</span></div><div class="token-line"><span class="token plain">          (topicPartition, commitRecordOffsetAndMetadata.offsetAndMetadata)</span></div><div class="token-line"><span class="token plain">      }.toMap</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>该方法接收两个参数。</p><ul><li>baseTimestamp：它是一个函数类型，接收CommitRecordMetadataAndOffset类型的字段，然后计算时间戳，并返回；</li><li>subscribedTopics：即订阅主题集合，默认是空。</li></ul><p>方法开始时，代码从offsets字段中过滤出同时满足3个条件的所有分区。</p><p><strong>条件1</strong>：分区所属主题不在订阅主题列表之内。当方法传入了不为空的主题集合时，就说明该消费者组此时正在消费中，正在消费的主题是不能执行过期位移移除的。</p><p><strong>条件2</strong>：主题分区已经完成位移提交，那种处于提交中状态，也就是保存在pendingOffsetCommits字段中的分区，不予考虑。</p><p><strong>条件3</strong>：该主题分区在位移主题中对应消息的存在时间超过了阈值。老版本的Kafka消息直接指定了过期时间戳，因此，只需要判断当前时间是否越过了这个过期时间。但是，目前，新版Kafka判断过期与否，主要是<strong>基于消费者组状态</strong>。如果是Empty状态，过期的判断依据就是当前时间与组变为Empty状态时间的差值，是否超过Broker端参数offsets.retention.minutes值；如果不是Empty状态，就看当前时间与提交位移消息中的时间戳差值是否超过了offsets.retention.minutes值。如果超过了，就视为已过期，对应的位移值需要被移除；如果没有超过，就不需要移除了。</p><p>当过滤出同时满足这3个条件的分区后，提取出它们对应的位移值对象并返回。</p><p>学过了getExpiredOffsets方法代码的实现之后，removeExpiredOffsets方法剩下的代码就很容易理解了。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def removeExpiredOffsets(</span></div><div class="token-line"><span class="token plain">      currentTimestamp: Long, offsetRetentionMs: Long): Map[TopicPartition, OffsetAndMetadata] = {</span></div><div class="token-line"><span class="token plain">      // getExpiredOffsets方法代码......</span></div><div class="token-line"><span class="token plain">      // 调用getExpiredOffsets方法获取主题分区的过期位移</span></div><div class="token-line"><span class="token plain">      val expiredOffsets: Map[TopicPartition, OffsetAndMetadata] = protocolType match {</span></div><div class="token-line"><span class="token plain">        case Some(_) if is(Empty) =&gt;</span></div><div class="token-line"><span class="token plain">          getExpiredOffsets(</span></div><div class="token-line"><span class="token plain">            commitRecordMetadataAndOffset =&gt; currentStateTimestamp    .getOrElse(commitRecordMetadataAndOffset.offsetAndMetadata.commitTimestamp)</span></div><div class="token-line"><span class="token plain">          )</span></div><div class="token-line"><span class="token plain">        case Some(ConsumerProtocol.PROTOCOL_TYPE) if subscribedTopics.isDefined =&gt;</span></div><div class="token-line"><span class="token plain">          getExpiredOffsets(</span></div><div class="token-line"><span class="token plain">            _.offsetAndMetadata.commitTimestamp,</span></div><div class="token-line"><span class="token plain">            subscribedTopics.get</span></div><div class="token-line"><span class="token plain">          )</span></div><div class="token-line"><span class="token plain">        case None =&gt;</span></div><div class="token-line"><span class="token plain">          getExpiredOffsets(_.offsetAndMetadata.commitTimestamp)</span></div><div class="token-line"><span class="token plain">        case _ =&gt;</span></div><div class="token-line"><span class="token plain">          Map()</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">      if (expiredOffsets.nonEmpty)</span></div><div class="token-line"><span class="token plain">        debug(s&quot;Expired offsets from group &#x27;$groupId&#x27;: ${expiredOffsets.keySet}&quot;)</span></div><div class="token-line"><span class="token plain">      // 将过期位移对应的主题分区从offsets中移除</span></div><div class="token-line"><span class="token plain">      offsets --= expiredOffsets.keySet</span></div><div class="token-line"><span class="token plain">      // 返回主题分区对应的过期位移</span></div><div class="token-line"><span class="token plain">      expiredOffsets</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>代码根据消费者组的protocolType类型和组状态调用getExpiredOffsets方法，同时决定传入什么样的参数：</p><ul><li>如果消费者组状态是Empty，就传入组变更为Empty状态的时间，若该时间没有被记录，则使用提交位移消息本身的写入时间戳，来获取过期位移；</li><li>如果是普通的消费者组类型，且订阅主题信息已知，就传入提交位移消息本身的写入时间戳和订阅主题集合共同确定过期位移值；</li><li>如果protocolType为None，就表示，这个消费者组其实是一个Standalone消费者，依然是传入提交位移消息本身的写入时间戳，来决定过期位移值；</li><li>如果消费者组的状态不符合刚刚说的这些情况，那就说明，没有过期位移值需要被移除。</li></ul><p>当确定了要被移除的位移值集合后，代码会将它们从offsets中移除，然后返回这些被移除的位移值信息。至此，方法结束。</p><h2 id="分区分配策略管理方法"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#分区分配策略管理方法"><span class="icon icon-link"></span></a>分区分配策略管理方法</h2><p>最后，我们讨论下消费者组分区分配策略的管理，也就是字段supportedProtocols的管理。supportedProtocols是分区分配策略的支持票数，这个票数在添加成员、移除成员时，会进行相应的更新。</p><p>消费者组每次Rebalance的时候，都要重新确认本次Rebalance结束之后，要使用哪个分区分配策略，因此，就需要特定的方法来对这些票数进行统计，把票数最多的那个策略作为新的策略。</p><p>GroupMetadata类中定义了两个方法来做这件事情，分别是candidateProtocols和selectProtocol方法。</p><h3 id="确认消费者组支持的分区分配策略集"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#确认消费者组支持的分区分配策略集"><span class="icon icon-link"></span></a>确认消费者组支持的分区分配策略集</h3><p>首先来看candidateProtocols方法。它的作用是<strong>找出组内所有成员都支持的分区分配策略集</strong>。代码如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private def candidateProtocols: Set[String] = {</span></div><div class="token-line"><span class="token plain">      val numMembers = members.size // 获取组内成员数</span></div><div class="token-line"><span class="token plain">      // 找出支持票数=总成员数的策略，返回它们的名称</span></div><div class="token-line"><span class="token plain">      supportedProtocols.filter(_._2 == numMembers).map(_._1).toSet</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>该方法首先会获取组内的总成员数，然后，找出supportedProtocols中那些支持票数等于总成员数的分配策略，并返回它们的名称。<strong>支持票数等于总成员数的意思，等同于所有成员都支持该策略</strong>。</p><h3 id="选出消费者组的分区消费分配策略"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#选出消费者组的分区消费分配策略"><span class="icon icon-link"></span></a>选出消费者组的分区消费分配策略</h3><p>接下来，我们看下selectProtocol方法，它的作用是<strong>选出消费者组的分区消费分配策略</strong>。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def selectProtocol: String = {</span></div><div class="token-line"><span class="token plain">      // 如果没有任何成员，自然无法确定选用哪个策略</span></div><div class="token-line"><span class="token plain">      if (members.isEmpty)</span></div><div class="token-line"><span class="token plain">        throw new IllegalStateException(&quot;Cannot select protocol for empty group&quot;)</span></div><div class="token-line"><span class="token plain">      // 获取所有成员都支持的策略集合</span></div><div class="token-line"><span class="token plain">      val candidates = candidateProtocols</span></div><div class="token-line"><span class="token plain">      // 让每个成员投票，票数最多的那个策略当选</span></div><div class="token-line"><span class="token plain">      val (protocol, _) = allMemberMetadata</span></div><div class="token-line"><span class="token plain">        .map(_.vote(candidates))</span></div><div class="token-line"><span class="token plain">        .groupBy(identity)</span></div><div class="token-line"><span class="token plain">        .maxBy { case (_, votes) =&gt; votes.size }</span></div><div class="token-line"><span class="token plain">      protocol</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>这个方法首先会判断组内是否有成员。如果没有任何成员，自然就无法确定选用哪个策略了，方法就会抛出异常，并退出。否则的话，代码会调用刚才的candidateProtocols方法，获取所有成员都支持的策略集合，然后让每个成员投票，票数最多的那个策略当选。</p><p>你可能会好奇，这里的vote方法是怎么实现的呢？其实，它就是简单地查找而已。我举一个简单的例子，来帮助你理解。</p><p>比如，candidates字段的值是[“策略A”，“策略B”]，成员1支持[“策略B”，“策略A”]，成员2支持[“策略A”，“策略B”，“策略C”]，成员3支持[“策略D”，“策略B”，“策略A”]，那么，vote方法会将candidates与每个成员的支持列表进行比对，找出成员支持列表中第一个包含在candidates中的策略。因此，对于这个例子来说，成员1投票策略B，成员2投票策略A，成员3投票策略B。可以看到，投票的结果是，策略B是两票，策略A是1票。所以，selectProtocol方法返回策略B作为新的策略。</p><p>有一点你需要注意，<strong>成员支持列表中的策略是有顺序的</strong>。这就是说，[“策略B”，“策略A”]和[“策略A”，“策略B”]是不同的，成员会倾向于选择靠前的策略。</p><h2 id="总结"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#总结"><span class="icon icon-link"></span></a>总结</h2><p>今天，我们结合GroupMetadata源码，学习了Kafka对消费者组元数据的管理，主要包括组状态、成员、位移和分区分配策略四个维度。我建议你在课下再仔细地阅读一下这些管理数据的方法，对照着源码和注释走一遍完整的操作流程。</p><p>另外，在这两节课中，我没有谈及待决成员列表（Pending Members）和待决位移（Pending Offsets）的管理，因为这两个元数据项属于中间临时状态，因此我没有展开讲，不理解这部分代码的话，也不会影响我们理解消费者组元数据以及Coordinator是如何使用它们的。不过，我建议你可以阅读下与它们相关的代码部分。要知道，Kafka是非常喜欢引用中间状态变量来管理各类元数据或状态的。</p><p>现在，我们再来简单回顾下这节课的重点。</p><ul><li>消费者组元数据管理：主要包括对组状态、成员、位移和分区分配策略的管理。</li><li>组状态管理：transitionTo方法负责设置状态，is、not和get方法用于查询状态。</li><li>成员管理：add、remove方法用于增减成员，has和get方法用于查询特定成员。</li><li>分区分配策略管理：定义了专属方法selectProtocols，用于在每轮Rebalance时选举分区分配策略。</li></ul><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimagea3e7a3eafee6b5d17b97f7661c24ccdcd4e7.f9dbdaa4.jpg" alt=""/></p><p>至此，我们花了两节课的时间，详细地学习了消费者组元数据及其管理方法的源码。这些操作元数据的方法被上层调用方GroupCoordinator大量使用，就像我在开头提到的，如果现在我们不彻底掌握这些元数据被操作的手法，等我们学到GroupCoordinator代码时，就会感到有些吃力，所以，你一定要好好地学习这两节课。有了这些基础，等到学习GroupCoordinator源码时，你就能更加深刻地理解它的底层实现原理了。</p><h2 id="课后讨论"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02#课后讨论"><span class="icon icon-link"></span></a>课后讨论</h2><p>在讲到MemberMetadata时，我说过，每个成员都有自己的Rebalance超时时间设置，那么，Kafka是怎么确认消费者组使用哪个成员的超时时间作为整个组的超时时间呢？</p><p>欢迎在留言区写下你的思考和答案，跟我交流讨论，也欢迎你把今天的内容分享给你的朋友。</p></div><div class="__dumi-default-layout-footer-meta"><a target="_blank" rel="noopener noreferrer" href="https://github.com/GGwujun/blog/edit/master/ssrc/kafka核心源码解读/08.消费者组管理模块/02.md">在 GitHub 上编辑此页<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><span data-updated-text="最后更新时间：">2023/9/27 11:15:40</span></div></div></div></div>
	<script>
  window.g_useSSR = true;
  window.g_initialProps = {};
	</script>

    <script>
      (function () {
        if (!location.port) {
          (function (i, s, o, g, r, a, m) {
            i["GoogleAnalyticsObject"] = r;
            (i[r] =
              i[r] ||
              function () {
                (i[r].q = i[r].q || []).push(arguments);
              }),
              (i[r].l = 1 * new Date());
            (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m);
          })(
            window,
            document,
            "script",
            "//www.google-analytics.com/analytics.js",
            "ga"
          );
          ga("create", "UA-149864185-1", "auto");
          ga("send", "pageview");
        }
      })();
    </script>
    <script src="/blog-backend/umi.e14e5a14.js"></script>
  </body>
</html>
