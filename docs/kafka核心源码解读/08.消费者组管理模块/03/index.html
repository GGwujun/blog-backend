<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"
    />
    <link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
    <link rel="stylesheet" href="/blog-backend/umi.3ec1f225.css" />
    <script>
      window.routerBase = "/blog-backend";
    </script>
    <script>
      //! umi version: 3.5.41
    </script>
    <script>
      !(function () {
        var e =
            navigator.cookieEnabled && void 0 !== window.localStorage
              ? localStorage.getItem("dumi:prefers-color")
              : "auto",
          o = window.matchMedia("(prefers-color-scheme: dark)").matches,
          t = ["light", "dark", "auto"];
        document.documentElement.setAttribute(
          "data-prefers-color",
          e === t[2] ? (o ? t[1] : t[0]) : t.indexOf(e) > -1 ? e : t[0]
        );
      })();
    </script>
    <title>
      29 | GroupMetadataManager：组元数据管理器是个什么东西？ - 大师兄
    </title>
  </head>
  <body>
    <div id="root"><div class="__dumi-default-layout" data-route="/kafka核心源码解读/08.消费者组管理模块/03" data-show-sidemenu="true" data-show-slugs="true" data-site-mode="true" data-gapless="false"><div class="__dumi-default-navbar" data-mode="site"><button class="__dumi-default-navbar-toggle"></button><a class="__dumi-default-navbar-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-backend/">大师兄</a><nav><div class="__dumi-default-search"><input type="search" class="__dumi-default-search-input" value=""/><ul></ul></div><span>后端开发<ul><li><a href="/blog-backend/go语言核心36讲">go语言核心36讲</a></li><li><a href="/blog-backend/go并发编程实战">go并发编程实战</a></li><li><a href="/blog-backend/go语言项目开发实战">go语言项目开发实战</a></li><li><a href="/blog-backend/kafka核心技术与实战">kafka核心技术与实战</a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/零基础学python">零基础学python</a></li><li><a href="/blog-backend/python核心技术与实战">python核心技术与实战</a></li><li><a href="/blog-backend/redis核心技术与实战">redis核心技术与实战</a></li><li><a href="/blog-backend/redis源码剖析与实战">redis源码剖析与实战</a></li><li><a href="/blog-backend/陈天rust编程第一课">陈天rust编程第一课</a></li><li><a href="/blog-backend/tonybaigo语言第一课">tonybaigo语言第一课</a></li><li><a href="/blog-backend/后端存储实战课">后端存储实战课</a></li><li><a href="/blog-backend/后端技术面试38讲">后端技术面试38讲</a></li><li><a href="/blog-backend/深入c语言和程序运行原理">深入c语言和程序运行原理</a></li><li><a href="/blog-backend/现代c编程实战">现代c编程实战</a></li><li><a href="/blog-backend/罗剑锋的c实战笔记">罗剑锋的c实战笔记</a></li><li><a href="/blog-backend/零基础入门spark">零基础入门spark</a></li></ul></span><span>架构师<ul><li><a href="/blog-backend/mysql实战45讲">mysql实战45讲</a></li><li><a href="/blog-backend/数据中台实战课">数据中台实战课</a></li></ul></span><div class="__dumi-default-navbar-tool"><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "></div></div></div></nav></div><div class="__dumi-default-menu" data-mode="site"><div class="__dumi-default-menu-inner"><div class="__dumi-default-menu-header"><a class="__dumi-default-menu-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-backend/"></a><h1>大师兄</h1><p></p></div><div class="__dumi-default-menu-mobile-area"><ul class="__dumi-default-menu-nav-list"><li>后端开发<ul><li><a href="/blog-backend/go语言核心36讲">go语言核心36讲</a></li><li><a href="/blog-backend/go并发编程实战">go并发编程实战</a></li><li><a href="/blog-backend/go语言项目开发实战">go语言项目开发实战</a></li><li><a href="/blog-backend/kafka核心技术与实战">kafka核心技术与实战</a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/零基础学python">零基础学python</a></li><li><a href="/blog-backend/python核心技术与实战">python核心技术与实战</a></li><li><a href="/blog-backend/redis核心技术与实战">redis核心技术与实战</a></li><li><a href="/blog-backend/redis源码剖析与实战">redis源码剖析与实战</a></li><li><a href="/blog-backend/陈天rust编程第一课">陈天rust编程第一课</a></li><li><a href="/blog-backend/tonybaigo语言第一课">tonybaigo语言第一课</a></li><li><a href="/blog-backend/后端存储实战课">后端存储实战课</a></li><li><a href="/blog-backend/后端技术面试38讲">后端技术面试38讲</a></li><li><a href="/blog-backend/深入c语言和程序运行原理">深入c语言和程序运行原理</a></li><li><a href="/blog-backend/现代c编程实战">现代c编程实战</a></li><li><a href="/blog-backend/罗剑锋的c实战笔记">罗剑锋的c实战笔记</a></li><li><a href="/blog-backend/零基础入门spark">零基础入门spark</a></li></ul></li><li>架构师<ul><li><a href="/blog-backend/mysql实战45讲">mysql实战45讲</a></li><li><a href="/blog-backend/数据中台实战课">数据中台实战课</a></li></ul></li></ul><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "><button title="Dark theme" class="__dumi-default-dark-moon "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3854" width="22" height="22"><path d="M991.816611 674.909091a69.166545 69.166545 0 0 0-51.665455-23.272727 70.795636 70.795636 0 0 0-27.438545 5.585454A415.674182 415.674182 0 0 1 754.993338 698.181818c-209.594182 0-393.472-184.785455-393.472-395.636363 0-52.363636 38.539636-119.621818 69.515637-173.614546 4.887273-8.610909 9.634909-16.756364 14.103272-24.901818A69.818182 69.818182 0 0 0 384.631156 0a70.842182 70.842182 0 0 0-27.438545 5.585455C161.678429 90.298182 14.362065 307.898182 14.362065 512c0 282.298182 238.824727 512 532.38691 512a522.286545 522.286545 0 0 0 453.957818-268.334545A69.818182 69.818182 0 0 0 991.816611 674.909091zM546.679156 954.181818c-248.785455 0-462.941091-192-462.941091-442.181818 0-186.647273 140.637091-372.829091 300.939637-442.181818-36.817455 65.629091-92.578909 151.970909-92.578909 232.727273 0 250.181818 214.109091 465.454545 462.917818 465.454545a488.331636 488.331636 0 0 0 185.181091-46.545455 453.003636 453.003636 0 0 1-393.565091 232.727273z m103.656728-669.323636l-14.266182 83.781818a34.909091 34.909091 0 0 0 50.362182 36.770909l74.775272-39.563636 74.752 39.563636a36.142545 36.142545 0 0 0 16.174546 3.956364 34.909091 34.909091 0 0 0 34.210909-40.727273l-14.289455-83.781818 60.509091-59.345455a35.025455 35.025455 0 0 0-19.223272-59.578182l-83.61891-12.101818-37.376-76.101818a34.56 34.56 0 0 0-62.254545 0l-37.376 76.101818-83.618909 12.101818a34.909091 34.909091 0 0 0-19.246546 59.578182z m70.423272-64.698182a34.280727 34.280727 0 0 0 26.135273-19.083636l14.312727-29.090909 14.336 29.090909a34.257455 34.257455 0 0 0 26.135273 19.083636l32.046546 4.887273-23.272728 22.574545a35.234909 35.234909 0 0 0-10.007272 30.952727l5.46909 32.116364-28.625454-15.127273a34.490182 34.490182 0 0 0-32.302546 0l-28.695272 15.127273 5.469091-32.116364a35.141818 35.141818 0 0 0-9.984-30.952727l-23.272728-22.574545z" p-id="3855"></path></svg></button><button title="Light theme" class="__dumi-default-dark-sun "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4026" width="22" height="22"><path d="M915.2 476.16h-43.968c-24.704 0-44.736 16-44.736 35.84s20.032 35.904 44.736 35.904H915.2c24.768 0 44.8-16.064 44.8-35.904s-20.032-35.84-44.8-35.84zM512 265.6c-136.704 0-246.464 109.824-246.464 246.4 0 136.704 109.76 246.464 246.464 246.464S758.4 648.704 758.4 512c0-136.576-109.696-246.4-246.4-246.4z m0 425.6c-99.008 0-179.2-80.128-179.2-179.2 0-98.944 80.192-179.2 179.2-179.2S691.2 413.056 691.2 512c0 99.072-80.192 179.2-179.2 179.2zM197.44 512c0-19.84-19.136-35.84-43.904-35.84H108.8c-24.768 0-44.8 16-44.8 35.84s20.032 35.904 44.8 35.904h44.736c24.768 0 43.904-16.064 43.904-35.904zM512 198.464c19.776 0 35.84-20.032 35.84-44.8v-44.8C547.84 84.032 531.84 64 512 64s-35.904 20.032-35.904 44.8v44.8c0 24.768 16.128 44.864 35.904 44.864z m0 627.136c-19.776 0-35.904 20.032-35.904 44.8v44.736C476.096 940.032 492.16 960 512 960s35.84-20.032 35.84-44.8v-44.736c0-24.768-16.064-44.864-35.84-44.864z m329.92-592.832c17.472-17.536 20.288-43.072 6.4-57.024-14.016-14.016-39.488-11.2-57.024 6.336-4.736 4.864-26.496 26.496-31.36 31.36-17.472 17.472-20.288 43.008-6.336 57.024 13.952 14.016 39.488 11.2 57.024-6.336 4.8-4.864 26.496-26.56 31.296-31.36zM213.376 759.936c-4.864 4.8-26.56 26.624-31.36 31.36-17.472 17.472-20.288 42.944-6.4 56.96 14.016 13.952 39.552 11.2 57.024-6.336 4.8-4.736 26.56-26.496 31.36-31.36 17.472-17.472 20.288-43.008 6.336-56.96-14.016-13.952-39.552-11.072-56.96 6.336z m19.328-577.92c-17.536-17.536-43.008-20.352-57.024-6.336-14.08 14.016-11.136 39.488 6.336 57.024 4.864 4.864 26.496 26.56 31.36 31.424 17.536 17.408 43.008 20.288 56.96 6.336 14.016-14.016 11.264-39.488-6.336-57.024-4.736-4.864-26.496-26.56-31.296-31.424z m527.168 628.608c4.864 4.864 26.624 26.624 31.36 31.424 17.536 17.408 43.072 20.224 57.088 6.336 13.952-14.016 11.072-39.552-6.4-57.024-4.864-4.8-26.56-26.496-31.36-31.36-17.472-17.408-43.072-20.288-57.024-6.336-13.952 14.016-11.008 39.488 6.336 56.96z" p-id="4027"></path></svg></button><button title="Default to system" class="__dumi-default-dark-auto "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="11002" width="22" height="22"><path d="M127.658667 492.885333c0-51.882667 10.24-101.717333 30.378666-149.162666s47.786667-88.064 81.92-122.538667 75.093333-61.781333 122.538667-81.92 96.938667-30.378667 149.162667-30.378667 101.717333 10.24 149.162666 30.378667 88.405333 47.786667 122.88 81.92 61.781333 75.093333 81.92 122.538667 30.378667 96.938667 30.378667 149.162666-10.24 101.717333-30.378667 149.162667-47.786667 88.405333-81.92 122.88-75.093333 61.781333-122.88 81.92-97.28 30.378667-149.162666 30.378667-101.717333-10.24-149.162667-30.378667-88.064-47.786667-122.538667-81.92-61.781333-75.093333-81.92-122.88-30.378667-96.938667-30.378666-149.162667z m329.045333 0c0 130.048 13.994667 244.394667 41.984 343.381334h12.970667c46.762667 0 91.136-9.216 133.461333-27.306667s78.848-42.666667 109.568-73.386667 54.954667-67.242667 73.386667-109.568 27.306667-86.698667 27.306666-133.461333c0-46.421333-9.216-90.794667-27.306666-133.12s-42.666667-78.848-73.386667-109.568-67.242667-54.954667-109.568-73.386667-86.698667-27.306667-133.461333-27.306666h-11.605334c-28.672 123.562667-43.349333 237.909333-43.349333 343.722666z" p-id="11003"></path></svg></button></div></div></div><ul class="__dumi-default-menu-list"><li><a href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学">01.课前必学</a><ul><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/01"><span>开篇词 |  阅读源码，逐渐成了职业进阶道路上的“必选项”</span></a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/02"><span>导读 | 构建Kafka工程和源码阅读环境、Scala语言热身</span></a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/03"><span>重磅加餐 | 带你快速入门Scala语言</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块">02.日志模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/01"><span>01 | 日志段：保存消息文件的对象是怎么实现的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/02"><span>02 | 日志（上）：日志究竟是如何加载日志段的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/03"><span>03 | 日志（下）：彻底搞懂Log对象的常见操作</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/04"><span>04 | 索引（上）：改进的二分查找算法在Kafka索引的应用</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/05"><span>05 | 索引（下）：位移索引和时间戳索引的区别是什么？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块">03.请求处理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/01"><span>06 | 请求通道：如何实现Kafka请求队列？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/02"><span>07 | SocketServer（上）：Kafka到底是怎么应用NIO实现网络通信的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/03"><span>08 | SocketServer（中）：请求还要区分优先级？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/04"><span>09 | SocketServer（下）：请求处理全流程源码分析</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/05"><span>10 | KafkaApis：Kafka最重要的源码入口，没有之一</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块">04.Controller模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/01"><span>11 | Controller元数据：Controller都保存有哪些东西？有几种状态？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/02"><span>12 | ControllerChannelManager：Controller如何管理请求发送？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/03"><span>13 | ControllerEventManager：变身单线程后的Controller如何处理事件？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/04"><span>14 | Controller选举是怎么实现的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/05"><span>15 | 如何理解Controller在Kafka集群中的作用？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块">05.状态机模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/01"><span>16 | TopicDeletionManager： Topic是怎么被删除的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/02"><span>17 | ReplicaStateMachine：揭秘副本状态机实现原理</span></a></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/03"><span>18 | PartitionStateMachine：分区状态转换如何实现？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块">06.延迟操作模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块/01"><span>19 | TimingWheel：探究Kafka定时器背后的高效时间轮算法</span></a></li><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块/02"><span>20 | DelayedOperation：Broker是怎么延时处理请求的？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块">07.副本管理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/01"><span>21 | AbstractFetcherThread：拉取消息分几步？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/02"><span>22 | ReplicaFetcherThread：Follower如何拉取Leader消息？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/03"><span>23 | ReplicaManager（上）：必须要掌握的副本管理类定义和核心字段</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/04"><span>24 | ReplicaManager（中）：副本管理器是如何读写副本的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/05"><span>25 | ReplicaManager（下）：副本管理器是如何管理副本的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/06"><span>26 | MetadataCache：Broker是怎么异步更新元数据缓存的？</span></a></li></ul></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块">08.消费者组管理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/01"><span>27 | 消费者组元数据（上）：消费者组都有哪些元数据？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02"><span>28 | 消费者组元数据（下）：Kafka如何管理这些元数据？</span></a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03"><span>29 | GroupMetadataManager：组元数据管理器是个什么东西？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/04"><span>30 | GroupMetadataManager：位移主题保存的只是位移吗？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/05"><span>31 | GroupMetadataManager：查询位移时，不用读取位移主题？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/06"><span>32 | GroupCoordinator：在Rebalance中，Coordinator如何处理成员入组？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/07"><span>33 | GroupCoordinator：在Rebalance中，如何进行组同步？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送">09.特别放送</a><ul><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/01"><span>特别放送（一）| 经典的Kafka学习资料有哪些？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/02"><span>特别放送（二）| 一篇文章带你了解参与开源社区的全部流程</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/03"><span>特别放送（三）| 我是怎么度过日常一天的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/04"><span>特别放送（四）| 20道经典的Kafka面试题详解</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/05"><span>特别放送（五） | Kafka 社区的重磅功能：移除 ZooKeeper 依赖</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/10.测试题">10.测试题</a><ul><li><a href="/blog-backend/kafka核心源码解读/10.测试题/01"><span>期中测试 | 这些源码知识，你都掌握了吗？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/10.测试题/02"><span>期末测试 | 一套习题，测试你的掌握程度</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/11.结束语">11.结束语</a><ul><li><a href="/blog-backend/kafka核心源码解读/11.结束语/01"><span>结束语 | 源码学习，我们才刚上路呢</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/summary">kafka核心源码解读</a></li></ul></div></div><ul role="slug-list" class="__dumi-default-layout-toc"><li title="消费者组元数据管理" data-depth="2"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#消费者组元数据管理"><span>消费者组元数据管理</span></a></li><li title="查询获取消费者组元数据" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#查询获取消费者组元数据"><span>查询获取消费者组元数据</span></a></li><li title="移除消费者组元数据" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#移除消费者组元数据"><span>移除消费者组元数据</span></a></li><li title="添加消费者组元数据" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#添加消费者组元数据"><span>添加消费者组元数据</span></a></li><li title="加载消费者组元数据" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#加载消费者组元数据"><span>加载消费者组元数据</span></a></li><li title="消费者组位移管理" data-depth="2"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#消费者组位移管理"><span>消费者组位移管理</span></a></li><li title="保存消费者组位移" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#保存消费者组位移"><span>保存消费者组位移</span></a></li><li title="查询消费者组位移" data-depth="3"><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#查询消费者组位移"><span>查询消费者组位移</span></a></li></ul><div class="__dumi-default-layout-content"><div class="markdown"><h1 id="29--groupmetadatamanager组元数据管理器是个什么东西"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#29--groupmetadatamanager组元数据管理器是个什么东西"><span class="icon icon-link"></span></a>29 | GroupMetadataManager：组元数据管理器是个什么东西？</h1><p>你好，我是胡夕。今天，我们学习GroupMetadataManager类的源码。从名字上来看，它是组元数据管理器，但是，从它提供的功能来看，我更愿意将它称作消费者组管理器，因为它定义的方法，提供的都是添加消费者组、移除组、查询组这样组级别的基础功能。</p><p>不过，这个类的知名度不像KafkaController、GroupCoordinator那么高，你之前可能都没有听说过它。但是，它其实是非常重要的消费者组管理类。</p><p>GroupMetadataManager类是在消费者组Coordinator组件被创建时被实例化的。这就是说，每个Broker在启动过程中，都会创建并维持一个GroupMetadataManager实例，以实现对该Broker负责的消费者组进行管理。更重要的是，生产环境输出日志中的与消费者组相关的大多数信息，都和它息息相关。</p><p>我举一个简单的例子。你应该见过这样的日志输出：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">Removed ××× expired offsets in ××× milliseconds.</span></div></pre></div><p>这条日志每10分钟打印一次。你有没有想过，它为什么要这么操作呢？其实，这是由GroupMetadataManager类创建的定时任务引发的。如果你不清楚GroupMetadataManager的原理，虽然暂时不会影响你使用，但是，一旦你在实际环境中看到了有关消费者组的错误日志，仅凭日志输出，你是无法定位错误原因的。要解决这个问题，就只有一个办法：<strong>通过阅读源码，彻底搞懂底层实现原理，做到以不变应万变</strong>。</p><p>关于这个类，最重要的就是要掌握它是如何管理消费者组的，以及它对内部位移主题的操作方法。这两个都是重磅功能，我们必须要吃透它们的原理，这也是我们这三节课的学习重点。今天，我们先学习它的类定义和管理消费者组的方法。</p><h1 id="类定义与字段"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#类定义与字段"><span class="icon icon-link"></span></a>类定义与字段</h1><p>GroupMetadataManager类定义在coordinator.group包下的同名scala文件中。这个类的代码将近1000行，逐行分析的话，显然效率不高，也没有必要。所以，我从类定义和字段、重要方法两个维度给出主要逻辑的代码分析。下面的代码是该类的定义，以及我选取的重要字段信息。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">// brokerId：所在Broker的Id</span></div><div class="token-line"><span class="token plain">    // interBrokerProtocolVersion：Broker端参数inter.broker.protocol.version值</span></div><div class="token-line"><span class="token plain">    // config: 内部位移主题配置类</span></div><div class="token-line"><span class="token plain">    // replicaManager: 副本管理器类</span></div><div class="token-line"><span class="token plain">    // zkClient: ZooKeeper客户端</span></div><div class="token-line"><span class="token plain">    class GroupMetadataManager(</span></div><div class="token-line"><span class="token plain">      brokerId: Int, </span></div><div class="token-line"><span class="token plain">      interBrokerProtocolVersion: ApiVersion,</span></div><div class="token-line"><span class="token plain">      config: OffsetConfig, </span></div><div class="token-line"><span class="token plain">      replicaManager: ReplicaManager, </span></div><div class="token-line"><span class="token plain">      zkClient: KafkaZkClient, </span></div><div class="token-line"><span class="token plain">      time: Time,</span></div><div class="token-line"><span class="token plain">      metrics: Metrics) extends Logging with KafkaMetricsGroup {</span></div><div class="token-line"><span class="token plain">      // 压缩器类型。向位移主题写入消息时执行压缩操作</span></div><div class="token-line"><span class="token plain">      private val compressionType: CompressionType = CompressionType.forId(config.offsetsTopicCompressionCodec.codec)</span></div><div class="token-line"><span class="token plain">      // 消费者组元数据容器，保存Broker管理的所有消费者组的数据</span></div><div class="token-line"><span class="token plain">      private val groupMetadataCache = new Pool[String, GroupMetadata]</span></div><div class="token-line"><span class="token plain">      // 位移主题下正在执行加载操作的分区</span></div><div class="token-line"><span class="token plain">      private val loadingPartitions: mutable.Set[Int] = mutable.Set()</span></div><div class="token-line"><span class="token plain">      // 位移主题下完成加载操作的分区</span></div><div class="token-line"><span class="token plain">      private val ownedPartitions: mutable.Set[Int] = mutable.Set()</span></div><div class="token-line"><span class="token plain">      // 位移主题总分区数</span></div><div class="token-line"><span class="token plain">      private val groupMetadataTopicPartitionCount = getGroupMetadataTopicPartitionCount</span></div><div class="token-line"><span class="token plain">      ......</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>这个类的构造函数需要7个参数，后面的time和metrics只是起辅助作用，因此，我重点解释一下前5个参数的含义。</p><ul><li>brokerId：这个参数我们已经无比熟悉了。它是所在Broker的ID值，也就是broker.id参数值。</li><li>interBrokerProtocolVersion：保存Broker间通讯使用的请求版本。它是Broker端参数inter.broker.protocol.version值。这个参数的主要用途是<strong>确定位移主题消息格式的版本</strong>。</li><li>config：这是一个OffsetConfig类型。该类型定义了与位移管理相关的重要参数，比如位移主题日志段大小设置、位移主题备份因子、位移主题分区数配置等。</li><li>replicaManager：副本管理器类。GroupMetadataManager类使用该字段实现获取分区对象、日志对象以及写入分区消息的目的。</li><li>zkClient：ZooKeeper客户端。该类中的此字段只有一个目的：从ZooKeeper中获取位移主题的分区数。</li></ul><p>除了构造函数所需的字段，该类还定义了其他关键字段，我给你介绍几个非常重要的。</p><p><strong>1.compressionType</strong></p><p><strong>压缩器类型</strong>。Kafka向位移主题写入消息前，可以选择对消息执行压缩操作。是否压缩，取决于Broker端参数offsets.topic.compression.codec值，默认是不进行压缩。如果你的位移主题占用的磁盘空间比较多的话，可以考虑启用压缩，以节省资源。</p><p><strong>2.groupMetadataCache</strong></p><p>**该字段是GroupMetadataManager类上最重要的属性，它****保存这个Broker上GroupCoordinator组件管理的所有消费者组元数据。**它的Key是消费者组名称，Value是消费者组元数据，也就是GroupMetadata。源码通过该字段实现对消费者组的添加、删除和遍历操作。</p><p><strong>3.loadingPartitions</strong></p><p><strong>位移主题下正在执行加载操作的分区号集合</strong>。这里需要注意两点：首先，这些分区都是位移主题分区，也就是__consumer_offsets主题下的分区；其次，所谓的加载，是指读取位移主题消息数据，填充GroupMetadataCache字段的操作。</p><p><strong>4.ownedPartitions</strong></p><p><strong>位移主题下完成加载操作的分区号集合</strong>。与loadingPartitions类似的是，该字段保存的分区也是位移主题下的分区。和loadingPartitions不同的是，它保存的分区都是<strong>已经完成加载操作</strong>的分区。</p><p><strong>5.groupMetadataTopicPartitionCount</strong></p><p><strong>位移主题的分区数</strong>。它是Broker端参数offsets.topic.num.partitions的值，默认是50个分区。若要修改分区数，除了变更该参数值之外，你也可以手动创建位移主题，并指定不同的分区数。</p><p>在这些字段中，groupMetadataCache是最重要的，GroupMetadataManager类大量使用该字段实现对消费者组的管理。接下来，我们就重点学习一下该类是如何管理消费者组的。</p><h1 id="重要方法"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#重要方法"><span class="icon icon-link"></span></a>重要方法</h1><p>管理消费者组包含两个方面，对消费者组元数据的管理以及对消费者组位移的管理。组元数据和组位移都是Coordinator端重要的消费者组管理对象。</p><h2 id="消费者组元数据管理"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#消费者组元数据管理"><span class="icon icon-link"></span></a>消费者组元数据管理</h2><p>消费者组元数据管理分为查询获取组信息、添加组、移除组和加载组信息。从代码复杂度来讲，查询获取、移除和添加的逻辑相对简单，加载的过程稍微费事些。我们先说说查询获取。</p><h3 id="查询获取消费者组元数据"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#查询获取消费者组元数据"><span class="icon icon-link"></span></a>查询获取消费者组元数据</h3><p>GroupMetadataManager类中查询及获取组数据的方法有很多。大多逻辑简单，你一看就能明白，比如下面的getGroup方法和getOrMaybeCreateGroup方法：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">// getGroup方法：返回给定消费者组的元数据信息。</span></div><div class="token-line"><span class="token plain">    // 若该组信息不存在，返回None</span></div><div class="token-line"><span class="token plain">    def getGroup(groupId: String): Option[GroupMetadata] = {</span></div><div class="token-line"><span class="token plain">      Option(groupMetadataCache.get(groupId))</span></div><div class="token-line"><span class="token plain">    }</span></div><div class="token-line"><span class="token plain">    // getOrMaybeCreateGroup方法：返回给定消费者组的元数据信息。</span></div><div class="token-line"><span class="token plain">    // 若不存在，则视createIfNotExist参数值决定是否需要添加该消费者组</span></div><div class="token-line"><span class="token plain">    def getOrMaybeCreateGroup(groupId: String, createIfNotExist: Boolean): Option[GroupMetadata] = {</span></div><div class="token-line"><span class="token plain">      if (createIfNotExist)</span></div><div class="token-line"><span class="token plain">        // 若不存在且允许添加，则添加一个状态是Empty的消费者组元数据对象</span></div><div class="token-line"><span class="token plain">        Option(groupMetadataCache.getAndMaybePut(groupId, new GroupMetadata(groupId, Empty, time)))</span></div><div class="token-line"><span class="token plain">      else</span></div><div class="token-line"><span class="token plain">        Option(groupMetadataCache.get(groupId))</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>GroupMetadataManager类的上层组件GroupCoordinator会大量使用这两个方法来获取给定消费者组的数据。这两个方法都会返回给定消费者组的元数据信息，但是它们之间是有区别的。</p><p>对于getGroup方法而言，如果该组信息不存在，就返回None，而这通常表明，消费者组确实不存在，或者是，该组对应的Coordinator组件变更到其他Broker上了。</p><p>而对于getOrMaybeCreateGroup方法而言，若组信息不存在，就根据createIfNotExist参数值决定是否需要添加该消费者组。而且，getOrMaybeCreateGroup方法是在消费者组第一个成员加入组时被调用的，用于把组创建出来。</p><p>在GroupMetadataManager类中，还有一些地方也散落着组查询获取的逻辑。不过它们与这两个方法中的代码大同小异，很容易理解，课下你可以自己阅读下。</p><h3 id="移除消费者组元数据"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#移除消费者组元数据"><span class="icon icon-link"></span></a>移除消费者组元数据</h3><p>接下来，我们看下如何移除消费者组信息。当Broker卸任某些消费者组的Coordinator角色时，它需要将这些消费者组从groupMetadataCache中全部移除掉，这就是removeGroupsForPartition方法要做的事情。我们看下它的源码：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def removeGroupsForPartition(offsetsPartition: Int,</span></div><div class="token-line"><span class="token plain">                                 onGroupUnloaded: GroupMetadata =&gt; Unit): Unit = {</span></div><div class="token-line"><span class="token plain">      // 位移主题分区</span></div><div class="token-line"><span class="token plain">      val topicPartition = new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, offsetsPartition)</span></div><div class="token-line"><span class="token plain">      info(s&quot;Scheduling unloading of offsets and group metadata from $topicPartition&quot;)</span></div><div class="token-line"><span class="token plain">      // 创建异步任务，移除组信息和位移信息</span></div><div class="token-line"><span class="token plain">      scheduler.schedule(topicPartition.toString, () =&gt; removeGroupsAndOffsets)</span></div><div class="token-line"><span class="token plain">      // 内部方法，用于移除组信息和位移信息</span></div><div class="token-line"><span class="token plain">      def removeGroupsAndOffsets(): Unit = {</span></div><div class="token-line"><span class="token plain">        var numOffsetsRemoved = 0</span></div><div class="token-line"><span class="token plain">        var numGroupsRemoved = 0</span></div><div class="token-line"><span class="token plain">        inLock(partitionLock) {</span></div><div class="token-line"><span class="token plain">          // 移除ownedPartitions中特定位移主题分区记录</span></div><div class="token-line"><span class="token plain">          ownedPartitions.remove(offsetsPartition)</span></div><div class="token-line"><span class="token plain">          // 遍历所有消费者组信息</span></div><div class="token-line"><span class="token plain">          for (group &lt;- groupMetadataCache.values) {</span></div><div class="token-line"><span class="token plain">            // 如果该组信息保存在特定位移主题分区中</span></div><div class="token-line"><span class="token plain">            if (partitionFor(group.groupId) == offsetsPartition) {</span></div><div class="token-line"><span class="token plain">              // 执行组卸载逻辑</span></div><div class="token-line"><span class="token plain">              onGroupUnloaded(group)</span></div><div class="token-line"><span class="token plain">              // 关键步骤！将组信息从groupMetadataCache中移除</span></div><div class="token-line"><span class="token plain">              groupMetadataCache.remove(group.groupId, group)</span></div><div class="token-line"><span class="token plain">              // 把消费者组从producer对应的组集合中移除</span></div><div class="token-line"><span class="token plain">              removeGroupFromAllProducers(group.groupId)</span></div><div class="token-line"><span class="token plain">              // 更新已移除组计数器</span></div><div class="token-line"><span class="token plain">              numGroupsRemoved += 1</span></div><div class="token-line"><span class="token plain">              // 更新已移除位移值计数器</span></div><div class="token-line"><span class="token plain">              numOffsetsRemoved += group.numOffsets</span></div><div class="token-line"><span class="token plain">            }</span></div><div class="token-line"><span class="token plain">          }</span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        info(s&quot;Finished unloading $topicPartition. Removed $numOffsetsRemoved cached offsets &quot; +</span></div><div class="token-line"><span class="token plain">          s&quot;and $numGroupsRemoved cached groups.&quot;)</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>该方法的主要逻辑是，先定义一个内部方法removeGroupsAndOffsets，然后创建一个异步任务，调用该方法来执行移除消费者组信息和位移信息。</p><p>那么，怎么判断要移除哪些消费者组呢？这里的依据就是<strong>传入的位移主题分区</strong>。每个消费者组及其位移的数据，都只会保存在位移主题的一个分区下。一旦给定了位移主题分区，那么，元数据保存在这个位移主题分区下的消费者组就要被移除掉。removeGroupsForPartition方法传入的offsetsPartition参数，表示Leader发生变更的位移主题分区，因此，这些分区保存的消费者组都要从该Broker上移除掉。</p><p>具体的执行逻辑是什么呢？我来解释一下。</p><p>首先，异步任务从ownedPartitions中移除给定位移主题分区。</p><p>其次，遍历消费者组元数据缓存中的所有消费者组对象，如果消费者组正是在给定位移主题分区下保存的，就依次执行下面的步骤。</p><ul><li>第1步，调用onGroupUnloaded方法执行组卸载逻辑。这个方法的逻辑是上层组件GroupCoordinator传过来的。它主要做两件事情：将消费者组状态变更到Dead状态；封装异常表示Coordinator已发生变更，然后调用回调函数返回。</li><li>第2步，把消费者组信息从groupMetadataCache中移除。这一步非常关键，目的是彻底清除掉该组的“痕迹”。</li><li>第3步，把消费者组从producer对应的组集合中移除。这里的producer，是给Kafka事务用的。</li><li>第4步，增加已移除组计数器。</li><li>第5步，更新已移除位移值计数器。</li></ul><p>到这里，方法结束。</p><h3 id="添加消费者组元数据"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#添加消费者组元数据"><span class="icon icon-link"></span></a>添加消费者组元数据</h3><p>下面，我们学习添加消费者组的管理方法，即addGroup。它特别简单，仅仅是调用putIfNotExists将给定组添加进groupMetadataCache中而已。代码如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def addGroup(group: GroupMetadata): GroupMetadata = {</span></div><div class="token-line"><span class="token plain">      val currentGroup = groupMetadataCache.putIfNotExists(group.groupId, group)</span></div><div class="token-line"><span class="token plain">      if (currentGroup != null) {</span></div><div class="token-line"><span class="token plain">        currentGroup</span></div><div class="token-line"><span class="token plain">      } else {</span></div><div class="token-line"><span class="token plain">        group</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><h3 id="加载消费者组元数据"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#加载消费者组元数据"><span class="icon icon-link"></span></a>加载消费者组元数据</h3><p>现在轮到相对复杂的加载消费者组了。GroupMetadataManager类中定义了一个loadGroup方法执行对应的加载过程。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private def loadGroup(</span></div><div class="token-line"><span class="token plain">      group: GroupMetadata, offsets: Map[TopicPartition, CommitRecordMetadataAndOffset],</span></div><div class="token-line"><span class="token plain">      pendingTransactionalOffsets: Map[Long, mutable.Map[TopicPartition, CommitRecordMetadataAndOffset]]): Unit = {</span></div><div class="token-line"><span class="token plain">      trace(s&quot;Initialized offsets $offsets for group ${group.groupId}&quot;)</span></div><div class="token-line"><span class="token plain">      // 初始化消费者组的位移信息</span></div><div class="token-line"><span class="token plain">      group.initializeOffsets(offsets, pendingTransactionalOffsets.toMap)</span></div><div class="token-line"><span class="token plain">      // 调用addGroup方法添加消费者组</span></div><div class="token-line"><span class="token plain">      val currentGroup = addGroup(group)</span></div><div class="token-line"><span class="token plain">      if (group != currentGroup)</span></div><div class="token-line"><span class="token plain">        debug(s&quot;Attempt to load group ${group.groupId} from log with generation ${group.generationId} failed &quot; +</span></div><div class="token-line"><span class="token plain">          s&quot;because there is already a cached group with generation ${currentGroup.generationId}&quot;)</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>该方法的逻辑有两步。</p><p>第1步，通过initializeOffsets方法，将位移值添加到offsets字段标识的消费者组提交位移元数据中，实现加载消费者组订阅分区提交位移的目的。</p><p>第2步，调用addGroup方法，将该消费者组元数据对象添加进消费者组元数据缓存，实现加载消费者组元数据的目的。</p><h2 id="消费者组位移管理"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#消费者组位移管理"><span class="icon icon-link"></span></a>消费者组位移管理</h2><p>除了消费者组的管理，GroupMetadataManager类的另一大类功能，是提供消费者组位移的管理，主要包括位移数据的保存和查询。我们总说，位移主题是保存消费者组位移信息的地方。实际上，<strong>当消费者组程序在查询位移时，Kafka总是从内存中的位移缓存数据查询，而不会直接读取底层的位移主题数据。</strong></p><h3 id="保存消费者组位移"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#保存消费者组位移"><span class="icon icon-link"></span></a>保存消费者组位移</h3><p>storeOffsets方法负责保存消费者组位移。该方法的代码很长，我先画一张图来展示下它的完整流程，帮助你建立起对这个方法的整体认知。接下来，我们再从它的方法签名和具体代码两个维度，来具体了解一下它的执行逻辑。</p><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimage76e676116b323c0c7b024ebe95c3c08e6ae6.ef83f7b5.jpg" alt=""/></p><p>我先给你解释一下保存消费者组位移的全部流程。</p><p><strong>首先</strong>，storeOffsets方法要过滤出满足特定条件的待保存位移信息。是否满足特定条件，要看validateOffsetMetadataLength方法的返回值。这里的特定条件，是指位移提交记录中的自定义数据大小，要小于Broker端参数offset.metadata.max.bytes的值，默认值是4KB。</p><p>如果没有一个分区满足条件，就构造OFFSET_METADATA_TOO_LARGE异常，并调用回调函数。这里的回调函数执行发送位移提交Response的动作。</p><p>倘若有分区满足了条件，<strong>接下来</strong>，方法会判断当前Broker是不是该消费者组的Coordinator，如果不是的话，就构造NOT_COORDINATOR异常，并提交给回调函数；如果是的话，就构造位移主题消息，并将消息写入进位移主题下。</p><p><strong>然后</strong>，调用一个名为putCacheCallback的内置方法，填充groupMetadataCache中各个消费者组元数据中的位移值，<strong>最后</strong>，调用回调函数返回。</p><p>接下来，我们结合代码来查看下storeOffsets方法的实现逻辑。</p><p>首先我们看下它的方法签名。既然是保存消费者组提交位移的，那么，我们就要知道上层调用方都给这个方法传入了哪些参数。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">// group：消费者组元数据</span></div><div class="token-line"><span class="token plain">    // consumerId：消费者组成员ID</span></div><div class="token-line"><span class="token plain">    // offsetMetadata：待保存的位移值，按照分区分组</span></div><div class="token-line"><span class="token plain">    // responseCallback：处理完成后的回调函数</span></div><div class="token-line"><span class="token plain">    // producerId：事务型Producer ID</span></div><div class="token-line"><span class="token plain">    // producerEpoch：事务型Producer Epoch值</span></div><div class="token-line"><span class="token plain">    def storeOffsets(</span></div><div class="token-line"><span class="token plain">      group: GroupMetadata,</span></div><div class="token-line"><span class="token plain">      consumerId: String,</span></div><div class="token-line"><span class="token plain">      offsetMetadata: immutable.Map[TopicPartition, OffsetAndMetadata],</span></div><div class="token-line"><span class="token plain">      responseCallback: immutable.Map[TopicPartition, Errors] =&gt; Unit,</span></div><div class="token-line"><span class="token plain">      producerId: Long = RecordBatch.NO_PRODUCER_ID,</span></div><div class="token-line"><span class="token plain">      producerEpoch: Short = RecordBatch.NO_PRODUCER_EPOCH): Unit = {</span></div><div class="token-line"><span class="token plain">      ......</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>这个方法接收6个参数，它们的含义我都用注释的方式标注出来了。producerId和producerEpoch这两个参数是与Kafka事务相关的，你简单了解下就行。我们要重点掌握前面4个参数的含义。</p><ul><li>group：消费者组元数据信息。该字段的类型就是我们之前学到的GroupMetadata类。</li><li>consumerId：消费者组成员ID，仅用于DEBUG调试。</li><li>offsetMetadata：待保存的位移值，按照分区分组。</li><li>responseCallback：位移保存完成后需要执行的回调函数。</li></ul><p>接下来，我们看下storeOffsets的代码。为了便于你理解，我删除了与Kafka事务操作相关的部分。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">// 过滤出满足特定条件的待保存位移数据</span></div><div class="token-line"><span class="token plain">    val filteredOffsetMetadata = offsetMetadata.filter { case (_, offsetAndMetadata) =&gt;</span></div><div class="token-line"><span class="token plain">      validateOffsetMetadataLength(offsetAndMetadata.metadata)</span></div><div class="token-line"><span class="token plain">    }</span></div><div class="token-line"><span class="token plain">    ......</span></div><div class="token-line"><span class="token plain">    val isTxnOffsetCommit = producerId != RecordBatch.NO_PRODUCER_ID</span></div><div class="token-line"><span class="token plain">    // 如果没有任何分区的待保存位移满足特定条件</span></div><div class="token-line"><span class="token plain">    if (filteredOffsetMetadata.isEmpty) {</span></div><div class="token-line"><span class="token plain">      // 构造OFFSET_METADATA_TOO_LARGE异常并调用responseCallback返回</span></div><div class="token-line"><span class="token plain">      val commitStatus = offsetMetadata.map { case (k, _) =&gt; k -&gt; Errors.OFFSET_METADATA_TOO_LARGE }</span></div><div class="token-line"><span class="token plain">      responseCallback(commitStatus)</span></div><div class="token-line"><span class="token plain">      None</span></div><div class="token-line"><span class="token plain">    } else {</span></div><div class="token-line"><span class="token plain">      // 查看当前Broker是否为给定消费者组的Coordinator</span></div><div class="token-line"><span class="token plain">      getMagic(partitionFor(group.groupId)) match {</span></div><div class="token-line"><span class="token plain">        // 如果是Coordinator</span></div><div class="token-line"><span class="token plain">        case Some(magicValue) =&gt;</span></div><div class="token-line"><span class="token plain">          val timestampType = TimestampType.CREATE_TIME</span></div><div class="token-line"><span class="token plain">          val timestamp = time.milliseconds()</span></div><div class="token-line"><span class="token plain">          // 构造位移主题的位移提交消息</span></div><div class="token-line"><span class="token plain">          val records = filteredOffsetMetadata.map { case (topicPartition, offsetAndMetadata) =&gt;</span></div><div class="token-line"><span class="token plain">            val key = GroupMetadataManager.offsetCommitKey(group.groupId, topicPartition)</span></div><div class="token-line"><span class="token plain">            val value = GroupMetadataManager.offsetCommitValue(offsetAndMetadata, interBrokerProtocolVersion)</span></div><div class="token-line"><span class="token plain">            new SimpleRecord(timestamp, key, value)</span></div><div class="token-line"><span class="token plain">          }</span></div><div class="token-line"><span class="token plain">          val offsetTopicPartition = new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, partitionFor(group.groupId))</span></div><div class="token-line"><span class="token plain">          // 为写入消息创建内存Buffer</span></div><div class="token-line"><span class="token plain">          val buffer = ByteBuffer.allocate(AbstractRecords.estimateSizeInBytes(magicValue, compressionType, records.asJava))</span></div><div class="token-line"><span class="token plain">          if (isTxnOffsetCommit &amp;&amp; magicValue &lt; RecordBatch.MAGIC_VALUE_V2)</span></div><div class="token-line"><span class="token plain">            throw Errors.UNSUPPORTED_FOR_MESSAGE_FORMAT.exception(&quot;Attempting to make a transaction offset commit with an invalid magic: &quot; + magicValue)</span></div><div class="token-line"><span class="token plain">          val builder = MemoryRecords.builder(buffer, magicValue, compressionType, timestampType, 0L, time.milliseconds(),</span></div><div class="token-line"><span class="token plain">            producerId, producerEpoch, 0, isTxnOffsetCommit, RecordBatch.NO_PARTITION_LEADER_EPOCH)</span></div><div class="token-line"><span class="token plain">          records.foreach(builder.append)</span></div><div class="token-line"><span class="token plain">          val entries = Map(offsetTopicPartition -&gt; builder.build())</span></div><div class="token-line"><span class="token plain">          // putCacheCallback函数定义......</span></div><div class="token-line"><span class="token plain">          if (isTxnOffsetCommit) {</span></div><div class="token-line"><span class="token plain">            ......</span></div><div class="token-line"><span class="token plain">          } else {</span></div><div class="token-line"><span class="token plain">            group.inLock {</span></div><div class="token-line"><span class="token plain">              group.prepareOffsetCommit(offsetMetadata)</span></div><div class="token-line"><span class="token plain">            }</span></div><div class="token-line"><span class="token plain">          }</span></div><div class="token-line"><span class="token plain">          // 写入消息到位移主题，同时调用putCacheCallback方法更新消费者元数据</span></div><div class="token-line"><span class="token plain">          appendForGroup(group, entries, putCacheCallback)</span></div><div class="token-line"><span class="token plain">        // 如果是Coordinator</span></div><div class="token-line"><span class="token plain">        case None =&gt;</span></div><div class="token-line"><span class="token plain">          // 构造NOT_COORDINATOR异常并提交给responseCallback方法</span></div><div class="token-line"><span class="token plain">          val commitStatus = offsetMetadata.map {</span></div><div class="token-line"><span class="token plain">            case (topicPartition, _) =&gt;</span></div><div class="token-line"><span class="token plain">              (topicPartition, Errors.NOT_COORDINATOR)</span></div><div class="token-line"><span class="token plain">          }</span></div><div class="token-line"><span class="token plain">          responseCallback(commitStatus)</span></div><div class="token-line"><span class="token plain">          None</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>我为方法的关键步骤都标注了注释，具体流程前面我也介绍过了，应该很容易理解。不过，这里还需要注意两点，也就是appendForGroup和putCacheCallback方法。前者是向位移主题写入消息；后者是填充元数据缓存的。我们结合代码来学习下。</p><p>appendForGroup方法负责写入消息到位移主题，同时传入putCacheCallback方法，更新消费者元数据。以下是它的代码：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private def appendForGroup(</span></div><div class="token-line"><span class="token plain">      group: GroupMetadata,</span></div><div class="token-line"><span class="token plain">      records: Map[TopicPartition, MemoryRecords],</span></div><div class="token-line"><span class="token plain">      callback: Map[TopicPartition, PartitionResponse] =&gt; Unit): Unit = {</span></div><div class="token-line"><span class="token plain">      replicaManager.appendRecords(</span></div><div class="token-line"><span class="token plain">        timeout = config.offsetCommitTimeoutMs.toLong,</span></div><div class="token-line"><span class="token plain">        requiredAcks = config.offsetCommitRequiredAcks,</span></div><div class="token-line"><span class="token plain">        internalTopicsAllowed = true,</span></div><div class="token-line"><span class="token plain">        origin = AppendOrigin.Coordinator,</span></div><div class="token-line"><span class="token plain">        entriesPerPartition = records,</span></div><div class="token-line"><span class="token plain">        delayedProduceLock = Some(group.lock),</span></div><div class="token-line"><span class="token plain">        responseCallback = callback)</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>可以看到，该方法就是调用ReplicaManager的appendRecords方法，将消息写入到位移主题中。</p><p>下面，我们再关注一下putCacheCallback方法的实现，也就是将写入的位移值填充到缓存中。我先画一张图来展示下putCacheCallback的逻辑。</p><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimagebc42bc2fcf199a685a5cc6d32846c53c3042.1664ccdd.jpg" alt=""/></p><p>现在，我们结合代码，学习下它的逻辑实现。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def putCacheCallback(responseStatus: Map[TopicPartition, PartitionResponse]): Unit = {</span></div><div class="token-line"><span class="token plain">      // 确保消息写入到指定位移主题分区，否则抛出异常</span></div><div class="token-line"><span class="token plain">      if (responseStatus.size != 1 || !responseStatus.contains(offsetTopicPartition))</span></div><div class="token-line"><span class="token plain">        throw new IllegalStateException(&quot;Append status %s should only have one partition %s&quot;</span></div><div class="token-line"><span class="token plain">          .format(responseStatus, offsetTopicPartition))</span></div><div class="token-line"><span class="token plain">      // 更新已提交位移数指标</span></div><div class="token-line"><span class="token plain">      offsetCommitsSensor.record(records.size)</span></div><div class="token-line"><span class="token plain">      val status = responseStatus(offsetTopicPartition)</span></div><div class="token-line"><span class="token plain">      val responseError = group.inLock {</span></div><div class="token-line"><span class="token plain">        // 写入结果没有错误</span></div><div class="token-line"><span class="token plain">        if (status.error == Errors.NONE) {</span></div><div class="token-line"><span class="token plain">          // 如果不是Dead状态</span></div><div class="token-line"><span class="token plain">          if (!group.is(Dead)) {</span></div><div class="token-line"><span class="token plain">            filteredOffsetMetadata.foreach { case (topicPartition, offsetAndMetadata) =&gt;</span></div><div class="token-line"><span class="token plain">              if (isTxnOffsetCommit)</span></div><div class="token-line"><span class="token plain">                ......</span></div><div class="token-line"><span class="token plain">              else</span></div><div class="token-line"><span class="token plain">                // 调用GroupMetadata的onOffsetCommitAppend方法填充元数据</span></div><div class="token-line"><span class="token plain">                group.onOffsetCommitAppend(topicPartition, CommitRecordMetadataAndOffset(Some(status.baseOffset), offsetAndMetadata))</span></div><div class="token-line"><span class="token plain">            }</span></div><div class="token-line"><span class="token plain">          }</span></div><div class="token-line"><span class="token plain">          Errors.NONE</span></div><div class="token-line"><span class="token plain">        // 写入结果有错误</span></div><div class="token-line"><span class="token plain">        } else {</span></div><div class="token-line"><span class="token plain">          if (!group.is(Dead)) {</span></div><div class="token-line"><span class="token plain">            ......</span></div><div class="token-line"><span class="token plain">            filteredOffsetMetadata.foreach { case (topicPartition, offsetAndMetadata) =&gt;</span></div><div class="token-line"><span class="token plain">              if (isTxnOffsetCommit)</span></div><div class="token-line"><span class="token plain">                group.failPendingTxnOffsetCommit(producerId, topicPartition)</span></div><div class="token-line"><span class="token plain">              else</span></div><div class="token-line"><span class="token plain">                // 取消未完成的位移消息写入</span></div><div class="token-line"><span class="token plain">                group.failPendingOffsetWrite(topicPartition, offsetAndMetadata)</span></div><div class="token-line"><span class="token plain">            }</span></div><div class="token-line"><span class="token plain">          }</span></div><div class="token-line"><span class="token plain">          ......</span></div><div class="token-line"><span class="token plain">          // 确认异常类型</span></div><div class="token-line"><span class="token plain">          status.error match {</span></div><div class="token-line"><span class="token plain">            case Errors.UNKNOWN_TOPIC_OR_PARTITION</span></div><div class="token-line"><span class="token plain">                 | Errors.NOT_ENOUGH_REPLICAS</span></div><div class="token-line"><span class="token plain">                 | Errors.NOT_ENOUGH_REPLICAS_AFTER_APPEND =&gt;</span></div><div class="token-line"><span class="token plain">              Errors.COORDINATOR_NOT_AVAILABLE</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">            case Errors.NOT_LEADER_FOR_PARTITION</span></div><div class="token-line"><span class="token plain">                 | Errors.KAFKA_STORAGE_ERROR =&gt;</span></div><div class="token-line"><span class="token plain">              Errors.NOT_COORDINATOR</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">            case Errors.MESSAGE_TOO_LARGE</span></div><div class="token-line"><span class="token plain">                 | Errors.RECORD_LIST_TOO_LARGE</span></div><div class="token-line"><span class="token plain">                 | Errors.INVALID_FETCH_SIZE =&gt;</span></div><div class="token-line"><span class="token plain">              Errors.INVALID_COMMIT_OFFSET_SIZE</span></div><div class="token-line"><span class="token plain">            case other =&gt; other</span></div><div class="token-line"><span class="token plain">          }</span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">      // 利用异常类型构建提交返回状态</span></div><div class="token-line"><span class="token plain">      val commitStatus = offsetMetadata.map { case (topicPartition, offsetAndMetadata) =&gt;</span></div><div class="token-line"><span class="token plain">        if (validateOffsetMetadataLength(offsetAndMetadata.metadata))</span></div><div class="token-line"><span class="token plain">          (topicPartition, responseError)</span></div><div class="token-line"><span class="token plain">        else</span></div><div class="token-line"><span class="token plain">          (topicPartition, Errors.OFFSET_METADATA_TOO_LARGE)</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">      // 调用回调函数</span></div><div class="token-line"><span class="token plain">      responseCallback(commitStatus)</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>putCacheCallback方法的主要目的，是将多个消费者组位移值填充到GroupMetadata的offsets元数据缓存中。</p><p><strong>首先</strong>，该方法要确保位移消息写入到指定位移主题分区，否则就抛出异常。</p><p><strong>之后</strong>，更新已提交位移数指标，然后判断写入结果是否有错误。</p><p>如果没有错误，只要组状态不是Dead状态，就调用GroupMetadata的onOffsetCommitAppend方法填充元数据。onOffsetCommitAppend方法的主体逻辑，是将消费者组订阅分区的位移值写入到offsets字段保存的集合中。当然，如果状态是Dead，则什么都不做。</p><p>如果刚才的写入结果有错误，那么，就通过failPendingOffsetWrite方法取消未完成的位移消息写入。</p><p><strong>接下来</strong>，代码要将日志写入的异常类型转换成表征提交状态错误的异常类型。具体来说，就是将UNKNOWN_TOPIC_OR_PARTITION、NOT_LEADER_FOR_PARTITION和MESSAGE_TOO_LARGE这样的异常，转换到COORDINATOR_NOT_AVAILABLE和NOT_COORDINATOR这样的异常。之后，再将这些转换后的异常封装进commitStatus字段中传给回调函数。</p><p><strong>最后</strong>，调用回调函数返回。至此，方法结束。</p><p>好了，保存消费者组位移信息的storeOffsets方法，我们就学完了，它的关键逻辑，是构造位移主题消息并写入到位移主题，然后将位移值填充到消费者组元数据中。</p><h3 id="查询消费者组位移"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#查询消费者组位移"><span class="icon icon-link"></span></a>查询消费者组位移</h3><p>现在，我再说说查询消费者组位移，也就是getOffsets方法的代码实现。比起storeOffsets，这个方法要更容易理解。我们看下它的源码：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def getOffsets(</span></div><div class="token-line"><span class="token plain">      groupId: String, </span></div><div class="token-line"><span class="token plain">      requireStable: Boolean, </span></div><div class="token-line"><span class="token plain">      topicPartitionsOpt: Option[Seq[TopicPartition]]): Map[TopicPartition, PartitionData] = {</span></div><div class="token-line"><span class="token plain">      ......</span></div><div class="token-line"><span class="token plain">      // 从groupMetadataCache字段中获取指定消费者组的元数据</span></div><div class="token-line"><span class="token plain">      val group = groupMetadataCache.get(groupId)</span></div><div class="token-line"><span class="token plain">      // 如果没有组数据，返回空数据</span></div><div class="token-line"><span class="token plain">      if (group == null) {</span></div><div class="token-line"><span class="token plain">        topicPartitionsOpt.getOrElse(Seq.empty[TopicPartition]).map { topicPartition =&gt;</span></div><div class="token-line"><span class="token plain">          val partitionData = new PartitionData(OffsetFetchResponse.INVALID_OFFSET,</span></div><div class="token-line"><span class="token plain">            Optional.empty(), &quot;&quot;, Errors.NONE)</span></div><div class="token-line"><span class="token plain">          topicPartition -&gt; partitionData</span></div><div class="token-line"><span class="token plain">        }.toMap</span></div><div class="token-line"><span class="token plain">      // 如果存在组数据</span></div><div class="token-line"><span class="token plain">      } else {</span></div><div class="token-line"><span class="token plain">        group.inLock {</span></div><div class="token-line"><span class="token plain">          // 如果组处于Dead状态，则返回空数据</span></div><div class="token-line"><span class="token plain">          if (group.is(Dead)) {</span></div><div class="token-line"><span class="token plain">            topicPartitionsOpt.getOrElse(Seq.empty[TopicPartition]).map { topicPartition =&gt;</span></div><div class="token-line"><span class="token plain">              val partitionData = new PartitionData(OffsetFetchResponse.INVALID_OFFSET,</span></div><div class="token-line"><span class="token plain">                Optional.empty(), &quot;&quot;, Errors.NONE)</span></div><div class="token-line"><span class="token plain">              topicPartition -&gt; partitionData</span></div><div class="token-line"><span class="token plain">            }.toMap</span></div><div class="token-line"><span class="token plain">          } else {</span></div><div class="token-line"><span class="token plain">            val topicPartitions = topicPartitionsOpt.getOrElse(group.allOffsets.keySet)</span></div><div class="token-line"><span class="token plain">            topicPartitions.map { topicPartition =&gt;</span></div><div class="token-line"><span class="token plain">              if (requireStable &amp;&amp; group.hasPendingOffsetCommitsForTopicPartition(topicPartition)) {</span></div><div class="token-line"><span class="token plain">                topicPartition -&gt; new PartitionData(OffsetFetchResponse.INVALID_OFFSET,</span></div><div class="token-line"><span class="token plain">                  Optional.empty(), &quot;&quot;, Errors.UNSTABLE_OFFSET_COMMIT)</span></div><div class="token-line"><span class="token plain">              } else {</span></div><div class="token-line"><span class="token plain">                val partitionData = group.offset(topicPartition) match {</span></div><div class="token-line"><span class="token plain">                  // 如果没有该分区位移数据，返回空数据</span></div><div class="token-line"><span class="token plain">                  case None =&gt;</span></div><div class="token-line"><span class="token plain">                    new PartitionData(OffsetFetchResponse.INVALID_OFFSET,</span></div><div class="token-line"><span class="token plain">                      Optional.empty(), &quot;&quot;, Errors.NONE)</span></div><div class="token-line"><span class="token plain">                  // 从消费者组元数据中返回指定分区的位移数据</span></div><div class="token-line"><span class="token plain">                  case Some(offsetAndMetadata) =&gt;</span></div><div class="token-line"><span class="token plain">                    new PartitionData(offsetAndMetadata.offset,</span></div><div class="token-line"><span class="token plain">                      offsetAndMetadata.leaderEpoch, offsetAndMetadata.metadata, Errors.NONE)</span></div><div class="token-line"><span class="token plain">                }</span></div><div class="token-line"><span class="token plain">                topicPartition -&gt; partitionData</span></div><div class="token-line"><span class="token plain">              }</span></div><div class="token-line"><span class="token plain">            }.toMap</span></div><div class="token-line"><span class="token plain">          }</span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>getOffsets方法首先会读取groupMetadataCache中的组元数据，如果不存在对应的记录，则返回空数据集，如果存在，就接着判断组是否处于Dead状态。</p><p>如果是Dead状态，就说明消费者组已经被销毁了，位移数据也被视为不可用了，依然返回空数据集；若状态不是Dead，就提取出消费者组订阅的分区信息，再依次为它们获取对应的位移数据并返回。至此，方法结束。</p><h1 id="总结"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#总结"><span class="icon icon-link"></span></a>总结</h1><p>今天，我们学习了GroupMetadataManager类的源码。作为消费者组管理器，它负责管理消费者组的方方面面。其中，非常重要的两个管理功能是消费者组元数据管理和消费者组位移管理，分别包括查询获取、移除、添加和加载消费者组元数据，以及保存和查询消费者组位移，这些方法是上层组件GroupCoordinator倚重的重量级功能载体，你一定要彻底掌握它们。</p><p>我画了一张思维导图，帮助你复习一下今天的重点内容。</p><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimageeb5aeb8fe45e1d152e2ac9cb52c81390265a.f77fac4c.jpg" alt=""/></p><p>实际上，GroupMetadataManager类的地位举足轻重。虽然它在Coordinator组件中不显山不露水，但却是一些线上问题的根源所在。</p><p>我再跟你分享一个小案例。</p><p>之前，我碰到过一个问题：在消费者组成员超多的情况下，无法完成位移加载，这导致Consumer端总是接收到Marking the coordinator dead的错误。</p><p>当时，我查遍各种资料，都无法定位问题，最终，还是通过阅读源码，发现是这个类的doLoadGroupsAndOffsets方法中创建的buffer过小导致的。后来，通过调大offsets.load.buffer.size参数值，我们顺利地解决了问题。</p><p>试想一下，如果当时没有阅读这部分的源码，仅凭日志，我们肯定无法解决这个问题。因此，我们花三节课的时间，专门阅读GroupMetadataManager类源码，是非常值得的。下节课，我将带你继续研读GroupMetadataManager源码，去探寻有关位移主题的那些代码片段。</p><h1 id="课后讨论"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03#课后讨论"><span class="icon icon-link"></span></a>课后讨论</h1><p>请思考这样一个问题：在什么场景下，需要移除GroupMetadataManager中保存的消费者组记录？</p><p>欢迎在留言区写下你的思考和答案，跟我交流讨论，也欢迎你把今天的内容分享给你的朋友。</p></div><div class="__dumi-default-layout-footer-meta"><a target="_blank" rel="noopener noreferrer" href="https://github.com/GGwujun/blog/edit/master/ssrc/kafka核心源码解读/08.消费者组管理模块/03.md">在 GitHub 上编辑此页<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><span data-updated-text="最后更新时间：">2023/9/27 11:15:40</span></div></div></div></div>
	<script>
  window.g_useSSR = true;
  window.g_initialProps = {};
	</script>

    <script>
      (function () {
        if (!location.port) {
          (function (i, s, o, g, r, a, m) {
            i["GoogleAnalyticsObject"] = r;
            (i[r] =
              i[r] ||
              function () {
                (i[r].q = i[r].q || []).push(arguments);
              }),
              (i[r].l = 1 * new Date());
            (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m);
          })(
            window,
            document,
            "script",
            "//www.google-analytics.com/analytics.js",
            "ga"
          );
          ga("create", "UA-149864185-1", "auto");
          ga("send", "pageview");
        }
      })();
    </script>
    <script src="/blog-backend/umi.e14e5a14.js"></script>
  </body>
</html>
