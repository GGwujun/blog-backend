<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"
    />
    <link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
    <link rel="stylesheet" href="/blog-backend/umi.3ec1f225.css" />
    <script>
      window.routerBase = "/blog-backend";
    </script>
    <script>
      //! umi version: 3.5.41
    </script>
    <script>
      !(function () {
        var e =
            navigator.cookieEnabled && void 0 !== window.localStorage
              ? localStorage.getItem("dumi:prefers-color")
              : "auto",
          o = window.matchMedia("(prefers-color-scheme: dark)").matches,
          t = ["light", "dark", "auto"];
        document.documentElement.setAttribute(
          "data-prefers-color",
          e === t[2] ? (o ? t[1] : t[0]) : t.indexOf(e) > -1 ? e : t[0]
        );
      })();
    </script>
    <title>02 | 日志（上）：日志究竟是如何加载日志段的？ - 大师兄</title>
  </head>
  <body>
    <div id="root"><div class="__dumi-default-layout" data-route="/kafka核心源码解读/02.日志模块/02" data-show-sidemenu="true" data-show-slugs="true" data-site-mode="true" data-gapless="false"><div class="__dumi-default-navbar" data-mode="site"><button class="__dumi-default-navbar-toggle"></button><a class="__dumi-default-navbar-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-backend/">大师兄</a><nav><div class="__dumi-default-search"><input type="search" class="__dumi-default-search-input" value=""/><ul></ul></div><span>后端开发<ul><li><a href="/blog-backend/go语言核心36讲">go语言核心36讲</a></li><li><a href="/blog-backend/go并发编程实战">go并发编程实战</a></li><li><a href="/blog-backend/go语言项目开发实战">go语言项目开发实战</a></li><li><a href="/blog-backend/kafka核心技术与实战">kafka核心技术与实战</a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/零基础学python">零基础学python</a></li><li><a href="/blog-backend/python核心技术与实战">python核心技术与实战</a></li><li><a href="/blog-backend/redis核心技术与实战">redis核心技术与实战</a></li><li><a href="/blog-backend/redis源码剖析与实战">redis源码剖析与实战</a></li><li><a href="/blog-backend/陈天rust编程第一课">陈天rust编程第一课</a></li><li><a href="/blog-backend/tonybaigo语言第一课">tonybaigo语言第一课</a></li><li><a href="/blog-backend/后端存储实战课">后端存储实战课</a></li><li><a href="/blog-backend/后端技术面试38讲">后端技术面试38讲</a></li><li><a href="/blog-backend/深入c语言和程序运行原理">深入c语言和程序运行原理</a></li><li><a href="/blog-backend/现代c编程实战">现代c编程实战</a></li><li><a href="/blog-backend/罗剑锋的c实战笔记">罗剑锋的c实战笔记</a></li><li><a href="/blog-backend/零基础入门spark">零基础入门spark</a></li></ul></span><span>架构师<ul><li><a href="/blog-backend/mysql实战45讲">mysql实战45讲</a></li><li><a href="/blog-backend/数据中台实战课">数据中台实战课</a></li></ul></span><div class="__dumi-default-navbar-tool"><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "></div></div></div></nav></div><div class="__dumi-default-menu" data-mode="site"><div class="__dumi-default-menu-inner"><div class="__dumi-default-menu-header"><a class="__dumi-default-menu-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-backend/"></a><h1>大师兄</h1><p></p></div><div class="__dumi-default-menu-mobile-area"><ul class="__dumi-default-menu-nav-list"><li>后端开发<ul><li><a href="/blog-backend/go语言核心36讲">go语言核心36讲</a></li><li><a href="/blog-backend/go并发编程实战">go并发编程实战</a></li><li><a href="/blog-backend/go语言项目开发实战">go语言项目开发实战</a></li><li><a href="/blog-backend/kafka核心技术与实战">kafka核心技术与实战</a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/零基础学python">零基础学python</a></li><li><a href="/blog-backend/python核心技术与实战">python核心技术与实战</a></li><li><a href="/blog-backend/redis核心技术与实战">redis核心技术与实战</a></li><li><a href="/blog-backend/redis源码剖析与实战">redis源码剖析与实战</a></li><li><a href="/blog-backend/陈天rust编程第一课">陈天rust编程第一课</a></li><li><a href="/blog-backend/tonybaigo语言第一课">tonybaigo语言第一课</a></li><li><a href="/blog-backend/后端存储实战课">后端存储实战课</a></li><li><a href="/blog-backend/后端技术面试38讲">后端技术面试38讲</a></li><li><a href="/blog-backend/深入c语言和程序运行原理">深入c语言和程序运行原理</a></li><li><a href="/blog-backend/现代c编程实战">现代c编程实战</a></li><li><a href="/blog-backend/罗剑锋的c实战笔记">罗剑锋的c实战笔记</a></li><li><a href="/blog-backend/零基础入门spark">零基础入门spark</a></li></ul></li><li>架构师<ul><li><a href="/blog-backend/mysql实战45讲">mysql实战45讲</a></li><li><a href="/blog-backend/数据中台实战课">数据中台实战课</a></li></ul></li></ul><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "><button title="Dark theme" class="__dumi-default-dark-moon "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3854" width="22" height="22"><path d="M991.816611 674.909091a69.166545 69.166545 0 0 0-51.665455-23.272727 70.795636 70.795636 0 0 0-27.438545 5.585454A415.674182 415.674182 0 0 1 754.993338 698.181818c-209.594182 0-393.472-184.785455-393.472-395.636363 0-52.363636 38.539636-119.621818 69.515637-173.614546 4.887273-8.610909 9.634909-16.756364 14.103272-24.901818A69.818182 69.818182 0 0 0 384.631156 0a70.842182 70.842182 0 0 0-27.438545 5.585455C161.678429 90.298182 14.362065 307.898182 14.362065 512c0 282.298182 238.824727 512 532.38691 512a522.286545 522.286545 0 0 0 453.957818-268.334545A69.818182 69.818182 0 0 0 991.816611 674.909091zM546.679156 954.181818c-248.785455 0-462.941091-192-462.941091-442.181818 0-186.647273 140.637091-372.829091 300.939637-442.181818-36.817455 65.629091-92.578909 151.970909-92.578909 232.727273 0 250.181818 214.109091 465.454545 462.917818 465.454545a488.331636 488.331636 0 0 0 185.181091-46.545455 453.003636 453.003636 0 0 1-393.565091 232.727273z m103.656728-669.323636l-14.266182 83.781818a34.909091 34.909091 0 0 0 50.362182 36.770909l74.775272-39.563636 74.752 39.563636a36.142545 36.142545 0 0 0 16.174546 3.956364 34.909091 34.909091 0 0 0 34.210909-40.727273l-14.289455-83.781818 60.509091-59.345455a35.025455 35.025455 0 0 0-19.223272-59.578182l-83.61891-12.101818-37.376-76.101818a34.56 34.56 0 0 0-62.254545 0l-37.376 76.101818-83.618909 12.101818a34.909091 34.909091 0 0 0-19.246546 59.578182z m70.423272-64.698182a34.280727 34.280727 0 0 0 26.135273-19.083636l14.312727-29.090909 14.336 29.090909a34.257455 34.257455 0 0 0 26.135273 19.083636l32.046546 4.887273-23.272728 22.574545a35.234909 35.234909 0 0 0-10.007272 30.952727l5.46909 32.116364-28.625454-15.127273a34.490182 34.490182 0 0 0-32.302546 0l-28.695272 15.127273 5.469091-32.116364a35.141818 35.141818 0 0 0-9.984-30.952727l-23.272728-22.574545z" p-id="3855"></path></svg></button><button title="Light theme" class="__dumi-default-dark-sun "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4026" width="22" height="22"><path d="M915.2 476.16h-43.968c-24.704 0-44.736 16-44.736 35.84s20.032 35.904 44.736 35.904H915.2c24.768 0 44.8-16.064 44.8-35.904s-20.032-35.84-44.8-35.84zM512 265.6c-136.704 0-246.464 109.824-246.464 246.4 0 136.704 109.76 246.464 246.464 246.464S758.4 648.704 758.4 512c0-136.576-109.696-246.4-246.4-246.4z m0 425.6c-99.008 0-179.2-80.128-179.2-179.2 0-98.944 80.192-179.2 179.2-179.2S691.2 413.056 691.2 512c0 99.072-80.192 179.2-179.2 179.2zM197.44 512c0-19.84-19.136-35.84-43.904-35.84H108.8c-24.768 0-44.8 16-44.8 35.84s20.032 35.904 44.8 35.904h44.736c24.768 0 43.904-16.064 43.904-35.904zM512 198.464c19.776 0 35.84-20.032 35.84-44.8v-44.8C547.84 84.032 531.84 64 512 64s-35.904 20.032-35.904 44.8v44.8c0 24.768 16.128 44.864 35.904 44.864z m0 627.136c-19.776 0-35.904 20.032-35.904 44.8v44.736C476.096 940.032 492.16 960 512 960s35.84-20.032 35.84-44.8v-44.736c0-24.768-16.064-44.864-35.84-44.864z m329.92-592.832c17.472-17.536 20.288-43.072 6.4-57.024-14.016-14.016-39.488-11.2-57.024 6.336-4.736 4.864-26.496 26.496-31.36 31.36-17.472 17.472-20.288 43.008-6.336 57.024 13.952 14.016 39.488 11.2 57.024-6.336 4.8-4.864 26.496-26.56 31.296-31.36zM213.376 759.936c-4.864 4.8-26.56 26.624-31.36 31.36-17.472 17.472-20.288 42.944-6.4 56.96 14.016 13.952 39.552 11.2 57.024-6.336 4.8-4.736 26.56-26.496 31.36-31.36 17.472-17.472 20.288-43.008 6.336-56.96-14.016-13.952-39.552-11.072-56.96 6.336z m19.328-577.92c-17.536-17.536-43.008-20.352-57.024-6.336-14.08 14.016-11.136 39.488 6.336 57.024 4.864 4.864 26.496 26.56 31.36 31.424 17.536 17.408 43.008 20.288 56.96 6.336 14.016-14.016 11.264-39.488-6.336-57.024-4.736-4.864-26.496-26.56-31.296-31.424z m527.168 628.608c4.864 4.864 26.624 26.624 31.36 31.424 17.536 17.408 43.072 20.224 57.088 6.336 13.952-14.016 11.072-39.552-6.4-57.024-4.864-4.8-26.56-26.496-31.36-31.36-17.472-17.408-43.072-20.288-57.024-6.336-13.952 14.016-11.008 39.488 6.336 56.96z" p-id="4027"></path></svg></button><button title="Default to system" class="__dumi-default-dark-auto "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="11002" width="22" height="22"><path d="M127.658667 492.885333c0-51.882667 10.24-101.717333 30.378666-149.162666s47.786667-88.064 81.92-122.538667 75.093333-61.781333 122.538667-81.92 96.938667-30.378667 149.162667-30.378667 101.717333 10.24 149.162666 30.378667 88.405333 47.786667 122.88 81.92 61.781333 75.093333 81.92 122.538667 30.378667 96.938667 30.378667 149.162666-10.24 101.717333-30.378667 149.162667-47.786667 88.405333-81.92 122.88-75.093333 61.781333-122.88 81.92-97.28 30.378667-149.162666 30.378667-101.717333-10.24-149.162667-30.378667-88.064-47.786667-122.538667-81.92-61.781333-75.093333-81.92-122.88-30.378667-96.938667-30.378666-149.162667z m329.045333 0c0 130.048 13.994667 244.394667 41.984 343.381334h12.970667c46.762667 0 91.136-9.216 133.461333-27.306667s78.848-42.666667 109.568-73.386667 54.954667-67.242667 73.386667-109.568 27.306667-86.698667 27.306666-133.461333c0-46.421333-9.216-90.794667-27.306666-133.12s-42.666667-78.848-73.386667-109.568-67.242667-54.954667-109.568-73.386667-86.698667-27.306667-133.461333-27.306666h-11.605334c-28.672 123.562667-43.349333 237.909333-43.349333 343.722666z" p-id="11003"></path></svg></button></div></div></div><ul class="__dumi-default-menu-list"><li><a href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学">01.课前必学</a><ul><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/01"><span>开篇词 |  阅读源码，逐渐成了职业进阶道路上的“必选项”</span></a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/02"><span>导读 | 构建Kafka工程和源码阅读环境、Scala语言热身</span></a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/03"><span>重磅加餐 | 带你快速入门Scala语言</span></a></li></ul></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读/02.日志模块">02.日志模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/01"><span>01 | 日志段：保存消息文件的对象是怎么实现的？</span></a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读/02.日志模块/02"><span>02 | 日志（上）：日志究竟是如何加载日志段的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/03"><span>03 | 日志（下）：彻底搞懂Log对象的常见操作</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/04"><span>04 | 索引（上）：改进的二分查找算法在Kafka索引的应用</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/05"><span>05 | 索引（下）：位移索引和时间戳索引的区别是什么？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块">03.请求处理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/01"><span>06 | 请求通道：如何实现Kafka请求队列？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/02"><span>07 | SocketServer（上）：Kafka到底是怎么应用NIO实现网络通信的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/03"><span>08 | SocketServer（中）：请求还要区分优先级？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/04"><span>09 | SocketServer（下）：请求处理全流程源码分析</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/05"><span>10 | KafkaApis：Kafka最重要的源码入口，没有之一</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块">04.Controller模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/01"><span>11 | Controller元数据：Controller都保存有哪些东西？有几种状态？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/02"><span>12 | ControllerChannelManager：Controller如何管理请求发送？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/03"><span>13 | ControllerEventManager：变身单线程后的Controller如何处理事件？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/04"><span>14 | Controller选举是怎么实现的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/05"><span>15 | 如何理解Controller在Kafka集群中的作用？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块">05.状态机模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/01"><span>16 | TopicDeletionManager： Topic是怎么被删除的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/02"><span>17 | ReplicaStateMachine：揭秘副本状态机实现原理</span></a></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/03"><span>18 | PartitionStateMachine：分区状态转换如何实现？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块">06.延迟操作模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块/01"><span>19 | TimingWheel：探究Kafka定时器背后的高效时间轮算法</span></a></li><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块/02"><span>20 | DelayedOperation：Broker是怎么延时处理请求的？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块">07.副本管理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/01"><span>21 | AbstractFetcherThread：拉取消息分几步？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/02"><span>22 | ReplicaFetcherThread：Follower如何拉取Leader消息？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/03"><span>23 | ReplicaManager（上）：必须要掌握的副本管理类定义和核心字段</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/04"><span>24 | ReplicaManager（中）：副本管理器是如何读写副本的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/05"><span>25 | ReplicaManager（下）：副本管理器是如何管理副本的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/06"><span>26 | MetadataCache：Broker是怎么异步更新元数据缓存的？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块">08.消费者组管理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/01"><span>27 | 消费者组元数据（上）：消费者组都有哪些元数据？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02"><span>28 | 消费者组元数据（下）：Kafka如何管理这些元数据？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03"><span>29 | GroupMetadataManager：组元数据管理器是个什么东西？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/04"><span>30 | GroupMetadataManager：位移主题保存的只是位移吗？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/05"><span>31 | GroupMetadataManager：查询位移时，不用读取位移主题？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/06"><span>32 | GroupCoordinator：在Rebalance中，Coordinator如何处理成员入组？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/07"><span>33 | GroupCoordinator：在Rebalance中，如何进行组同步？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送">09.特别放送</a><ul><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/01"><span>特别放送（一）| 经典的Kafka学习资料有哪些？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/02"><span>特别放送（二）| 一篇文章带你了解参与开源社区的全部流程</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/03"><span>特别放送（三）| 我是怎么度过日常一天的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/04"><span>特别放送（四）| 20道经典的Kafka面试题详解</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/05"><span>特别放送（五） | Kafka 社区的重磅功能：移除 ZooKeeper 依赖</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/10.测试题">10.测试题</a><ul><li><a href="/blog-backend/kafka核心源码解读/10.测试题/01"><span>期中测试 | 这些源码知识，你都掌握了吗？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/10.测试题/02"><span>期末测试 | 一套习题，测试你的掌握程度</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/11.结束语">11.结束语</a><ul><li><a href="/blog-backend/kafka核心源码解读/11.结束语/01"><span>结束语 | 源码学习，我们才刚上路呢</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/summary">kafka核心源码解读</a></li></ul></div></div><ul role="slug-list" class="__dumi-default-layout-toc"><li title="Log源码结构" data-depth="2"><a href="/blog-backend/kafka核心源码解读/02.日志模块/02#log源码结构"><span>Log源码结构</span></a></li><li title="Log Class &amp; Object" data-depth="2"><a href="/blog-backend/kafka核心源码解读/02.日志模块/02#log-class--object"><span>Log Class &amp; Object</span></a></li><li title="总结" data-depth="2"><a href="/blog-backend/kafka核心源码解读/02.日志模块/02#总结"><span>总结</span></a></li><li title="课后讨论" data-depth="2"><a href="/blog-backend/kafka核心源码解读/02.日志模块/02#课后讨论"><span>课后讨论</span></a></li></ul><div class="__dumi-default-layout-content"><div class="markdown"><h1 id="02--日志上日志究竟是如何加载日志段的"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/02.日志模块/02#02--日志上日志究竟是如何加载日志段的"><span class="icon icon-link"></span></a>02 | 日志（上）：日志究竟是如何加载日志段的？</h1><p>你好，我是胡夕。今天我来讲讲Kafka源码的日志（Log）对象。</p><p>上节课，我们学习了日志段部分的源码，你可以认为，<strong>日志是日志段的容器，里面定义了很多管理日志段的操作</strong>。坦率地说，如果看Kafka源码却不看Log，就跟你买了这门课却不知道作者是谁一样。在我看来，Log对象是Kafka源码（特别是Broker端）最核心的部分，没有之一。</p><p>它到底有多重要呢？我和你分享一个例子，你先感受下。我最近正在修复一个Kafka的Bug（<a target="_blank" rel="noopener noreferrer" href="https://issues.apache.org/jira/browse/KAFKA-9157">KAFKA-9157<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>）：在某些情况下，Kafka的Compaction操作会产生很多空的日志段文件。如果要避免这些空日志段文件被创建出来，就必须搞懂创建日志段文件的原理，而这些代码恰恰就在Log源码中。</p><p>既然Log源码要管理日志段对象，那么它就必须先把所有日志段对象加载到内存里面。这个过程是怎么实现的呢？今天，我就带你学习下日志加载日志段的过程。</p><p>首先，我们来看下Log对象的源码结构。</p><h2 id="log源码结构"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/02.日志模块/02#log源码结构"><span class="icon icon-link"></span></a>Log源码结构</h2><p>Log源码位于Kafka core工程的log源码包下，文件名是Log.scala。总体上，该文件定义了10个类和对象，如下图所示：</p><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimage81ce8126a191f63d9abea860d71992b0aece.3da6d7a4.jpg" alt=""/></p><p>那么，这10个类和对象都是做什么的呢？我先给你简单介绍一下，你可以对它们有个大致的了解。</p><p>不过，在介绍之前，我先提一句，图中括号里的C表示Class，O表示Object。还记得我在上节课提到过的伴生对象吗？没错，同时定义同名的Class和Object，就属于Scala中的伴生对象用法。</p><p>我们先来看伴生对象，也就是LogAppendInfo、Log和RollParams。</p><p><strong>1.LogAppendInfo</strong></p><ul><li>LogAppendInfo（C）：保存了一组待写入消息的各种元数据信息。比如，这组消息中第一条消息的位移值是多少、最后一条消息的位移值是多少；再比如，这组消息中最大的消息时间戳又是多少。总之，这里面的数据非常丰富（下节课我再具体说说）。</li><li>LogAppendInfo（O）: 可以理解为其对应伴生类的工厂方法类，里面定义了一些工厂方法，用于创建特定的LogAppendInfo实例。</li></ul><p><strong>2.Log</strong></p><ul><li>Log（C）: Log源码中最核心的代码。这里我先卖个关子，一会儿细聊。</li><li>Log（O）：同理，Log伴生类的工厂方法，定义了很多常量以及一些辅助方法。</li></ul><p><strong>3.RollParams</strong></p><ul><li>RollParams（C）：定义用于控制日志段是否切分（Roll）的数据结构。</li><li>RollParams（O）：同理，RollParams伴生类的工厂方法。</li></ul><p>除了这3组伴生对象之外，还有4类源码。</p><ul><li>LogMetricNames：定义了Log对象的监控指标。</li><li>LogOffsetSnapshot：封装分区所有位移元数据的容器类。</li><li>LogReadInfo：封装读取日志返回的数据及其元数据。</li><li>CompletedTxn：记录已完成事务的元数据，主要用于构建事务索引。</li></ul><h2 id="log-class--object"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/02.日志模块/02#log-class--object"><span class="icon icon-link"></span></a>Log Class &amp; Object</h2><p>下面，我会按照这些类和对象的重要程度，对它们一一进行拆解。首先，咱们先说说Log类及其伴生对象。</p><p>考虑到伴生对象多用于保存静态变量和静态方法（比如静态工厂方法等），因此我们先看伴生对象（即Log Object）的实现。毕竟，柿子先找软的捏！</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">object Log {</span></div><div class="token-line"><span class="token plain">      val LogFileSuffix = &quot;.log&quot;</span></div><div class="token-line"><span class="token plain">      val IndexFileSuffix = &quot;.index&quot;</span></div><div class="token-line"><span class="token plain">      val TimeIndexFileSuffix = &quot;.timeindex&quot;</span></div><div class="token-line"><span class="token plain">      val ProducerSnapshotFileSuffix = &quot;.snapshot&quot;</span></div><div class="token-line"><span class="token plain">      val TxnIndexFileSuffix = &quot;.txnindex&quot;</span></div><div class="token-line"><span class="token plain">      val DeletedFileSuffix = &quot;.deleted&quot;</span></div><div class="token-line"><span class="token plain">      val CleanedFileSuffix = &quot;.cleaned&quot;</span></div><div class="token-line"><span class="token plain">      val SwapFileSuffix = &quot;.swap&quot;</span></div><div class="token-line"><span class="token plain">      val CleanShutdownFile = &quot;.kafka_cleanshutdown&quot;</span></div><div class="token-line"><span class="token plain">      val DeleteDirSuffix = &quot;-delete&quot;</span></div><div class="token-line"><span class="token plain">      val FutureDirSuffix = &quot;-future&quot;</span></div><div class="token-line"><span class="token plain">    ……</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>这是Log Object定义的所有常量。如果有面试官问你Kafka中定义了多少种文件类型，你可以自豪地把这些说出来。耳熟能详的.log、.index、.timeindex和.txnindex我就不解释了，我们来了解下其他几种文件类型。</p><ul><li>.snapshot是Kafka为幂等型或事务型Producer所做的快照文件。鉴于我们现在还处于阅读源码的初级阶段，事务或幂等部分的源码我就不详细展开讲了。</li><li>.deleted是删除日志段操作创建的文件。目前删除日志段文件是异步操作，Broker端把日志段文件从.log后缀修改为.deleted后缀。如果你看到一大堆.deleted后缀的文件名，别慌，这是Kafka在执行日志段文件删除。</li><li>.cleaned和.swap都是Compaction操作的产物，等我们讲到Cleaner的时候再说。</li><li>-delete则是应用于文件夹的。当你删除一个主题的时候，主题的分区文件夹会被加上这个后缀。</li><li>-future是用于变更主题分区文件夹地址的，属于比较高阶的用法。</li></ul><p>总之，记住这些常量吧。记住它们的主要作用是，以后不要被面试官唬住！开玩笑，其实这些常量最重要的地方就在于，它们能够让你了解Kafka定义的各种文件类型。</p><p>Log Object还定义了超多的工具类方法。由于它们都很简单，这里我只给出一个方法的源码，我们一起读一下。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">def filenamePrefixFromOffset(offset: Long): String = {</span></div><div class="token-line"><span class="token plain">        val nf = NumberFormat.getInstance()</span></div><div class="token-line"><span class="token plain">        nf.setMinimumIntegerDigits(20)</span></div><div class="token-line"><span class="token plain">        nf.setMaximumFractionDigits(0)</span></div><div class="token-line"><span class="token plain">        nf.setGroupingUsed(false)</span></div><div class="token-line"><span class="token plain">        nf.format(offset)</span></div><div class="token-line"><span class="token plain">      }</span></div></pre></div><p>这个方法的作用是<strong>通过给定的位移值计算出对应的日志段文件名</strong>。Kafka日志文件固定是20位的长度，filenamePrefixFromOffset方法就是用前面补0的方式，把给定位移值扩充成一个固定20位长度的字符串。</p><p>举个例子，我们给定一个位移值是12345，那么Broker端磁盘上对应的日志段文件名就应该是00000000000000012345.log。怎么样，很简单吧？其他的工具类方法也很简单，我就不一一展开说了。</p><p>下面我们来看Log源码部分的重头戏：<strong>Log类</strong>。这是一个2000多行的大类。放眼整个Kafka源码，像Log这么大的类也不多见，足见它的重要程度。我们先来看这个类的定义：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">class Log(@volatile var dir: File,</span></div><div class="token-line"><span class="token plain">              @volatile var config: LogConfig,</span></div><div class="token-line"><span class="token plain">              @volatile var logStartOffset: Long,</span></div><div class="token-line"><span class="token plain">              @volatile var recoveryPoint: Long,</span></div><div class="token-line"><span class="token plain">              scheduler: Scheduler,</span></div><div class="token-line"><span class="token plain">              brokerTopicStats: BrokerTopicStats,</span></div><div class="token-line"><span class="token plain">              val time: Time,</span></div><div class="token-line"><span class="token plain">              val maxProducerIdExpirationMs: Int,</span></div><div class="token-line"><span class="token plain">              val producerIdExpirationCheckIntervalMs: Int,</span></div><div class="token-line"><span class="token plain">              val topicPartition: TopicPartition,</span></div><div class="token-line"><span class="token plain">              val producerStateManager: ProducerStateManager,</span></div><div class="token-line"><span class="token plain">              logDirFailureChannel: LogDirFailureChannel) extends Logging with KafkaMetricsGroup {</span></div><div class="token-line"><span class="token plain">    ……</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>看着好像有很多属性，但其实，你只需要记住两个属性的作用就够了：<strong>dir和logStartOffset</strong>。dir就是这个日志所在的文件夹路径，也就是<strong>主题分区的路径</strong>。而logStartOffset，表示<strong>日志的当前最早位移</strong>。dir和logStartOffset都是volatile var类型，表示它们的值是变动的，而且可能被多个线程更新。</p><p>你可能听过日志的当前末端位移，也就是Log End Offset（LEO），它是表示日志下一条待插入消息的位移值，而这个Log Start Offset是跟它相反的，它表示日志当前对外可见的最早一条消息的位移值。我用一张图来标识它们的区别：</p><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimage38b4388672f6dab8571f272ed47c9679c2b4.f45f9a77.jpg" alt=""/></p><p>图中绿色的位移值3是日志的Log Start Offset，而位移值15表示LEO。另外，位移值8是高水位值，它是区分已提交消息和未提交消息的分水岭。</p><p>有意思的是，Log End Offset可以简称为LEO，但Log Start Offset却不能简称为LSO。因为在Kafka中，LSO特指Log Stable Offset，属于Kafka事务的概念。这个课程中不会涉及LSO，你只需要知道Log Start Offset不等于LSO即可。</p><p>Log类的其他属性你暂时不用理会，因为它们要么是很明显的工具类属性，比如timer和scheduler，要么是高阶用法才会用到的高级属性，比如producerStateManager和logDirFailureChannel。工具类的代码大多是做辅助用的，跳过它们也不妨碍我们理解Kafka的核心功能；而高阶功能代码设计复杂，学习成本高，性价比不高。</p><p>其实，除了Log类签名定义的这些属性之外，Log类还定义了一些很重要的属性，比如下面这段代码：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">@volatile private var nextOffsetMetadata: LogOffsetMetadata = _</span></div><div class="token-line"><span class="token plain">        @volatile private var highWatermarkMetadata: LogOffsetMetadata = LogOffsetMetadata(logStartOffset)</span></div><div class="token-line"><span class="token plain">        private val segments: ConcurrentNavigableMap[java.lang.Long, LogSegment] = new ConcurrentSkipListMap[java.lang.Long, LogSegment]</span></div><div class="token-line"><span class="token plain">        @volatile var leaderEpochCache: Option[LeaderEpochFileCache] = None</span></div></pre></div><p>第一个属性nextOffsetMetadata，它封装了下一条待插入消息的位移值，你基本上可以把这个属性和LEO等同起来。</p><p>第二个属性highWatermarkMetadata，是分区日志高水位值。关于高水位的概念，我们在<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/intro/100029201">《Kafka核心技术与实战》<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>这个课程中做过详细解释，你可以看一下<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/112118">这篇文章<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>（下节课我还会再具体给你介绍下）。</p><p>第三个属性segments，我认为这是Log类中最重要的属性。它保存了分区日志下所有的日志段信息，只不过是用Map的数据结构来保存的。Map的Key值是日志段的起始位移值，Value则是日志段对象本身。Kafka源码使用ConcurrentNavigableMap数据结构来保存日志段对象，就可以很轻松地利用该类提供的线程安全和各种支持排序的方法，来管理所有日志段对象。</p><p>第四个属性是Leader Epoch Cache对象。Leader Epoch是社区于0.11.0.0版本引入源码中的，主要是用来判断出现Failure时是否执行日志截断操作（Truncation）。之前靠高水位来判断的机制，可能会造成副本间数据不一致的情形。这里的Leader Epoch Cache是一个缓存类数据，里面保存了分区Leader的Epoch值与对应位移值的映射关系，我建议你查看下LeaderEpochFileCache类，深入地了解下它的实现原理。</p><p>掌握了这些基本属性之后，我们看下Log类的初始化逻辑：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">locally {</span></div><div class="token-line"><span class="token plain">            val startMs = time.milliseconds</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            // create the log directory if it doesn&#x27;t exist</span></div><div class="token-line"><span class="token plain">            Files.createDirectories(dir.toPath)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            initializeLeaderEpochCache()</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            val nextOffset = loadSegments()</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            /* Calculate the offset of the next message */</span></div><div class="token-line"><span class="token plain">            nextOffsetMetadata = LogOffsetMetadata(nextOffset, activeSegment.baseOffset, activeSegment.size)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            leaderEpochCache.foreach(_.truncateFromEnd(nextOffsetMetadata.messageOffset))</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            logStartOffset = math.max(logStartOffset, segments.firstEntry.getValue.baseOffset)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            // The earliest leader epoch may not be flushed during a hard failure. Recover it here.</span></div><div class="token-line"><span class="token plain">            leaderEpochCache.foreach(_.truncateFromStart(logStartOffset))</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            // Any segment loading or recovery code must not use producerStateManager, so that we can build the full state here</span></div><div class="token-line"><span class="token plain">            // from scratch.</span></div><div class="token-line"><span class="token plain">            if (!producerStateManager.isEmpty)</span></div><div class="token-line"><span class="token plain">              throw new IllegalStateException(&quot;Producer state must be empty during log initialization&quot;)</span></div><div class="token-line"><span class="token plain">            loadProducerState(logEndOffset, reloadFromCleanShutdown = hasCleanShutdownFile)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            info(s&quot;Completed load of log with ${segments.size} segments, log start offset $logStartOffset and &quot; +</span></div><div class="token-line"><span class="token plain">              s&quot;log end offset $logEndOffset in ${time.milliseconds() - startMs}</span></div></pre></div><p>在详细解释这段初始化代码之前，我使用一张图来说明它到底做了什么：</p><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimagea1a8a10b81680a449e5b1d8882939061f7a8.a106d72f.jpg" alt=""/></p><p>这里我们重点说说第三步，即加载日志段的实现逻辑，以下是loadSegments的实现代码：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private def loadSegments(): Long = {</span></div><div class="token-line"><span class="token plain">            // first do a pass through the files in the log directory and remove any temporary files</span></div><div class="token-line"><span class="token plain">            // and find any interrupted swap operations</span></div><div class="token-line"><span class="token plain">            val swapFiles = removeTempFilesAndCollectSwapFiles()</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            // Now do a second pass and load all the log and index files.</span></div><div class="token-line"><span class="token plain">            // We might encounter legacy log segments with offset overflow (KAFKA-6264). We need to split such segments. When</span></div><div class="token-line"><span class="token plain">            // this happens, restart loading segment files from scratch.</span></div><div class="token-line"><span class="token plain">            retryOnOffsetOverflow {</span></div><div class="token-line"><span class="token plain">              // In case we encounter a segment with offset overflow, the retry logic will split it after which we need to retry</span></div><div class="token-line"><span class="token plain">              // loading of segments. In that case, we also need to close all segments that could have been left open in previous</span></div><div class="token-line"><span class="token plain">              // call to loadSegmentFiles().</span></div><div class="token-line"><span class="token plain">              logSegments.foreach(_.close())</span></div><div class="token-line"><span class="token plain">              segments.clear()</span></div><div class="token-line"><span class="token plain">              loadSegmentFiles()</span></div><div class="token-line"><span class="token plain">            }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            // Finally, complete any interrupted swap operations. To be crash-safe,</span></div><div class="token-line"><span class="token plain">            // log files that are replaced by the swap segment should be renamed to .deleted</span></div><div class="token-line"><span class="token plain">            // before the swap file is restored as the new segment file.</span></div><div class="token-line"><span class="token plain">            completeSwapOperations(swapFiles)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            if (!dir.getAbsolutePath.endsWith(Log.DeleteDirSuffix)) {</span></div><div class="token-line"><span class="token plain">              val nextOffset = retryOnOffsetOverflow {</span></div><div class="token-line"><span class="token plain">                recoverLog()</span></div><div class="token-line"><span class="token plain">              }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">              // reset the index size of the currently active log segment to allow more entries</span></div><div class="token-line"><span class="token plain">              activeSegment.resizeIndexes(config.maxIndexSize)</span></div><div class="token-line"><span class="token plain">              nextOffset</span></div><div class="token-line"><span class="token plain">            } else {</span></div><div class="token-line"><span class="token plain">               if (logSegments.isEmpty) {</span></div><div class="token-line"><span class="token plain">                  addSegment(LogSegment.open(dir = dir,</span></div><div class="token-line"><span class="token plain">                    baseOffset = 0,</span></div><div class="token-line"><span class="token plain">                    config,</span></div><div class="token-line"><span class="token plain">                    time = time,</span></div><div class="token-line"><span class="token plain">                    fileAlreadyExists = false,</span></div><div class="token-line"><span class="token plain">                    initFileSize = this.initFileSize,</span></div><div class="token-line"><span class="token plain">                    preallocate = false))</span></div><div class="token-line"><span class="token plain">               }</span></div><div class="token-line"><span class="token plain">              0</span></div><div class="token-line"><span class="token plain">            }</span></div></pre></div><p>这段代码会对分区日志路径遍历两次。</p><p>首先，它会移除上次Failure遗留下来的各种临时文件（包括.cleaned、.swap、.deleted文件等），removeTempFilesAndCollectSwapFiles方法实现了这个逻辑。</p><p>之后，它会清空所有日志段对象，并且再次遍历分区路径，重建日志段segments Map并删除无对应日志段文件的孤立索引文件。</p><p>待执行完这两次遍历之后，它会完成未完成的swap操作，即调用completeSwapOperations方法。等这些都做完之后，再调用recoverLog方法恢复日志段对象，然后返回恢复之后的分区日志LEO值。</p><p>如果你现在觉得有点蒙，也没关系，我把这段代码再进一步拆解下，以更小的粒度跟你讲下它们做了什么。理解了这段代码之后，你大致就能搞清楚大部分的分区日志操作了。所以，这部分代码绝对值得我们多花一点时间去学习。</p><p>我们首先来看第一步，removeTempFilesAndCollectSwapFiles方法的实现。我用注释的方式详细解释了每行代码的作用：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private def removeTempFilesAndCollectSwapFiles(): Set[File] = {</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 在方法内部定义一个名为deleteIndicesIfExist的方法，用于删除日志文件对应的索引文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        def deleteIndicesIfExist(baseFile: File, suffix: String = &quot;&quot;): Unit = {</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        info(s&quot;Deleting index files with suffix $suffix for baseFile $baseFile&quot;)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val offset = offsetFromFile(baseFile)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        Files.deleteIfExists(Log.offsetIndexFile(dir, offset, suffix).toPath)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        Files.deleteIfExists(Log.timeIndexFile(dir, offset, suffix).toPath)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        Files.deleteIfExists(Log.transactionIndexFile(dir, offset, suffix).toPath)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        var swapFiles = Set[File]()</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        var cleanFiles = Set[File]()</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        var minCleanedFileOffset = Long.MaxValue</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 遍历分区日志路径下的所有文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        for (file &lt;- dir.listFiles if file.isFile) {</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        if (!file.canRead) // 如果不可读，直接抛出IOException</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        throw new IOException(s&quot;Could not read file $file&quot;)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val filename = file.getName</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        if (filename.endsWith(DeletedFileSuffix)) { // 如果以.deleted结尾</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        debug(s&quot;Deleting stray temporary file ${file.getAbsolutePath}&quot;)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        Files.deleteIfExists(file.toPath) // 说明是上次Failure遗留下来的文件，直接删除</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        } else if (filename.endsWith(CleanedFileSuffix)) { // 如果以.cleaned结尾</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        minCleanedFileOffset = Math.min(offsetFromFileName(filename), minCleanedFileOffset) // 选取文件名中位移值最小的.cleaned文件，获取其位移值，并将该文件加入待删除文件集合中</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        cleanFiles += file</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        } else if (filename.endsWith(SwapFileSuffix)) { // 如果以.swap结尾</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val baseFile = new File(CoreUtils.replaceSuffix(file.getPath, SwapFileSuffix, &quot;&quot;))</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        info(s&quot;Found file ${file.getAbsolutePath} from interrupted swap operation.&quot;)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        if (isIndexFile(baseFile)) { // 如果该.swap文件原来是索引文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        deleteIndicesIfExist(baseFile) // 删除原来的索引文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        } else if (isLogFile(baseFile)) { // 如果该.swap文件原来是日志文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        deleteIndicesIfExist(baseFile) // 删除掉原来的索引文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        swapFiles += file // 加入待恢复的.swap文件集合中</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 从待恢复swap集合中找出那些起始位移值大于minCleanedFileOffset值的文件，直接删掉这些无效的.swap文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val (invalidSwapFiles, validSwapFiles) = swapFiles.partition(file =&gt; offsetFromFile(file) &gt;= minCleanedFileOffset)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        invalidSwapFiles.foreach { file =&gt;</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        debug(s&quot;Deleting invalid swap file ${file.getAbsoluteFile} minCleanedFileOffset: $minCleanedFileOffset&quot;)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val baseFile = new File(CoreUtils.replaceSuffix(file.getPath, SwapFileSuffix, &quot;&quot;))</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        deleteIndicesIfExist(baseFile, SwapFileSuffix)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        Files.deleteIfExists(file.toPath)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // Now that we have deleted all .swap files that constitute an incomplete split operation, let&#x27;s delete all .clean files</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 清除所有待删除文件集合中的文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        cleanFiles.foreach { file =&gt;</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        debug(s&quot;Deleting stray .clean file ${file.getAbsolutePath}&quot;)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        Files.deleteIfExists(file.toPath)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 最后返回当前有效的.swap文件集合</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        validSwapFiles</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div></pre></div><p>执行完了removeTempFilesAndCollectSwapFiles逻辑之后，源码开始清空已有日志段集合，并重新加载日志段文件。这就是第二步。这里调用的主要方法是loadSegmentFiles。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private def loadSegmentFiles(): Unit = {</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 按照日志段文件名中的位移值正序排列，然后遍历每个文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        for (file &lt;- dir.listFiles.sortBy(_.getName) if file.isFile) {</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        if (isIndexFile(file)) { // 如果是索引文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val offset = offsetFromFile(file)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val logFile = Log.logFile(dir, offset)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        if (!logFile.exists) { // 确保存在对应的日志文件，否则记录一个警告，并删除该索引文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        warn(s&quot;Found an orphaned index file ${file.getAbsolutePath}, with no corresponding log file.&quot;)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        Files.deleteIfExists(file.toPath)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        } else if (isLogFile(file)) { // 如果是日志文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val baseOffset = offsetFromFile(file)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val timeIndexFileNewlyCreated = !Log.timeIndexFile(dir, baseOffset).exists()</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 创建对应的LogSegment对象实例，并加入segments中</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val segment = LogSegment.open(dir = dir,</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        baseOffset = baseOffset,</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        config,</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        time = time,</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        fileAlreadyExists = true)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        try segment.sanityCheck(timeIndexFileNewlyCreated)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        catch {</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        case _: NoSuchFileException =&gt;</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        error(s&quot;Could not find offset index file corresponding to log file ${segment.log.file.getAbsolutePath}, &quot; +</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        &quot;recovering segment and rebuilding index files...&quot;)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        recoverSegment(segment)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        case e: CorruptIndexException =&gt;</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        warn(s&quot;Found a corrupted index file corresponding to log file ${segment.log.file.getAbsolutePath} due &quot; +</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        s&quot;to ${e.getMessage}}, recovering segment and rebuilding index files...&quot;)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        recoverSegment(segment)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        addSegment(segment)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div></pre></div><p>第三步是处理第一步返回的有效.swap文件集合。completeSwapOperations方法就是做这件事的：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private def completeSwapOperations(swapFiles: Set[File]): Unit = {</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 遍历所有有效.swap文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        for (swapFile &lt;- swapFiles) {</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val logFile = new File(CoreUtils.replaceSuffix(swapFile.getPath, SwapFileSuffix, &quot;&quot;)) // 获取对应的日志文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val baseOffset = offsetFromFile(logFile) // 拿到日志文件的起始位移值</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 创建对应的LogSegment实例</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val swapSegment = LogSegment.open(swapFile.getParentFile,</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        baseOffset = baseOffset,</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        config,</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        time = time,</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        fileSuffix = SwapFileSuffix)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        info(s&quot;Found log file ${swapFile.getPath} from interrupted swap operation, repairing.&quot;)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 执行日志段恢复操作</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        recoverSegment(swapSegment)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // We create swap files for two cases:</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // (1) Log cleaning where multiple segments are merged into one, and</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // (2) Log splitting where one segment is split into multiple.</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        //</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // Both of these mean that the resultant swap segments be composed of the original set, i.e. the swap segment</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // must fall within the range of existing segment(s). If we cannot find such a segment, it means the deletion</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // of that segment was successful. In such an event, we should simply rename the .swap to .log without having to</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // do a replace with an existing segment.</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 确认之前删除日志段是否成功，是否还存在老的日志段文件</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        val oldSegments = logSegments(swapSegment.baseOffset, swapSegment.readNextOffset).filter { segment =&gt;</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        segment.readNextOffset &gt; swapSegment.baseOffset</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        // 将生成的.swap文件加入到日志中，删除掉swap之前的日志段</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        replaceSegments(Seq(swapSegment), oldSegments.toSeq, isRecoveredSwapFile = true)</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        }</span></div></pre></div><p>最后一步是recoverLog操作：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private def recoverLog(): Long = {</span></div><div class="token-line"><span class="token plain">            // if we have the clean shutdown marker, skip recovery</span></div><div class="token-line"><span class="token plain">            // 如果不存在以.kafka_cleanshutdown结尾的文件。通常都不存在</span></div><div class="token-line"><span class="token plain">            if (!hasCleanShutdownFile) {</span></div><div class="token-line"><span class="token plain">              // 获取到上次恢复点以外的所有unflushed日志段对象</span></div><div class="token-line"><span class="token plain">              val unflushed = logSegments(this.recoveryPoint, Long.MaxValue).toIterator</span></div><div class="token-line"><span class="token plain">              var truncated = false</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">              // 遍历这些unflushed日志段</span></div><div class="token-line"><span class="token plain">              while (unflushed.hasNext &amp;&amp; !truncated) {</span></div><div class="token-line"><span class="token plain">                val segment = unflushed.next</span></div><div class="token-line"><span class="token plain">                info(s&quot;Recovering unflushed segment ${segment.baseOffset}&quot;)</span></div><div class="token-line"><span class="token plain">                val truncatedBytes =</span></div><div class="token-line"><span class="token plain">                  try {</span></div><div class="token-line"><span class="token plain">                    // 执行恢复日志段操作</span></div><div class="token-line"><span class="token plain">                    recoverSegment(segment, leaderEpochCache)</span></div><div class="token-line"><span class="token plain">                  } catch {</span></div><div class="token-line"><span class="token plain">                    case _: InvalidOffsetException =&gt;</span></div><div class="token-line"><span class="token plain">                      val startOffset = segment.baseOffset</span></div><div class="token-line"><span class="token plain">                      warn(&quot;Found invalid offset during recovery. Deleting the corrupt segment and &quot; +</span></div><div class="token-line"><span class="token plain">                        s&quot;creating an empty one with starting offset $startOffset&quot;)</span></div><div class="token-line"><span class="token plain">                      segment.truncateTo(startOffset)</span></div><div class="token-line"><span class="token plain">                  }</span></div><div class="token-line"><span class="token plain">                if (truncatedBytes &gt; 0) { // 如果有无效的消息导致被截断的字节数不为0，直接删除剩余的日志段对象</span></div><div class="token-line"><span class="token plain">                  warn(s&quot;Corruption found in segment ${segment.baseOffset}, truncating to offset ${segment.readNextOffset}&quot;)</span></div><div class="token-line"><span class="token plain">                  removeAndDeleteSegments(unflushed.toList, asyncDelete = true)</span></div><div class="token-line"><span class="token plain">                  truncated = true</span></div><div class="token-line"><span class="token plain">                }</span></div><div class="token-line"><span class="token plain">              }</span></div><div class="token-line"><span class="token plain">            }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            // 这些都做完之后，如果日志段集合不为空</span></div><div class="token-line"><span class="token plain">            if (logSegments.nonEmpty) {</span></div><div class="token-line"><span class="token plain">              val logEndOffset = activeSegment.readNextOffset</span></div><div class="token-line"><span class="token plain">              if (logEndOffset &lt; logStartOffset) { // 验证分区日志的LEO值不能小于Log Start Offset值，否则删除这些日志段对象</span></div><div class="token-line"><span class="token plain">                warn(s&quot;Deleting all segments because logEndOffset ($logEndOffset) is smaller than logStartOffset ($logStartOffset). &quot; +</span></div><div class="token-line"><span class="token plain">                  &quot;This could happen if segment files were deleted from the file system.&quot;)</span></div><div class="token-line"><span class="token plain">                removeAndDeleteSegments(logSegments, asyncDelete = true)</span></div><div class="token-line"><span class="token plain">              }</span></div><div class="token-line"><span class="token plain">            }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            // 这些都做完之后，如果日志段集合为空了</span></div><div class="token-line"><span class="token plain">            if (logSegments.isEmpty) {</span></div><div class="token-line"><span class="token plain">            // 至少创建一个新的日志段，以logStartOffset为日志段的起始位移，并加入日志段集合中</span></div><div class="token-line"><span class="token plain">              addSegment(LogSegment.open(dir = dir,</span></div><div class="token-line"><span class="token plain">                baseOffset = logStartOffset,</span></div><div class="token-line"><span class="token plain">                config,</span></div><div class="token-line"><span class="token plain">                time = time,</span></div><div class="token-line"><span class="token plain">                fileAlreadyExists = false,</span></div><div class="token-line"><span class="token plain">                initFileSize = this.initFileSize,</span></div><div class="token-line"><span class="token plain">                preallocate = config.preallocate))</span></div><div class="token-line"><span class="token plain">            }</span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">        </span></div><div class="token-line"><span class="token plain">            // 更新上次恢复点属性，并返回</span></div><div class="token-line"><span class="token plain">            recoveryPoint = activeSegment.readNextOffset</span></div><div class="token-line"><span class="token plain">            recoveryPoint</span></div></pre></div><h2 id="总结"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/02.日志模块/02#总结"><span class="icon icon-link"></span></a>总结</h2><p>今天，我重点向你介绍了Kafka的Log源码，主要包括：</p><ol><li><strong>Log文件的源码结构</strong>：你可以看下下面的导图，它展示了Log类文件的架构组成，你要重点掌握Log类及其相关方法。</li><li><strong>加载日志段机制</strong>：我结合源码重点分析了日志在初始化时是如何加载日志段的。前面说过了，日志是日志段的容器，弄明白如何加载日志段是后续学习日志段管理的前提条件。</li></ol><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimageddfcdd2bf4882021d969accb14c0017d9dfc.66b1256f.jpg" alt=""/></p><p>总的来说，虽然洋洋洒洒几千字，但我也只讲了最重要的部分。我建议你多看几遍Log.scala中加载日志段的代码，这对后面我们理解Kafka Broker端日志段管理原理大有裨益。在下节课，我会继续讨论日志部分的源码，带你学习常见的Kafka日志操作。</p><h2 id="课后讨论"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/02.日志模块/02#课后讨论"><span class="icon icon-link"></span></a>课后讨论</h2><p>Log源码中有个maybeIncrementHighWatermark方法，你能说说它的实现原理吗？</p><p>欢迎你在留言区畅所欲言，跟我交流讨论，也欢迎你把今天的内容分享给你的朋友。</p></div><div class="__dumi-default-layout-footer-meta"><a target="_blank" rel="noopener noreferrer" href="https://github.com/GGwujun/blog/edit/master/ssrc/kafka核心源码解读/02.日志模块/02.md">在 GitHub 上编辑此页<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><span data-updated-text="最后更新时间：">2023/9/27 11:15:40</span></div></div></div></div>
	<script>
  window.g_useSSR = true;
  window.g_initialProps = {};
	</script>

    <script>
      (function () {
        if (!location.port) {
          (function (i, s, o, g, r, a, m) {
            i["GoogleAnalyticsObject"] = r;
            (i[r] =
              i[r] ||
              function () {
                (i[r].q = i[r].q || []).push(arguments);
              }),
              (i[r].l = 1 * new Date());
            (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m);
          })(
            window,
            document,
            "script",
            "//www.google-analytics.com/analytics.js",
            "ga"
          );
          ga("create", "UA-149864185-1", "auto");
          ga("send", "pageview");
        }
      })();
    </script>
    <script src="/blog-backend/umi.e14e5a14.js"></script>
  </body>
</html>
