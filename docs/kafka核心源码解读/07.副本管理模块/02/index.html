<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no"
    />
    <link rel="shortcut icon" type="image/x-icon" href="/logo.png" />
    <link rel="stylesheet" href="/blog-backend/umi.3ec1f225.css" />
    <script>
      window.routerBase = "/blog-backend";
    </script>
    <script>
      //! umi version: 3.5.41
    </script>
    <script>
      !(function () {
        var e =
            navigator.cookieEnabled && void 0 !== window.localStorage
              ? localStorage.getItem("dumi:prefers-color")
              : "auto",
          o = window.matchMedia("(prefers-color-scheme: dark)").matches,
          t = ["light", "dark", "auto"];
        document.documentElement.setAttribute(
          "data-prefers-color",
          e === t[2] ? (o ? t[1] : t[0]) : t.indexOf(e) > -1 ? e : t[0]
        );
      })();
    </script>
    <title>
      22 | ReplicaFetcherThread：Follower如何拉取Leader消息？ - 大师兄
    </title>
  </head>
  <body>
    <div id="root"><div class="__dumi-default-layout" data-route="/kafka核心源码解读/07.副本管理模块/02" data-show-sidemenu="true" data-show-slugs="true" data-site-mode="true" data-gapless="false"><div class="__dumi-default-navbar" data-mode="site"><button class="__dumi-default-navbar-toggle"></button><a class="__dumi-default-navbar-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-backend/">大师兄</a><nav><div class="__dumi-default-search"><input type="search" class="__dumi-default-search-input" value=""/><ul></ul></div><span>后端开发<ul><li><a href="/blog-backend/go语言核心36讲">go语言核心36讲</a></li><li><a href="/blog-backend/go并发编程实战">go并发编程实战</a></li><li><a href="/blog-backend/go语言项目开发实战">go语言项目开发实战</a></li><li><a href="/blog-backend/kafka核心技术与实战">kafka核心技术与实战</a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/零基础学python">零基础学python</a></li><li><a href="/blog-backend/python核心技术与实战">python核心技术与实战</a></li><li><a href="/blog-backend/redis核心技术与实战">redis核心技术与实战</a></li><li><a href="/blog-backend/redis源码剖析与实战">redis源码剖析与实战</a></li><li><a href="/blog-backend/陈天rust编程第一课">陈天rust编程第一课</a></li><li><a href="/blog-backend/tonybaigo语言第一课">tonybaigo语言第一课</a></li><li><a href="/blog-backend/后端存储实战课">后端存储实战课</a></li><li><a href="/blog-backend/后端技术面试38讲">后端技术面试38讲</a></li><li><a href="/blog-backend/深入c语言和程序运行原理">深入c语言和程序运行原理</a></li><li><a href="/blog-backend/现代c编程实战">现代c编程实战</a></li><li><a href="/blog-backend/罗剑锋的c实战笔记">罗剑锋的c实战笔记</a></li><li><a href="/blog-backend/零基础入门spark">零基础入门spark</a></li></ul></span><span>架构师<ul><li><a href="/blog-backend/mysql实战45讲">mysql实战45讲</a></li><li><a href="/blog-backend/数据中台实战课">数据中台实战课</a></li></ul></span><div class="__dumi-default-navbar-tool"><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "></div></div></div></nav></div><div class="__dumi-default-menu" data-mode="site"><div class="__dumi-default-menu-inner"><div class="__dumi-default-menu-header"><a class="__dumi-default-menu-logo" style="background-image:url(&#x27;/logo.png&#x27;)" href="/blog-backend/"></a><h1>大师兄</h1><p></p></div><div class="__dumi-default-menu-mobile-area"><ul class="__dumi-default-menu-nav-list"><li>后端开发<ul><li><a href="/blog-backend/go语言核心36讲">go语言核心36讲</a></li><li><a href="/blog-backend/go并发编程实战">go并发编程实战</a></li><li><a href="/blog-backend/go语言项目开发实战">go语言项目开发实战</a></li><li><a href="/blog-backend/kafka核心技术与实战">kafka核心技术与实战</a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/零基础学python">零基础学python</a></li><li><a href="/blog-backend/python核心技术与实战">python核心技术与实战</a></li><li><a href="/blog-backend/redis核心技术与实战">redis核心技术与实战</a></li><li><a href="/blog-backend/redis源码剖析与实战">redis源码剖析与实战</a></li><li><a href="/blog-backend/陈天rust编程第一课">陈天rust编程第一课</a></li><li><a href="/blog-backend/tonybaigo语言第一课">tonybaigo语言第一课</a></li><li><a href="/blog-backend/后端存储实战课">后端存储实战课</a></li><li><a href="/blog-backend/后端技术面试38讲">后端技术面试38讲</a></li><li><a href="/blog-backend/深入c语言和程序运行原理">深入c语言和程序运行原理</a></li><li><a href="/blog-backend/现代c编程实战">现代c编程实战</a></li><li><a href="/blog-backend/罗剑锋的c实战笔记">罗剑锋的c实战笔记</a></li><li><a href="/blog-backend/零基础入门spark">零基础入门spark</a></li></ul></li><li>架构师<ul><li><a href="/blog-backend/mysql实战45讲">mysql实战45讲</a></li><li><a href="/blog-backend/数据中台实战课">数据中台实战课</a></li></ul></li></ul><div class="__dumi-default-dark"><div class="__dumi-default-dark-switch "><button title="Dark theme" class="__dumi-default-dark-moon "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="3854" width="22" height="22"><path d="M991.816611 674.909091a69.166545 69.166545 0 0 0-51.665455-23.272727 70.795636 70.795636 0 0 0-27.438545 5.585454A415.674182 415.674182 0 0 1 754.993338 698.181818c-209.594182 0-393.472-184.785455-393.472-395.636363 0-52.363636 38.539636-119.621818 69.515637-173.614546 4.887273-8.610909 9.634909-16.756364 14.103272-24.901818A69.818182 69.818182 0 0 0 384.631156 0a70.842182 70.842182 0 0 0-27.438545 5.585455C161.678429 90.298182 14.362065 307.898182 14.362065 512c0 282.298182 238.824727 512 532.38691 512a522.286545 522.286545 0 0 0 453.957818-268.334545A69.818182 69.818182 0 0 0 991.816611 674.909091zM546.679156 954.181818c-248.785455 0-462.941091-192-462.941091-442.181818 0-186.647273 140.637091-372.829091 300.939637-442.181818-36.817455 65.629091-92.578909 151.970909-92.578909 232.727273 0 250.181818 214.109091 465.454545 462.917818 465.454545a488.331636 488.331636 0 0 0 185.181091-46.545455 453.003636 453.003636 0 0 1-393.565091 232.727273z m103.656728-669.323636l-14.266182 83.781818a34.909091 34.909091 0 0 0 50.362182 36.770909l74.775272-39.563636 74.752 39.563636a36.142545 36.142545 0 0 0 16.174546 3.956364 34.909091 34.909091 0 0 0 34.210909-40.727273l-14.289455-83.781818 60.509091-59.345455a35.025455 35.025455 0 0 0-19.223272-59.578182l-83.61891-12.101818-37.376-76.101818a34.56 34.56 0 0 0-62.254545 0l-37.376 76.101818-83.618909 12.101818a34.909091 34.909091 0 0 0-19.246546 59.578182z m70.423272-64.698182a34.280727 34.280727 0 0 0 26.135273-19.083636l14.312727-29.090909 14.336 29.090909a34.257455 34.257455 0 0 0 26.135273 19.083636l32.046546 4.887273-23.272728 22.574545a35.234909 35.234909 0 0 0-10.007272 30.952727l5.46909 32.116364-28.625454-15.127273a34.490182 34.490182 0 0 0-32.302546 0l-28.695272 15.127273 5.469091-32.116364a35.141818 35.141818 0 0 0-9.984-30.952727l-23.272728-22.574545z" p-id="3855"></path></svg></button><button title="Light theme" class="__dumi-default-dark-sun "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4026" width="22" height="22"><path d="M915.2 476.16h-43.968c-24.704 0-44.736 16-44.736 35.84s20.032 35.904 44.736 35.904H915.2c24.768 0 44.8-16.064 44.8-35.904s-20.032-35.84-44.8-35.84zM512 265.6c-136.704 0-246.464 109.824-246.464 246.4 0 136.704 109.76 246.464 246.464 246.464S758.4 648.704 758.4 512c0-136.576-109.696-246.4-246.4-246.4z m0 425.6c-99.008 0-179.2-80.128-179.2-179.2 0-98.944 80.192-179.2 179.2-179.2S691.2 413.056 691.2 512c0 99.072-80.192 179.2-179.2 179.2zM197.44 512c0-19.84-19.136-35.84-43.904-35.84H108.8c-24.768 0-44.8 16-44.8 35.84s20.032 35.904 44.8 35.904h44.736c24.768 0 43.904-16.064 43.904-35.904zM512 198.464c19.776 0 35.84-20.032 35.84-44.8v-44.8C547.84 84.032 531.84 64 512 64s-35.904 20.032-35.904 44.8v44.8c0 24.768 16.128 44.864 35.904 44.864z m0 627.136c-19.776 0-35.904 20.032-35.904 44.8v44.736C476.096 940.032 492.16 960 512 960s35.84-20.032 35.84-44.8v-44.736c0-24.768-16.064-44.864-35.84-44.864z m329.92-592.832c17.472-17.536 20.288-43.072 6.4-57.024-14.016-14.016-39.488-11.2-57.024 6.336-4.736 4.864-26.496 26.496-31.36 31.36-17.472 17.472-20.288 43.008-6.336 57.024 13.952 14.016 39.488 11.2 57.024-6.336 4.8-4.864 26.496-26.56 31.296-31.36zM213.376 759.936c-4.864 4.8-26.56 26.624-31.36 31.36-17.472 17.472-20.288 42.944-6.4 56.96 14.016 13.952 39.552 11.2 57.024-6.336 4.8-4.736 26.56-26.496 31.36-31.36 17.472-17.472 20.288-43.008 6.336-56.96-14.016-13.952-39.552-11.072-56.96 6.336z m19.328-577.92c-17.536-17.536-43.008-20.352-57.024-6.336-14.08 14.016-11.136 39.488 6.336 57.024 4.864 4.864 26.496 26.56 31.36 31.424 17.536 17.408 43.008 20.288 56.96 6.336 14.016-14.016 11.264-39.488-6.336-57.024-4.736-4.864-26.496-26.56-31.296-31.424z m527.168 628.608c4.864 4.864 26.624 26.624 31.36 31.424 17.536 17.408 43.072 20.224 57.088 6.336 13.952-14.016 11.072-39.552-6.4-57.024-4.864-4.8-26.56-26.496-31.36-31.36-17.472-17.408-43.072-20.288-57.024-6.336-13.952 14.016-11.008 39.488 6.336 56.96z" p-id="4027"></path></svg></button><button title="Default to system" class="__dumi-default-dark-auto "><svg viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="11002" width="22" height="22"><path d="M127.658667 492.885333c0-51.882667 10.24-101.717333 30.378666-149.162666s47.786667-88.064 81.92-122.538667 75.093333-61.781333 122.538667-81.92 96.938667-30.378667 149.162667-30.378667 101.717333 10.24 149.162666 30.378667 88.405333 47.786667 122.88 81.92 61.781333 75.093333 81.92 122.538667 30.378667 96.938667 30.378667 149.162666-10.24 101.717333-30.378667 149.162667-47.786667 88.405333-81.92 122.88-75.093333 61.781333-122.88 81.92-97.28 30.378667-149.162666 30.378667-101.717333-10.24-149.162667-30.378667-88.064-47.786667-122.538667-81.92-61.781333-75.093333-81.92-122.88-30.378667-96.938667-30.378666-149.162667z m329.045333 0c0 130.048 13.994667 244.394667 41.984 343.381334h12.970667c46.762667 0 91.136-9.216 133.461333-27.306667s78.848-42.666667 109.568-73.386667 54.954667-67.242667 73.386667-109.568 27.306667-86.698667 27.306666-133.461333c0-46.421333-9.216-90.794667-27.306666-133.12s-42.666667-78.848-73.386667-109.568-67.242667-54.954667-109.568-73.386667-86.698667-27.306667-133.461333-27.306666h-11.605334c-28.672 123.562667-43.349333 237.909333-43.349333 343.722666z" p-id="11003"></path></svg></button></div></div></div><ul class="__dumi-default-menu-list"><li><a href="/blog-backend/kafka核心源码解读">kafka核心源码解读</a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学">01.课前必学</a><ul><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/01"><span>开篇词 |  阅读源码，逐渐成了职业进阶道路上的“必选项”</span></a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/02"><span>导读 | 构建Kafka工程和源码阅读环境、Scala语言热身</span></a></li><li><a href="/blog-backend/kafka核心源码解读/01.课前必学/03"><span>重磅加餐 | 带你快速入门Scala语言</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块">02.日志模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/01"><span>01 | 日志段：保存消息文件的对象是怎么实现的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/02"><span>02 | 日志（上）：日志究竟是如何加载日志段的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/03"><span>03 | 日志（下）：彻底搞懂Log对象的常见操作</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/04"><span>04 | 索引（上）：改进的二分查找算法在Kafka索引的应用</span></a></li><li><a href="/blog-backend/kafka核心源码解读/02.日志模块/05"><span>05 | 索引（下）：位移索引和时间戳索引的区别是什么？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块">03.请求处理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/01"><span>06 | 请求通道：如何实现Kafka请求队列？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/02"><span>07 | SocketServer（上）：Kafka到底是怎么应用NIO实现网络通信的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/03"><span>08 | SocketServer（中）：请求还要区分优先级？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/04"><span>09 | SocketServer（下）：请求处理全流程源码分析</span></a></li><li><a href="/blog-backend/kafka核心源码解读/03.请求处理模块/05"><span>10 | KafkaApis：Kafka最重要的源码入口，没有之一</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块">04.Controller模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/01"><span>11 | Controller元数据：Controller都保存有哪些东西？有几种状态？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/02"><span>12 | ControllerChannelManager：Controller如何管理请求发送？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/03"><span>13 | ControllerEventManager：变身单线程后的Controller如何处理事件？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/04"><span>14 | Controller选举是怎么实现的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/04.controller模块/05"><span>15 | 如何理解Controller在Kafka集群中的作用？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块">05.状态机模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/01"><span>16 | TopicDeletionManager： Topic是怎么被删除的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/02"><span>17 | ReplicaStateMachine：揭秘副本状态机实现原理</span></a></li><li><a href="/blog-backend/kafka核心源码解读/05.状态机模块/03"><span>18 | PartitionStateMachine：分区状态转换如何实现？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块">06.延迟操作模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块/01"><span>19 | TimingWheel：探究Kafka定时器背后的高效时间轮算法</span></a></li><li><a href="/blog-backend/kafka核心源码解读/06.延迟操作模块/02"><span>20 | DelayedOperation：Broker是怎么延时处理请求的？</span></a></li></ul></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读/07.副本管理模块">07.副本管理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/01"><span>21 | AbstractFetcherThread：拉取消息分几步？</span></a></li><li><a aria-current="page" class="active" href="/blog-backend/kafka核心源码解读/07.副本管理模块/02"><span>22 | ReplicaFetcherThread：Follower如何拉取Leader消息？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/03"><span>23 | ReplicaManager（上）：必须要掌握的副本管理类定义和核心字段</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/04"><span>24 | ReplicaManager（中）：副本管理器是如何读写副本的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/05"><span>25 | ReplicaManager（下）：副本管理器是如何管理副本的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/06"><span>26 | MetadataCache：Broker是怎么异步更新元数据缓存的？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块">08.消费者组管理模块</a><ul><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/01"><span>27 | 消费者组元数据（上）：消费者组都有哪些元数据？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/02"><span>28 | 消费者组元数据（下）：Kafka如何管理这些元数据？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/03"><span>29 | GroupMetadataManager：组元数据管理器是个什么东西？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/04"><span>30 | GroupMetadataManager：位移主题保存的只是位移吗？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/05"><span>31 | GroupMetadataManager：查询位移时，不用读取位移主题？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/06"><span>32 | GroupCoordinator：在Rebalance中，Coordinator如何处理成员入组？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/08.消费者组管理模块/07"><span>33 | GroupCoordinator：在Rebalance中，如何进行组同步？</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送">09.特别放送</a><ul><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/01"><span>特别放送（一）| 经典的Kafka学习资料有哪些？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/02"><span>特别放送（二）| 一篇文章带你了解参与开源社区的全部流程</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/03"><span>特别放送（三）| 我是怎么度过日常一天的？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/04"><span>特别放送（四）| 20道经典的Kafka面试题详解</span></a></li><li><a href="/blog-backend/kafka核心源码解读/09.特别放送/05"><span>特别放送（五） | Kafka 社区的重磅功能：移除 ZooKeeper 依赖</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/10.测试题">10.测试题</a><ul><li><a href="/blog-backend/kafka核心源码解读/10.测试题/01"><span>期中测试 | 这些源码知识，你都掌握了吗？</span></a></li><li><a href="/blog-backend/kafka核心源码解读/10.测试题/02"><span>期末测试 | 一套习题，测试你的掌握程度</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/11.结束语">11.结束语</a><ul><li><a href="/blog-backend/kafka核心源码解读/11.结束语/01"><span>结束语 | 源码学习，我们才刚上路呢</span></a></li></ul></li><li><a href="/blog-backend/kafka核心源码解读/summary">kafka核心源码解读</a></li></ul></div></div><ul role="slug-list" class="__dumi-default-layout-toc"><li title="AbstractFetcherThread类：doWork方法" data-depth="2"><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#abstractfetcherthread类dowork方法"><span>AbstractFetcherThread类：doWork方法</span></a></li><li title="子类：ReplicaFetcherThread" data-depth="2"><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#子类replicafetcherthread"><span>子类：ReplicaFetcherThread</span></a></li><li title="类定义及字段" data-depth="3"><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#类定义及字段"><span>类定义及字段</span></a></li><li title="重要方法" data-depth="3"><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#重要方法"><span>重要方法</span></a></li><li title="总结" data-depth="2"><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#总结"><span>总结</span></a></li><li title="课后讨论" data-depth="2"><a href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#课后讨论"><span>课后讨论</span></a></li></ul><div class="__dumi-default-layout-content"><div class="markdown"><h1 id="22--replicafetcherthreadfollower如何拉取leader消息"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#22--replicafetcherthreadfollower如何拉取leader消息"><span class="icon icon-link"></span></a>22 | ReplicaFetcherThread：Follower如何拉取Leader消息？</h1><p>你好，我是胡夕。今天，我们继续学习Follower是如何拉取Leader消息的。</p><p>要弄明白这个问题，在学习源码的时候，我们需要从父类AbstractFetcherThread开始学起，因为这是理解子类ReplicaFetcherThread的基础。上节课，我们已经学习了AbstractFetcherThread的定义，以及processPartitionData、truncate、buildFetch这三个方法的作用。现在，你应该掌握了拉取线程源码的处理逻辑以及支撑这些逻辑实现的代码结构。</p><p>不过，在上节课的末尾，我卖了个关子——我把串联起这三个方法的doWork方法留到了今天这节课。等你今天学完doWork方法，以及这三个方法在子类ReplicaFetcherThread中的实现代码之后，你就能完整地理解Follower副本应用拉取线程（也就是ReplicaFetcherThread线程），从Leader副本获取消息并处理的流程了。</p><p>那么，现在我们就开启doWork以及子类ReplicaFetcherThread代码的阅读。</p><h2 id="abstractfetcherthread类dowork方法"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#abstractfetcherthread类dowork方法"><span class="icon icon-link"></span></a>AbstractFetcherThread类：doWork方法</h2><p>doWork方法是AbstractFetcherThread类的核心方法，是线程的主逻辑运行方法，代码如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">override def doWork(): Unit = {</span></div><div class="token-line"><span class="token plain">      maybeTruncate()   // 执行副本截断操作</span></div><div class="token-line"><span class="token plain">      maybeFetch()      // 执行消息获取操作</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>怎么样，简单吧？AbstractFetcherThread线程只要一直处于运行状态，就是会不断地重复这两个操作。获取消息这个逻辑容易理解，但是为什么AbstractFetcherThread线程总要不断尝试去做截断呢？</p><p>这是因为，分区的Leader可能会随时发生变化。每当有新Leader产生时，Follower副本就必须主动执行截断操作，将自己的本地日志裁剪成与Leader一模一样的消息序列，甚至，Leader副本本身也需要执行截断操作，将LEO调整到分区高水位处。</p><p>那么，具体到代码，这两个操作又是如何实现的呢？</p><p>首先，我们看看maybeTruncate方法。它的代码不长，还不到10行：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private def maybeTruncate(): Unit = {</span></div><div class="token-line"><span class="token plain">      // 将所有处于截断中状态的分区依据有无Leader Epoch值进行分组</span></div><div class="token-line"><span class="token plain">      val (partitionsWithEpochs, partitionsWithoutEpochs) = fetchTruncatingPartitions()</span></div><div class="token-line"><span class="token plain">      // 对于有Leader Epoch值的分区，将日志截断到Leader Epoch值对应的位移值处</span></div><div class="token-line"><span class="token plain">      if (partitionsWithEpochs.nonEmpty) {</span></div><div class="token-line"><span class="token plain">        truncateToEpochEndOffsets(partitionsWithEpochs)</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">      // 对于没有Leader Epoch值的分区，将日志截断到高水位值处</span></div><div class="token-line"><span class="token plain">      if (partitionsWithoutEpochs.nonEmpty) {</span></div><div class="token-line"><span class="token plain">        truncateToHighWatermark(partitionsWithoutEpochs)</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>maybeTruncate方法的逻辑特别简单。</p><p>首先，是对分区状态进行分组。既然是做截断操作的，那么该方法操作的就只能是处于截断中状态的分区。代码会判断这些分区是否存在对应的Leader Epoch值，并按照有无Epoch值进行分组。这就是fetchTruncatingPartitions方法做的事情。</p><p>我在<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/225993">第3讲<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>提到过Leader Epoch机制，它是用来替换高水位值在日志截断中的作用。这里便是Leader Epoch机制典型的应用场景：</p><ul><li>当分区存在Leader Epoch值时，源码会将副本的本地日志截断到Leader Epoch对应的最新位移值处，即方法truncateToEpochEndOffsets的逻辑实现；</li><li>相反地，如果分区不存在对应的Leader Epoch记录，那么依然使用原来的高水位机制，调用方法truncateToHighWatermark将日志调整到高水位值处。</li></ul><p>由于Leader Epoch机制属于比较高阶的知识内容，这里我们的重点是理解高水位值在截断操作中的应用，我就不再和你详细讲解Leader Epoch机制了。如果你希望深入理解这个机制，你可以研读一下LeaderEpochFileCache类的源码。</p><p>因此，我们重点看下truncateToHighWatermark方法的实现代码。</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private[server] def truncateToHighWatermark(</span></div><div class="token-line"><span class="token plain">      partitions: Set[TopicPartition]): Unit = inLock(partitionMapLock) {</span></div><div class="token-line"><span class="token plain">      val fetchOffsets = mutable.HashMap.empty[TopicPartition, OffsetTruncationState]</span></div><div class="token-line"><span class="token plain">      // 遍历每个要执行截断操作的分区对象</span></div><div class="token-line"><span class="token plain">      for (tp &lt;- partitions) {</span></div><div class="token-line"><span class="token plain">        // 获取分区的分区读取状态</span></div><div class="token-line"><span class="token plain">        val partitionState = partitionStates.stateValue(tp)</span></div><div class="token-line"><span class="token plain">        if (partitionState != null) {</span></div><div class="token-line"><span class="token plain">          // 取出高水位值。分区的最大可读取位移值就是高水位值</span></div><div class="token-line"><span class="token plain">          val highWatermark = partitionState.fetchOffset</span></div><div class="token-line"><span class="token plain">          val truncationState = OffsetTruncationState(highWatermark, truncationCompleted = true)</span></div><div class="token-line"><span class="token plain">    </span></div><div class="token-line"><span class="token plain">          info(s&quot;Truncating partition $tp to local high watermark $highWatermark&quot;)</span></div><div class="token-line"><span class="token plain">          // 执行截断到高水位值</span></div><div class="token-line"><span class="token plain">          if (doTruncate(tp, truncationState))</span></div><div class="token-line"><span class="token plain">            fetchOffsets.put(tp, truncationState)</span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">      // 更新这组分区的分区读取状态</span></div><div class="token-line"><span class="token plain">      updateFetchOffsetAndMaybeMarkTruncationComplete(fetchOffsets)</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>我来和你解释下truncateToHighWatermark方法的逻辑：首先，遍历给定的所有分区；然后，依次为每个分区获取当前的高水位值，并将其保存在前面提到的分区读取状态类中；之后调用doTruncate方法执行真正的日志截断操作。等到将给定的所有分区都执行了对应的操作之后，代码会更新这组分区的分区读取状态。</p><p>doTruncate方法底层调用了抽象方法truncate，而truncate方法是在ReplicaFetcherThread中实现的。我们一会儿再详细说它。至于updateFetchOffsetAndMaybeMarkTruncationComplete方法，是一个只有十几行代码的私有方法。我就把它当作课后思考题留给你，由你来思考一下它是做什么用的吧。</p><p>说完了maybeTruncate方法，我们再看看maybeFetch方法，代码如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private def maybeFetch(): Unit = {</span></div><div class="token-line"><span class="token plain">      val fetchRequestOpt = inLock(partitionMapLock) {</span></div><div class="token-line"><span class="token plain">        // 为partitionStates中的分区构造FetchRequest</span></div><div class="token-line"><span class="token plain">        // partitionStates中保存的是要去获取消息的分区以及对应的状态</span></div><div class="token-line"><span class="token plain">        val ResultWithPartitions(fetchRequestOpt, partitionsWithError) = </span></div><div class="token-line"><span class="token plain">          buildFetch(partitionStates.partitionStateMap.asScala)</span></div><div class="token-line"><span class="token plain">        // 处理出错的分区，处理方式主要是将这个分区加入到有序Map末尾</span></div><div class="token-line"><span class="token plain">        // 等待后续重试</span></div><div class="token-line"><span class="token plain">        handlePartitionsWithErrors(partitionsWithError, &quot;maybeFetch&quot;)</span></div><div class="token-line"><span class="token plain">        // 如果当前没有可读取的分区，则等待fetchBackOffMs时间等候后续重试</span></div><div class="token-line"><span class="token plain">        if (fetchRequestOpt.isEmpty) {</span></div><div class="token-line"><span class="token plain">          trace(s&quot;There are no active partitions. Back off for $fetchBackOffMs ms before sending a fetch request&quot;)</span></div><div class="token-line"><span class="token plain">          partitionMapCond.await(fetchBackOffMs, TimeUnit.MILLISECONDS)</span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        fetchRequestOpt</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">      // 发送FETCH请求给Leader副本，并处理Response</span></div><div class="token-line"><span class="token plain">      fetchRequestOpt.foreach { case ReplicaFetch(sessionPartitions, fetchRequest) =&gt;</span></div><div class="token-line"><span class="token plain">        processFetchRequest(sessionPartitions, fetchRequest)</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>同样地，maybeFetch做的事情也基本可以分为3步。</p><p>第1步，为partitionStates中的分区构造FetchRequest对象，严格来说是FetchRequest.Builder对象。构造了Builder对象之后，通过调用其build方法，就能创建出所需的FetchRequest请求对象。</p><p>这里的partitionStates中保存的是，要去获取消息的一组分区以及对应的状态信息。这一步的输出结果是两个对象：</p><ul><li>一个对象是ReplicaFetch，即要读取的分区核心信息+ FetchRequest.Builder对象。而这里的核心信息，就是指要读取哪个分区，从哪个位置开始读，最多读多少字节，等等。</li><li>另一个对象是一组出错分区。</li></ul><p>第2步，处理这组出错分区。处理方式是将这组分区加入到有序Map末尾等待后续重试。如果发现当前没有任何可读取的分区，代码会阻塞等待一段时间。</p><p>第3步，发送FETCH请求给对应的Leader副本，并处理相应的Response，也就是processFetchRequest方法要做的事情。</p><p>processFetchRequest是AbstractFetcherThread所有方法中代码量最多的方法，逻辑也有些复杂。为了更好地理解它，我提取了其中的精华代码展示给你，并在每个关键步骤上都加了注释：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">private def processFetchRequest(sessionPartitions: </span></div><div class="token-line"><span class="token plain">      util.Map[TopicPartition, FetchRequest.PartitionData],</span></div><div class="token-line"><span class="token plain">      fetchRequest: FetchRequest.Builder): Unit = {</span></div><div class="token-line"><span class="token plain">        val partitionsWithError = mutable.Set[TopicPartition]()</span></div><div class="token-line"><span class="token plain">        var responseData: Map[TopicPartition, FetchData] = Map.empty</span></div><div class="token-line"><span class="token plain">        try {</span></div><div class="token-line"><span class="token plain">          trace(s&quot;Sending fetch request $fetchRequest&quot;)</span></div><div class="token-line"><span class="token plain">          // 给Leader发送FETCH请求</span></div><div class="token-line"><span class="token plain">          responseData = fetchFromLeader(fetchRequest)</span></div><div class="token-line"><span class="token plain">        } catch {</span></div><div class="token-line"><span class="token plain">        	......</span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        // 更新请求发送速率指标</span></div><div class="token-line"><span class="token plain">        fetcherStats.requestRate.mark()</span></div><div class="token-line"><span class="token plain">        if (responseData.nonEmpty) {</span></div><div class="token-line"><span class="token plain">          inLock(partitionMapLock) {</span></div><div class="token-line"><span class="token plain">            responseData.foreach { case (topicPartition, partitionData) =&gt;</span></div><div class="token-line"><span class="token plain">              Option(partitionStates.stateValue(topicPartition)).foreach { currentFetchState =&gt;</span></div><div class="token-line"><span class="token plain">                // 获取分区核心信息</span></div><div class="token-line"><span class="token plain">                val fetchPartitionData = sessionPartitions.get(topicPartition)</span></div><div class="token-line"><span class="token plain">                // 处理Response的条件：</span></div><div class="token-line"><span class="token plain">                // 1. 要获取的位移值和之前已保存的下一条待获取位移值相等</span></div><div class="token-line"><span class="token plain">                // 2. 当前分区处于可获取状态</span></div><div class="token-line"><span class="token plain">                if (fetchPartitionData != null &amp;&amp; fetchPartitionData.fetchOffset == currentFetchState.fetchOffset &amp;&amp; currentFetchState.isReadyForFetch) {</span></div><div class="token-line"><span class="token plain">                  // 提取Response中的Leader Epoch值 </span></div><div class="token-line"><span class="token plain">                  val requestEpoch = if (fetchPartitionData.currentLeaderEpoch.isPresent) Some(fetchPartitionData.currentLeaderEpoch.get().toInt) else None</span></div><div class="token-line"><span class="token plain">                  partitionData.error match {</span></div><div class="token-line"><span class="token plain">                    // 如果没有错误</span></div><div class="token-line"><span class="token plain">                    case Errors.NONE =&gt;</span></div><div class="token-line"><span class="token plain">                      try {</span></div><div class="token-line"><span class="token plain">                        // 交由子类完成Response的处理</span></div><div class="token-line"><span class="token plain">                        val logAppendInfoOpt = processPartitionData(topicPartition, currentFetchState.fetchOffset,</span></div><div class="token-line"><span class="token plain">                          partitionData)</span></div><div class="token-line"><span class="token plain">                        logAppendInfoOpt.foreach { logAppendInfo =&gt;</span></div><div class="token-line"><span class="token plain">                          val validBytes = logAppendInfo.validBytes</span></div><div class="token-line"><span class="token plain">                          val nextOffset = if (validBytes &gt; 0) logAppendInfo.lastOffset + 1 else currentFetchState.fetchOffset</span></div><div class="token-line"><span class="token plain">                          val lag = Math.max(0L, partitionData.highWatermark - nextOffset)</span></div><div class="token-line"><span class="token plain">    fetcherLagStats.getAndMaybePut(topicPartition).lag = lag</span></div><div class="token-line"><span class="token plain">                          if (validBytes &gt; 0 &amp;&amp; partitionStates.contains(topicPartition)) {</span></div><div class="token-line"><span class="token plain">                            val newFetchState = PartitionFetchState(nextOffset, Some(lag), currentFetchState.currentLeaderEpoch, state = Fetching)</span></div><div class="token-line"><span class="token plain">                            // 将该分区放置在有序Map读取顺序的末尾，保证公平性</span></div><div class="token-line"><span class="token plain">                            partitionStates.updateAndMoveToEnd(</span></div><div class="token-line"><span class="token plain">                              topicPartition, newFetchState)</span></div><div class="token-line"><span class="token plain">                            fetcherStats.byteRate.mark(validBytes)</span></div><div class="token-line"><span class="token plain">                          }</span></div><div class="token-line"><span class="token plain">                        }</span></div><div class="token-line"><span class="token plain">                      } catch {</span></div><div class="token-line"><span class="token plain">                      	......</span></div><div class="token-line"><span class="token plain">                      }</span></div><div class="token-line"><span class="token plain">                    // 如果读取位移值越界，通常是因为Leader发生变更</span></div><div class="token-line"><span class="token plain">                    case Errors.OFFSET_OUT_OF_RANGE =&gt;</span></div><div class="token-line"><span class="token plain">                      // 调整越界，主要办法是做截断</span></div><div class="token-line"><span class="token plain">                      if (handleOutOfRangeError(topicPartition, currentFetchState, requestEpoch))</span></div><div class="token-line"><span class="token plain">                        // 如果依然不能成功，加入到出错分区列表</span></div><div class="token-line"><span class="token plain">                        partitionsWithError += topicPartition</span></div><div class="token-line"><span class="token plain">                    // 如果Leader Epoch值比Leader所在Broker上的Epoch值要新</span></div><div class="token-line"><span class="token plain">                    case Errors.UNKNOWN_LEADER_EPOCH =&gt;</span></div><div class="token-line"><span class="token plain">                      debug(s&quot;Remote broker has a smaller leader epoch for partition $topicPartition than &quot; +</span></div><div class="token-line"><span class="token plain">                        s&quot;this replica&#x27;s current leader epoch of ${currentFetchState.currentLeaderEpoch}.&quot;)</span></div><div class="token-line"><span class="token plain">                      // 加入到出错分区列表</span></div><div class="token-line"><span class="token plain">                      partitionsWithError += topicPartition</span></div><div class="token-line"><span class="token plain">                    // 如果Leader Epoch值比Leader所在Broker上的Epoch值要旧</span></div><div class="token-line"><span class="token plain">                    case Errors.FENCED_LEADER_EPOCH =&gt;</span></div><div class="token-line"><span class="token plain">                      if (onPartitionFenced(topicPartition, requestEpoch)) partitionsWithError += topicPartition</span></div><div class="token-line"><span class="token plain">                    // 如果Leader发生变更</span></div><div class="token-line"><span class="token plain">                    case Errors.NOT_LEADER_FOR_PARTITION =&gt;</span></div><div class="token-line"><span class="token plain">                      debug(s&quot;Remote broker is not the leader for partition $topicPartition, which could indicate &quot; +</span></div><div class="token-line"><span class="token plain">                        &quot;that the partition is being moved&quot;)</span></div><div class="token-line"><span class="token plain">                      // 加入到出错分区列表</span></div><div class="token-line"><span class="token plain">                      partitionsWithError += topicPartition</span></div><div class="token-line"><span class="token plain">                    case _ =&gt;</span></div><div class="token-line"><span class="token plain">                      error(s&quot;Error for partition $topicPartition at offset ${currentFetchState.fetchOffset}&quot;,</span></div><div class="token-line"><span class="token plain">                        partitionData.error.exception)</span></div><div class="token-line"><span class="token plain">                      // 加入到出错分区列表</span></div><div class="token-line"><span class="token plain">                      partitionsWithError += topicPartition</span></div><div class="token-line"><span class="token plain">                  }</span></div><div class="token-line"><span class="token plain">                }</span></div><div class="token-line"><span class="token plain">              }</span></div><div class="token-line"><span class="token plain">            }</span></div><div class="token-line"><span class="token plain">          }</span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">        if (partitionsWithError.nonEmpty) {</span></div><div class="token-line"><span class="token plain">          // 处理出错分区列表</span></div><div class="token-line"><span class="token plain">          handlePartitionsWithErrors(partitionsWithError, &quot;processFetchRequest&quot;)</span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>为了方便你记忆，我先用一张流程图来说明下processFetchRequest方法的执行逻辑：</p><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimage3849387568fa5477ba71fc6bbe2868d76349.b8cf5273.png" alt=""/></p><p>结合着代码注释和流程图，我再和你解释下processFetchRequest的核心逻辑吧。这样你肯定就能明白拉取线程是如何执行拉取动作的了。</p><p>我们可以把这个逻辑，分为以下3大部分。</p><p>第1步，调用fetchFromLeader方法给Leader发送FETCH请求，并阻塞等待Response的返回，然后更新FETCH请求发送速率的监控指标。</p><p>第2步，拿到Response之后，代码从中取出分区的核心信息，然后比较要读取的位移值，和当前AbstractFetcherThread线程缓存的、该分区下一条待读取的位移值是否相等，以及当前分区是否处于可获取状态。</p><p>如果不满足这两个条件，说明这个Request可能是一个之前等待了许久都未处理的请求，压根就不用处理了。</p><p>相反，如果满足这两个条件且Response没有错误，代码会提取Response中的Leader Epoch值，然后交由子类实现具体的Response处理，也就是调用processPartitionData方法。之后将该分区放置在有序Map的末尾以保证公平性。而如果该Response有错误，那么就调用对应错误的定制化处理逻辑，然后将出错分区加入到出错分区列表中。</p><p>第3步，调用handlePartitionsWithErrors方法，统一处理上一步处理过程中出现错误的分区。</p><h2 id="子类replicafetcherthread"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#子类replicafetcherthread"><span class="icon icon-link"></span></a>子类：ReplicaFetcherThread</h2><p>到此，AbstractFetcherThread类的学习我们就完成了。接下来，我们再看下Follower副本侧使用的ReplicaFetcherThread子类。</p><p>前面说过了，ReplicaFetcherThread继承了AbstractFetcherThread类。ReplicaFetcherThread是Follower副本端创建的线程，用于向Leader副本拉取消息数据。我们依然从类定义和重要方法两个维度来学习这个子类的源码。</p><p>ReplicaFetcherThread类的源码位于server包下的同名scala文件中。这是一个300多行的小文件，因为大部分的处理逻辑都在父类AbstractFetcherThread中定义过了。</p><h3 id="类定义及字段"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#类定义及字段"><span class="icon icon-link"></span></a>类定义及字段</h3><p>我们先学习下ReplicaFetcherThread类的定义和字段：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">class ReplicaFetcherThread(name: String,</span></div><div class="token-line"><span class="token plain">                               fetcherId: Int,</span></div><div class="token-line"><span class="token plain">                               sourceBroker: BrokerEndPoint,</span></div><div class="token-line"><span class="token plain">                               brokerConfig: KafkaConfig,</span></div><div class="token-line"><span class="token plain">                               failedPartitions: FailedPartitions,</span></div><div class="token-line"><span class="token plain">                               replicaMgr: ReplicaManager,</span></div><div class="token-line"><span class="token plain">                               metrics: Metrics,</span></div><div class="token-line"><span class="token plain">                               time: Time,</span></div><div class="token-line"><span class="token plain">                               quota: ReplicaQuota,</span></div><div class="token-line"><span class="token plain">                               leaderEndpointBlockingSend: Option[BlockingSend] = None)</span></div><div class="token-line"><span class="token plain">      extends AbstractFetcherThread(name = name,</span></div><div class="token-line"><span class="token plain">                                    clientId = name,</span></div><div class="token-line"><span class="token plain">                                    sourceBroker = sourceBroker,</span></div><div class="token-line"><span class="token plain">                                    failedPartitions,</span></div><div class="token-line"><span class="token plain">                                    fetchBackOffMs = brokerConfig.replicaFetchBackoffMs,</span></div><div class="token-line"><span class="token plain">                                    isInterruptible = false,</span></div><div class="token-line"><span class="token plain">                                    replicaMgr.brokerTopicStats) {</span></div><div class="token-line"><span class="token plain">      // 副本Id就是副本所在Broker的Id</span></div><div class="token-line"><span class="token plain">      private val replicaId = brokerConfig.brokerId</span></div><div class="token-line"><span class="token plain">      ......</span></div><div class="token-line"><span class="token plain">      // 用于执行请求发送的类</span></div><div class="token-line"><span class="token plain">      private val leaderEndpoint = leaderEndpointBlockingSend.getOrElse(</span></div><div class="token-line"><span class="token plain">        new ReplicaFetcherBlockingSend(sourceBroker, brokerConfig, metrics, time, fetcherId,</span></div><div class="token-line"><span class="token plain">          s&quot;broker-$replicaId-fetcher-$fetcherId&quot;, logContext))</span></div><div class="token-line"><span class="token plain">      // Follower发送的FETCH请求被处理返回前的最长等待时间</span></div><div class="token-line"><span class="token plain">      private val maxWait = brokerConfig.replicaFetchWaitMaxMs</span></div><div class="token-line"><span class="token plain">      // 每个FETCH Response返回前必须要累积的最少字节数</span></div><div class="token-line"><span class="token plain">      private val minBytes = brokerConfig.replicaFetchMinBytes</span></div><div class="token-line"><span class="token plain">      // 每个合法FETCH Response的最大字节数</span></div><div class="token-line"><span class="token plain">      private val maxBytes = brokerConfig.replicaFetchResponseMaxBytes</span></div><div class="token-line"><span class="token plain">      // 单个分区能够获取到的最大字节数</span></div><div class="token-line"><span class="token plain">      private val fetchSize = brokerConfig.replicaFetchMaxBytes</span></div><div class="token-line"><span class="token plain">      // 维持某个Broker连接上获取会话状态的类</span></div><div class="token-line"><span class="token plain">      val fetchSessionHandler = new FetchSessionHandler(</span></div><div class="token-line"><span class="token plain">        logContext, sourceBroker.id)</span></div><div class="token-line"><span class="token plain">      ......</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>ReplicaFetcherThread类的定义代码虽然有些长，但你会发现没那么难懂，因为构造函数中的大部分字段我们上节课都学习过了。现在，我们只要学习ReplicaFetcherThread类特有的几个字段就可以了。</p><ul><li>fetcherId：Follower拉取的线程Id，也就是线程的编号。单台Broker上，允许存在多个ReplicaFetcherThread线程。Broker端参数num.replica.fetchers，决定了Kafka到底创建多少个Follower拉取线程。</li><li>brokerConfig：KafkaConfig类实例。虽然我们没有正式学习过它的源码，但之前学过的很多组件代码中都有它的身影。它封装了Broker端所有的参数信息。同样地，ReplicaFetcherThread类也是通过它来获取Broker端指定参数的值。</li><li>replicaMgr：副本管理器。该线程类通过副本管理器来获取分区对象、副本对象以及它们下面的日志对象。</li><li>quota：用做限流。限流属于高阶用法，如果你想深入理解这部分内容的话，可以自行阅读ReplicationQuotaManager类的源码。现在，只要你下次在源码中碰到quota字样的，知道它是用作Follower副本拉取速度控制就行了。</li><li>leaderEndpointBlockingSend：这是用于实现同步发送请求的类。所谓的同步发送，是指该线程使用它给指定Broker发送请求，然后线程处于阻塞状态，直到接收到Broker返回的Response。</li></ul><p>除了构造函数中定义的字段之外，ReplicaFetcherThread类还定义了与消息获取息息相关的4个字段。</p><ul><li>maxWait：Follower发送的FETCH请求被处理返回前的最长等待时间。它是Broker端参数replica.fetch.wait.max.ms的值。</li><li>minBytes：每个FETCH Response返回前必须要累积的最少字节数。它是Broker端参数replica.fetch.min.bytes的值。</li><li>maxBytes：每个合法FETCH Response的最大字节数。它是Broker端参数replica.fetch.response.max.bytes的值。</li><li>fetchSize：单个分区能够获取到的最大字节数。它是Broker端参数replica.fetch.max.bytes的值。</li></ul><p>这4个参数都是FETCH请求的参数，主要控制了Follower副本拉取Leader副本消息的行为，比如一次请求到底能够获取多少字节的数据，或者当未达到累积阈值时，FETCH请求等待多长时间等。</p><h3 id="重要方法"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#重要方法"><span class="icon icon-link"></span></a>重要方法</h3><p>接下来，我们继续学习ReplicaFetcherThread的3个重要方法：processPartitionData、buildFetch和truncate。</p><p>为什么是这3个方法呢？因为它们代表了Follower副本拉取线程要做的最重要的三件事：处理拉取的消息、构建拉取消息的请求，以及执行截断日志操作。</p><h4 id="processpartitiondata方法"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#processpartitiondata方法"><span class="icon icon-link"></span></a>processPartitionData方法</h4><p>我们先来看processPartitionData方法。AbstractFetcherThread线程从Leader副本拉取回消息后，需要调用processPartitionData方法进行后续动作。该方法的代码很长，我给其中的关键步骤添加了注释：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">override def processPartitionData(</span></div><div class="token-line"><span class="token plain">      topicPartition: TopicPartition,</span></div><div class="token-line"><span class="token plain">      fetchOffset: Long,</span></div><div class="token-line"><span class="token plain">      partitionData: FetchData): Option[LogAppendInfo] = {</span></div><div class="token-line"><span class="token plain">      val logTrace = isTraceEnabled</span></div><div class="token-line"><span class="token plain">      // 从副本管理器获取指定主题分区对象</span></div><div class="token-line"><span class="token plain">      val partition = replicaMgr.nonOfflinePartition(topicPartition).get</span></div><div class="token-line"><span class="token plain">      // 获取日志对象</span></div><div class="token-line"><span class="token plain">      val log = partition.localLogOrException</span></div><div class="token-line"><span class="token plain">      // 将获取到的数据转换成符合格式要求的消息集合</span></div><div class="token-line"><span class="token plain">      val records = toMemoryRecords(partitionData.records)</span></div><div class="token-line"><span class="token plain">      maybeWarnIfOversizedRecords(records, topicPartition)</span></div><div class="token-line"><span class="token plain">      // 要读取的起始位移值如果不是本地日志LEO值则视为异常情况</span></div><div class="token-line"><span class="token plain">      if (fetchOffset != log.logEndOffset)</span></div><div class="token-line"><span class="token plain">        throw new IllegalStateException(&quot;Offset mismatch for partition %s: fetched offset = %d, log end offset = %d.&quot;.format(</span></div><div class="token-line"><span class="token plain">          topicPartition, fetchOffset, log.logEndOffset))</span></div><div class="token-line"><span class="token plain">      if (logTrace)</span></div><div class="token-line"><span class="token plain">        trace(&quot;Follower has replica log end offset %d for partition %s. Received %d messages and leader hw %d&quot;</span></div><div class="token-line"><span class="token plain">          .format(log.logEndOffset, topicPartition, records.sizeInBytes, partitionData.highWatermark))</span></div><div class="token-line"><span class="token plain">      // 写入Follower副本本地日志</span></div><div class="token-line"><span class="token plain">      val logAppendInfo = partition.appendRecordsToFollowerOrFutureReplica(records, isFuture = false)</span></div><div class="token-line"><span class="token plain">      if (logTrace)</span></div><div class="token-line"><span class="token plain">        trace(&quot;Follower has replica log end offset %d after appending %d bytes of messages for partition %s&quot;</span></div><div class="token-line"><span class="token plain">          .format(log.logEndOffset, records.sizeInBytes, topicPartition))</span></div><div class="token-line"><span class="token plain">      val leaderLogStartOffset = partitionData.logStartOffset</span></div><div class="token-line"><span class="token plain">      // 更新Follower副本的高水位值</span></div><div class="token-line"><span class="token plain">      val followerHighWatermark = </span></div><div class="token-line"><span class="token plain">        log.updateHighWatermark(partitionData.highWatermark)</span></div><div class="token-line"><span class="token plain">      // 尝试更新Follower副本的Log Start Offset值</span></div><div class="token-line"><span class="token plain">      log.maybeIncrementLogStartOffset(leaderLogStartOffset, LeaderOffsetIncremented)</span></div><div class="token-line"><span class="token plain">      if (logTrace)</span></div><div class="token-line"><span class="token plain">        trace(s&quot;Follower set replica high watermark for partition $topicPartition to $followerHighWatermark&quot;)</span></div><div class="token-line"><span class="token plain">      // 副本消息拉取限流</span></div><div class="token-line"><span class="token plain">      if (quota.isThrottled(topicPartition))</span></div><div class="token-line"><span class="token plain">        quota.record(records.sizeInBytes)</span></div><div class="token-line"><span class="token plain">      // 更新统计指标值</span></div><div class="token-line"><span class="token plain">      if (partition.isReassigning &amp;&amp; partition.isAddingLocalReplica)</span></div><div class="token-line"><span class="token plain">        brokerTopicStats.updateReassignmentBytesIn(records.sizeInBytes)</span></div><div class="token-line"><span class="token plain">      brokerTopicStats.updateReplicationBytesIn(records.sizeInBytes)</span></div><div class="token-line"><span class="token plain">      // 返回日志写入结果</span></div><div class="token-line"><span class="token plain">      logAppendInfo</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>在详细解释前，我使用一张流程图帮助你直观地理解这个方法到底做了什么事情。</p><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimaged026d0342f40ff5470086fb904983dbd3f26.2467bfc7.png" alt=""/></p><p>processPartitionData方法中的process，实际上就是写入Follower副本本地日志的意思。因此，这个方法的主体逻辑，就是调用分区对象Partition的appendRecordsToFollowerOrFutureReplica写入获取到的消息。如果你沿着这个写入方法一路追下去，就会发现它调用的是我们在<a target="_blank" rel="noopener noreferrer" href="https://time.geekbang.org/column/article/224795">第2讲<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>中讲到过的appendAsFollower方法。你看一切都能串联起来，源码也没什么大不了的，对吧？</p><p>当然，仅仅写入日志还不够。我们还要做一些更新操作。比如，需要更新Follower副本的高水位值，即将FETCH请求Response中包含的高水位值作为新的高水位值，同时代码还要尝试更新Follower副本的Log Start Offset值。</p><p>那为什么Log Start Offset值也可能发生变化呢？这是因为Leader的Log Start Offset可能发生变化，比如用户手动执行了删除消息的操作等。Follower副本的日志需要和Leader保持严格的一致，因此，如果Leader的该值发生变化，Follower自然也要发生变化，以保持一致。</p><p>除此之外，processPartitionData方法还会更新其他一些统计指标值，最后将写入结果返回。</p><h4 id="buildfetch方法"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#buildfetch方法"><span class="icon icon-link"></span></a>buildFetch方法</h4><p>接下来， 我们看下buildFetch方法。此方法的主要目的是，构建发送给Leader副本所在Broker的FETCH请求。它的代码如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">override def buildFetch(</span></div><div class="token-line"><span class="token plain">      partitionMap: Map[TopicPartition, PartitionFetchState]): ResultWithPartitions[Option[ReplicaFetch]] = {</span></div><div class="token-line"><span class="token plain">      val partitionsWithError = mutable.Set[TopicPartition]()</span></div><div class="token-line"><span class="token plain">      val builder = fetchSessionHandler.newBuilder(partitionMap.size, false)</span></div><div class="token-line"><span class="token plain">      // 遍历每个分区，将处于可获取状态的分区添加到builder后续统一处理</span></div><div class="token-line"><span class="token plain">      // 对于有错误的分区加入到出错分区列表</span></div><div class="token-line"><span class="token plain">      partitionMap.foreach { case (topicPartition, fetchState) =&gt;</span></div><div class="token-line"><span class="token plain">        if (fetchState.isReadyForFetch &amp;&amp; !shouldFollowerThrottle(quota, fetchState, topicPartition)) {</span></div><div class="token-line"><span class="token plain">          try {</span></div><div class="token-line"><span class="token plain">            val logStartOffset = this.logStartOffset(topicPartition)</span></div><div class="token-line"><span class="token plain">            builder.add(topicPartition, new FetchRequest.PartitionData(</span></div><div class="token-line"><span class="token plain">              fetchState.fetchOffset, logStartOffset, fetchSize, Optional.of(fetchState.currentLeaderEpoch)))</span></div><div class="token-line"><span class="token plain">          } catch {</span></div><div class="token-line"><span class="token plain">            case _: KafkaStorageException =&gt;</span></div><div class="token-line"><span class="token plain">              partitionsWithError += topicPartition</span></div><div class="token-line"><span class="token plain">          }</span></div><div class="token-line"><span class="token plain">        }</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">      val fetchData = builder.build()</span></div><div class="token-line"><span class="token plain">      val fetchRequestOpt = if (fetchData.sessionPartitions.isEmpty &amp;&amp; fetchData.toForget.isEmpty) {</span></div><div class="token-line"><span class="token plain">        None</span></div><div class="token-line"><span class="token plain">      } else {</span></div><div class="token-line"><span class="token plain">        // 构造FETCH请求的Builder对象</span></div><div class="token-line"><span class="token plain">        val requestBuilder = FetchRequest.Builder</span></div><div class="token-line"><span class="token plain">          .forReplica(fetchRequestVersion, replicaId, maxWait, minBytes, fetchData.toSend)</span></div><div class="token-line"><span class="token plain">          .setMaxBytes(maxBytes)</span></div><div class="token-line"><span class="token plain">          .toForget(fetchData.toForget)</span></div><div class="token-line"><span class="token plain">          .metadata(fetchData.metadata)</span></div><div class="token-line"><span class="token plain">        Some(ReplicaFetch(fetchData.sessionPartitions(), requestBuilder))</span></div><div class="token-line"><span class="token plain">      }</span></div><div class="token-line"><span class="token plain">      // 返回Builder对象以及出错分区列表</span></div><div class="token-line"><span class="token plain">      ResultWithPartitions(fetchRequestOpt, partitionsWithError)</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>同样，我使用一张图来展示其完整流程。</p><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimageb389b321756cdc623fe790aa94deae40f989.14b3826d.png" alt=""/></p><p>这个方法的逻辑比processPartitionData简单。前面说到过，它就是构造FETCH请求的Builder对象然后返回。有了Builder对象，我们就可以分分钟构造出FETCH请求了，仅需要调用builder.build()即可。</p><p>当然，这个方法的一个副产品是汇总出错分区，这样的话，调用方后续可以统一处理这些出错分区。值得一提的是，在构造Builder的过程中，源码会用到ReplicaFetcherThread类定义的那些与消息获取相关的字段，如maxWait、minBytes和maxBytes。</p><h4 id="truncate方法"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#truncate方法"><span class="icon icon-link"></span></a>truncate方法</h4><p>最后，我们看下truncate方法的实现。这个方法的主要目的是对给定分区执行日志截断操作。代码如下：</p><div class="__dumi-default-code-block"><pre class="prism-code language-unknown"><button class="__dumi-default-icon __dumi-default-code-block-copy-btn" data-status="ready"></button><div class="token-line"><span class="token plain">override def truncate(</span></div><div class="token-line"><span class="token plain">      tp: TopicPartition, </span></div><div class="token-line"><span class="token plain">      offsetTruncationState: OffsetTruncationState): Unit = {</span></div><div class="token-line"><span class="token plain">      // 拿到分区对象</span></div><div class="token-line"><span class="token plain">      val partition = replicaMgr.nonOfflinePartition(tp).get</span></div><div class="token-line"><span class="token plain">      //拿到分区本地日志 </span></div><div class="token-line"><span class="token plain">      val log = partition.localLogOrException</span></div><div class="token-line"><span class="token plain">      // 执行截断操作，截断到的位置由offsetTruncationState的offset指定</span></div><div class="token-line"><span class="token plain">      partition.truncateTo(offsetTruncationState.offset, isFuture = false)</span></div><div class="token-line"><span class="token plain">      if (offsetTruncationState.offset &lt; log.highWatermark)</span></div><div class="token-line"><span class="token plain">        warn(s&quot;Truncating $tp to offset ${offsetTruncationState.offset} below high watermark &quot; +</span></div><div class="token-line"><span class="token plain">          s&quot;${log.highWatermark}&quot;)</span></div><div class="token-line"><span class="token plain">      if (offsetTruncationState.truncationCompleted)</span></div><div class="token-line"><span class="token plain">        replicaMgr.replicaAlterLogDirsManager</span></div><div class="token-line"><span class="token plain">          .markPartitionsForTruncation(brokerConfig.brokerId, tp,</span></div><div class="token-line"><span class="token plain">          offsetTruncationState.offset)</span></div><div class="token-line"><span class="token plain">    }</span></div></pre></div><p>总体来说，truncate方法利用给定的offsetTruncationState的offset值，对给定分区的本地日志进行截断操作。该操作由Partition对象的truncateTo方法完成，但实际上底层调用的是Log的truncateTo方法。truncateTo方法的主要作用，是将日志截断到小于给定值的最大位移值处。</p><h2 id="总结"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#总结"><span class="icon icon-link"></span></a>总结</h2><p>好了，我们总结一下。就像我在开头时所说，AbstractFetcherThread线程的doWork方法把上一讲提到的3个重要方法全部连接在一起，共同完整了拉取线程要执行的逻辑，即日志截断（truncate）+日志获取（buildFetch）+日志处理（processPartitionData），而其子类ReplicaFetcherThread类是真正实现该3个方法的地方。如果用一句话归纳起来，那就是：Follower副本利用ReplicaFetcherThread线程实时地从Leader副本拉取消息并写入到本地日志，从而实现了与Leader副本之间的同步。以下是一些要点：</p><ul><li>doWork方法：拉取线程工作入口方法，联结所有重要的子功能方法，如执行截断操作，获取Leader副本消息以及写入本地日志。</li><li>truncate方法：根据Leader副本返回的位移值和Epoch值执行本地日志的截断操作。</li><li>buildFetch方法：为一组特定分区构建FetchRequest对象所需的数据结构。</li><li>processPartitionData方法：处理从Leader副本获取到的消息，主要是写入到本地日志中。</li></ul><p><img src="/blog-backend/static/httpsstatic001geekbangorgresourceimageeb52ebd9a667369fc304bce3yybdd439a152.dbded7f0.jpg" alt=""/></p><p>实际上，今天的内容中多次出现副本管理器的身影。如果你仔细查看代码，你会发现Follower副本正是利用它来获取对应分区Partition对象的，然后依靠该对象执行消息写入。那么，副本管理器还有哪些其他功能呢？下一讲我将一一为你揭晓。</p><h2 id="课后讨论"><a aria-hidden="true" tabindex="-1" href="/blog-backend/kafka核心源码解读/07.副本管理模块/02#课后讨论"><span class="icon icon-link"></span></a>课后讨论</h2><p>你可以去查阅下源码，说说updateFetchOffsetAndMaybeMarkTruncationComplete方法是做什么用的吗？</p><p>欢迎在留言区写下你的思考和答案，跟我交流讨论，也欢迎你把今天的内容分享给你的朋友。</p></div><div class="__dumi-default-layout-footer-meta"><a target="_blank" rel="noopener noreferrer" href="https://github.com/GGwujun/blog/edit/master/ssrc/kafka核心源码解读/07.副本管理模块/02.md">在 GitHub 上编辑此页<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="__dumi-default-external-link-icon"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a><span data-updated-text="最后更新时间：">2023/9/27 11:15:40</span></div></div></div></div>
	<script>
  window.g_useSSR = true;
  window.g_initialProps = {};
	</script>

    <script>
      (function () {
        if (!location.port) {
          (function (i, s, o, g, r, a, m) {
            i["GoogleAnalyticsObject"] = r;
            (i[r] =
              i[r] ||
              function () {
                (i[r].q = i[r].q || []).push(arguments);
              }),
              (i[r].l = 1 * new Date());
            (a = s.createElement(o)), (m = s.getElementsByTagName(o)[0]);
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m);
          })(
            window,
            document,
            "script",
            "//www.google-analytics.com/analytics.js",
            "ga"
          );
          ga("create", "UA-149864185-1", "auto");
          ga("send", "pageview");
        }
      })();
    </script>
    <script src="/blog-backend/umi.e14e5a14.js"></script>
  </body>
</html>
